October 12, 2023
[[KAIST]], NAVER AI Lab, UWashington, MIT
Paper: [Prometheus: Inducing Fine-Grained Evaluation Capability in Langauge Models](https://arxiv.org/abs/2310.08491)
#zotero 
Takeaway: ==A 13B parameter "evaluation" model that can assess any given long-form text based on *customized* score rubric provided by the user==. Also introduces [[Feedback Collection]], a new dataset of 1K fine-grained score rubrics, 20k instructions, and 100k responses and language feedback generated by GPT-4.

Compare with: [[Shepherd]], another critic model, which came out 2 months earlier on August 8, 2023. Their model was (iirc) on-par with ChatGPT, whereas ==Prometheus claims to be on-par with GPT-4's evaluation capabilities== (when the  appropriate reference materials (reference answer, score rubric) are accompanied). ((Though TBH having the "reference answer" to power your critique feels like cheating))
- ((==EDIT==)): I think that Prometheus is more about evaluation/rating, whereas Shepherd is more about generating critiques, so it's not an apples-to-apples.

Question: How does this evaluation model differ from reward models?

----

Notes:
- Paper notes that Humans are naturally able to discern the most important factors of assessment, such as ==brevity, creativity, tone, and cultural sensitivities==.
	- In contrast, conventional automated evaluation metrics like [[BLEU]] and [[ROGUE]] can't capture the full depth and granularity of human evaluation.
	- Thus, using LLMs as evaluators (eg GPT-4) has received substantial attention due to its parity with human evaluators.
- The author's strategy revolves around using an open-source LLM to evaluate and critique a response based on a human-provided scoring rubric. Authors want to *not* use a closed-source model like GPT-4 because of:
	1. Closed Source Nature: lack of transparency hinders academic efforts to refine or enhance its evaluation capabilities.
	2. Uncontrolled versioning: Proprietary models undero

# Abstract

>Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an ==evaluator== for long-form responses has become the de facto standard. ==However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable== due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the ==Feedback Collection==, a ==new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4==. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model atÂ [this https URL](https://kaistai.github.io/prometheus/).

# Paper Figures
![[Pasted image 20240508124353.png]]
Above: See that rather than testing some specific domain/task or assessing a score based on harmfulness and helpfulness, we're providing a human-written scoring rubric and asking an LLM to evaluate by that criteria. Specifically, the authors train an open-source, smaller model to do the evaluation, rather than using an expensive alternative like GPT-4.

# Non-Paper Figures