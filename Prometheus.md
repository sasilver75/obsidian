October 12, 2023
[[KAIST]], NAVER AI Lab, UWashington, MIT
Paper: [Prometheus: Inducing Fine-Grained Evaluation Capability in Langauge Models](https://arxiv.org/abs/2310.08491)
#zotero 
Takeaway: ==A 13B parameter "evaluation" model that can assess any given long-form text based on *customized* score rubric provided by the user==. 
- Authors note that the providing of reference materials (score rubrics, reference answers) are critical to the success of the evaluator model!
	- Authors also show *some* positive transfer between the "absolute grading" task (that this paper is about) and the "ranking grading" task (eg reward models).
- Also introduces [[Feedback Collection]], a new dataset of 1K fine-grained score rubrics, 20k instructions, and 100k responses and language feedback generated by GPT-4.

Compare with: [[Shepherd]], another critic model, which came out 2 months earlier on August 8, 2023. Their model was (iirc) on-par with ChatGPT, whereas ==Prometheus claims to be on-par with GPT-4's evaluation capabilities== (when the  appropriate reference materials (reference answer, score rubric) are accompanied). ((Though TBH having the "reference answer" to power your critique feels like cheating ;) ))
- ((==EDIT==)): Prometheus generates both a feedback and a score, whereas Shepherd just generates a feedback (a critique).

Question: How does this evaluation model differ from reward models?

----

Notes:
- Paper notes that Humans are naturally able to discern the most important factors of assessment, such as ==brevity, creativity, tone, and cultural sensitivities==.
	- In contrast, conventional automated evaluation metrics like [[BLEU]] and [[ROGUE]] can't capture the full depth and granularity of human evaluation.
	- Thus, using LLMs as evaluators (eg GPT-4) has received substantial attention due to its parity with human evaluators.
- The author's strategy revolves around using an open-source LLM to evaluate and critique a response based on a human-provided scoring rubric. Authors want to *not* use a closed-source model like GPT-4 because of:
	1. **Closed Source Nature**: lack of transparency hinders academic efforts to refine or enhance its evaluation capabilities.
	2. **Uncontrolled versioning**: Proprietary models undergo version updates that are often beyond users' purview or control, creating reproducibility problems.
	3. **Prohibitive costs**: Scaling costs can be prohibitive for academic institutions or researchers with limited budgets.
- Current evaluations are either too domain/task-specific (EM, ROGUE), or coarse-grained (e.g. helpfulness/harmlessness). In real world scenarios, users are interested in *customized rubrics* such as: "Which LLM generates responses that are playful and humorous," or "Which LLM answers with particular care for cultural sensitivities."
	- ==Authors find that even the larger open-source LLMS @ 70B are insufficient to evaluate using customized score rubrics==, compared to closed frontier models.
- Authors introduce ==Feedback Collection==, a new dataset crafted to *encapsulate DIVERSE and FINE-GRAINED user assessment score rubrics* that represent realistic user demands. Unlike prior feedback datasets, it uses *custom*, not *generic* preference score rubrics to train models to *flexibly generalize* to practical and diverse evaluation preferences.
	- They also say they're the first to explore the importance of including various reference materials -- particularly *"Reference Answers"* -- to effectively induce fine-grained evaluation capability.
- Authors use Feedback Collection to create [[Prometheus]], a fine-tune of [[LLaMA 2]]-Chat-13B.
	- ==Prometheus obtains a correlation of .897 with human evaluators==, similar to GPT-4's .882, which are both significantly better than GPT-3.5-Turbo's .392... as measured on 45 customized score rubrics sampled across three test sets (MT Bench, Vicuna Bench, Feedback Bench).
- ==Related Work==
	- Model-free scores that evaluate machine-generated text based on a golden candidate reference, like [[BLEU]] (2002) and [[ROGUE]] (2004).
	- Model-based approaches like BERTScore (2019) and BLEURT (2020)  and BARTScore (2021), which are able to capture *semantic information*, rather than only evaluating on *lexical* components.
	- LLM-based text evaluation using GPT-4 or fine-tuned critique LLMs as an evaluator along a *single dimension* of "preference," such as [[AlpacaFarm]] ("which response is better, based on your judgement and preferences"). Critique LLMs like Selfee and Shepherd
- Authors note that correct preference is often subjective, depending on applications, cultures, objectives, where degrees of brevity, formality, honesty, creativity, and political tone (among others) may vary.
- ==[[Feedback Collection]]==
	- A new dataset for the sole purpose of fine-tuning an open-source evaluator LLM.
	- ==4 main considerations== during dataset construction are:
		1. Including as many reference materials as possible (eg reference answer, scoring rubric)
		2. Maintaining a uniform length among the reference answers for each score (1 to 5) to *prevent undesired length bias*.
		3. Maintaining a *uniform score distribution* to prevent undesired decision bias.
		4. Limiting the scope of our instructions and responses to realistic situations where a user is interacting with an LLM.
	- ==Dataset size==: The dataset includes 1k score rubrics (fine-grained and customized), with 20 instructions accompanied (intermediate 20k) for each score rubric, and a 1-5 response for each instruction/rubric, leading to a final 100k dataset size.
	- ==Components of an input==:
		1. **Instruction**: An instruction that a user would prompt to an arbitrary LLM.
		2. **Response to Evaluate**: A response to the instruction that the evaluator LM has to evaluate.
		3. **Customized Score Rubric**: A specification of *novel criteria, designed by the user*. The evaluator should focus on this aspect during evaluation. The rubric consists of (1) a description of the criteria, and (2) a description of each scoring decision (1 to 5).
		4. **Reference Answer**: *A reference answer that would receive a score of 5*. Instead of requiring the evaluator LM to solve this instruction, it enables the evaluator to use the mutual information between the reference answer and the response to make a scoring decision.
			- ((I guess this means that we can't "one-shot" generations, if this is going to be done at inference time too; eg we need to provide an example of this instruction/rubric pair being "solved", before we use Prometheus to evaluate it.))
	- ==Components of an output==
		1. **Feedback**: A rationale of why the provided response should receive a particular score. This is analogous to [[Chain of Thought|CoT]], making the evaluation process "interpretable"
		2. **Score**: An integer score for the provided response that ranges from 1 to 5.
	- Dataset Construction Process
		1. **Curation of 50 initial seed rubrics**: The authors curate an initial detailed and fine-grained scoring rubrics.
		2. **Expansion of 1k new score rubrics through GPT-4**: Expand the score rubrics from the initial 50 to a more robust and diverse set of 1000 score rubrics. Specifically, sampling 4 random original/human-written score rubrics and use them as [[Few-Shot Prompting]] demonstrations for GPT-4 to brainstorm new novel score rubrics. Also prompt GPT-4 to *paraphrase* the newly generated rubrics to help Prometheus generalize to different wordings.
		3. **Augmentation of realistic instructions**: With a comprehensive dataset of 1,000 rubrics constructed, now we need to create N pertinent training instances for each. We prompt GPT-4 to generate 20k unique instructions (20 per rubric) that are highly relevant to each given score rubric.
		4. **Augmentation of remaining components** in training instances (responses, including reference answers, feedback, scores): Sequentially generate responses and feedbacks by asking GPT-4 to generate each component that will get a score of (1..5). To eliminate the effect of decision bias when fine-tuning our evaluator LM, we generate an equal number of responses (20k) for each score (1..5). Note that for the response with a score of 5, we generate *two* distinctive responses, so we can use one of them as an input.
- [[Prometheus]]
	- We finetune Llama-2-Chat-7B and Llama-2-Chat-14B to obtain Prometheus.
	- Similar to ==Chain-of-Thought Fine-Tuning==, we fine-tune to sequentially generate the *feedback*, and then the *score*. We ==highlight== that it's important to include a phrase like `[RESULT]` inbetween the feedback and score to prevent degeneration during inference.
- Authors test Prometheus on both Absolute Evaluation/Grading and on Ranking Grading
	- ==**Absolute Grading**==: Where the evaluator should generate a feedback and score within the range of 1..5 to given an instruction, a response to evaluate, and reference materials. The evaluator *does not* have any access to an opponent to compare with, and is required to provide a score *solely* based on internal decision.
	- **==Ranking Grading==**: To estimate if an evaluator trained only on Absolute grading could then be utilized as a universal reward model based on *any* criteria. The biggest challenge of employing an evaluator LM trained in an Absolute Grading setting and then evaluated in a Ranking Grading setting is that it could give the same score for both candidates. *==Therefore we use a temperature of 1.0 when evaluating each candidate independently, and iterate until there's a winner.==*
	- The authors really here are just ***curious*** whether an evaluator LM trained in AG could also generalize to RG.
- 

# Abstract

>Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an ==evaluator== for long-form responses has become the de facto standard. ==However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable== due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the ==Feedback Collection==, a ==new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4==. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at [this https URL](https://kaistai.github.io/prometheus/).

# Paper Figures
![[Pasted image 20240508124353.png]]
Above: See that rather than testing some specific domain/task or assessing a score based on harmfulness and helpfulness, we're providing a human-written scoring rubric and asking an LLM to evaluate by that criteria. Specifically, the authors train an open-source, smaller model to do the evaluation, rather than using an expensive alternative like GPT-4.

![[Pasted image 20240508154551.png]]
Above: A great image of the augmentation process used to construct Feedback Collection.

![[Pasted image 20240508155511.png]]
Above: I like this distinction between "Absolute Evaluation" and "Ranking Evaluation".

![[Pasted image 20240508160512.png]]
Above: This seems to show that Prometheus ***feedbacks*** (CoT explanations, pre-score) are even preferred relative to GPT-4 feedbacks, when it comes to human annotators assessment of which ***feedback*** is better.
- While GPT-4 was mainly not chosen due to providing general or abstract feedback, Prometheus was mainly not chosen because it was ***too*** critical about the given response! Given that Prometheus was mostly distilled from GPT-4 responses, this is interesting to hear...
# Non-Paper Figures