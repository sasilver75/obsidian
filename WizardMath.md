August 18, 2023 (~4 months after [[WizardLM]], 2.5 months after [[Let's Verify Step by Step]])
[[Microsoft Research]]
[WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)
#zotero 
Takeaway: ...

"Interesting in that it introduced the idea of using a [[Process Reward Model]] (PRM) as the reward model in [[Reinforcement Learning from Human Feedback|RLHF]] via [[Proximal Policy Optimization]] (PPO)."

------

## Introduction
- Large-scale LLMs have recently gathered significant attention (GPT, Claude, LLaMA), and releases of MPT, Falcon, StarCoder, Alpaca, Vicuna, LLM, etc. soon followed.
- Methods like [[Chain of Thought|CoT]] propose to design better prompts to generate step-by-step solutions, which can lead to improved performance.
- Refers to the [[Let's Verify Step by Step]] paper that found that *process supervision* with reinforcement learning significantly outperformed outcome supervision for solving challenging [[MATH]] problems.
- Inspired by [[Evol-Instruct]] and [[Process Reward Model]]-supervised RL (a la Let's verify Step by Step), we aim to enhance the math abilities of [[LLaMA 2]].
- Authors propose a new method named ==Reinforcement Learning from Evol-Instruct Feedback== (RLEIF), which can:
	1. Generate math instructions data by math-specific [[Evol-Instruct]]
		- Includes *downward evolution* and *upward evolution* processes to produce grade-school math and challenging math respectively.
		- We regenerate, filter, and finetune the original math instruction data from [[GSM8K]] and [[MATH]].
	2. Train an ==Instruction reward Model (IRM)== *and* a ==Process-Supervised Reward Model (PRM),== where the former indicates the *quality* of the evolved *instruction*, and the latter receives feedback for *each step* in the *solution*.

Contributions
- We introduce the [[WizardMath]] model, which enhances the mathematical reasoning abilities for open-source pretrained LLMs.
- We propose a new method (*[[Reinforcement Learning from Evol-Instruct Feedback]]* (RLEIF)) to improve LLM reasoning performance.

## Method
- [[Reinforcement Learning from Evol-Instruct Feedback]] integrates [[Evol-Instruct]] and reinforced process supervision methods to evolve GSM8K and MATH, and then finetune the pre-trained LLaMA 2 with the evolved data and reward models.
- Our methods apply three steps:
	1. Supervised fine-tuning
	2. Training an Instruction Reward Model (IRM) and Process-Supervised Reward Model (PRM)
	3. Active Evol-Instruct, and PPO training

Supervised Fine-Tuning
- Following [[InstructGPT]], we first finetune the base with supervised instruction-response pairs
	- We few-shot regenerate 15k answers for [[GSM8K]] and [[MATH]] with an Alpha version of [[WizardLM]] 70B to produce solutions in a step-by-step format, filter to those with the correct answer, and use them to finetune the base LLaMA 2 model.
	- We also sample 1.5K open-domain conversations from WizardLM's training data, then merge it with the above math corpus as the final SFT training data.

[[Evol-Instruct]] principles for Math
- This work attempts to adapt WizardLM to make math instructions with various complexities and diversity to enhance the pre-trained LLMs. We introduce:
	- ==Downward evolution==: Enhances instructions by making questions easier
	- ==Upward evolution==: Derived from the original Evol-Instruct, we deep and generate new and harder questions by adding constraints, concretizing, and increasing reasoning.

[[Reinforcement Learning from Evol-Instruct Feedback]]
Inspired by InstructGPT and PRMs, we train *two reward models* to predict the quality of instructions and the correctness of each step in the answer respectively.
- Instruction Reward Model (IRM): Model aims to judge the quality of evolved instructions on three aspects:
	1. Definition
	2. Precision
	3. Integrity
	- To produce ranking list training training data of IRM, for each instruction, they use ChatGPT and Wizard-E to generate 2-4 evolved instructions respectively. Then we leverage Wizard-E to rank the quality of those 4-8 instructions.
- Process-Supervised Reward Model ([[Process Reward Model|PRM]]): There's no simple way to support highly precision process supervision without professional human-labelers... so ==we use ChatGPT to provide process supervision, asking it to assess the correctness of each step in the solutions generated by our model.==
- PPO Training: We evolve the original math (GSM8K + MATH) instructions over 8 turns, increasing the data size from 15k -> 96k.
- ==They use IRM and PRM to generate an *instruction reward* and an *answer reward*, and then we apply the product of these as the final reward $r = r^I \cdot r^A$==  

## Experiment
- (Tables added for the comparison of WizardMath versus other models. It certainly beats other open source models being of the same size).

Authors evaluate each



## Related Work


## Conclusion and Future Work




Abstract
> Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of Llama-2, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. WizardMath surpasses all other open-source LLMs by a substantial margin. Furthermore, our model even outperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and model weights are public at [this https URL](https://github.com/nlpxucan/WizardLM) and [this https URL](https://huggingface.co/WizardLM).


# Paper Figures

![[Pasted image 20240720154638.png|500]]
Illustration of the ==Reinforcement Learning from Evol-Instruct Feedback==:
1. SFT
2. Instruction Reward Model (IRM) training and Process-Supervised Reward Model ([[Process Reward Model|PRM]]) training
4. Active Evol-Instruct and Reinforcement Learning via PPO

![[Pasted image 20240720160929.png|400]]


![[Pasted image 20240720164322.png|400]]
Performance of WizardMath compared to alternatives (red in terms of comparison to the "baseline model with similar parameter size")

![[Pasted image 20240720164410.png|600]]
Results of WizardMath 70B on each subset of [[MATH]]
