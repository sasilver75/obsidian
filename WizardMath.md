August 18, 2023 (~4 months after [[WizardLM]], 2.5 months after [[Let's Verify Step by Step]])
[[Microsoft Research]]
[WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)
#zotero 
Takeaway: Authors finetune [[LLaMA 2]] (7B, 13B, 70B) on An [[Evol-Instruct]]-amplified/altered dataset of [[GSM8K]] and [[MATH]] (With directed downward evolution and upward evolution or data) .They use a version of [[Proximal Policy Optimization|PPO]] based on a composite reward from both an *Instruction Reward Model (IRM)* and a *Process-supervised Reward Model (PRM)*. 
- For the [[Process Reward Model|PRM]], there's no way that the authors could support a large professional human data annotator effort, so the authors use ChatGPT to provide process supervision, asking it to assess the correctness of each step in the solutions generated by the model. Annoyingly it doesn't give *more* information (even in appendices) on what sort of labeling technique was used (Is it 0/1? Is it positive/negative/neutral? Is it scalar?)
- The Instruction Reward Model training process aims to judge the quality of Evol-Instruct-generated instructions on three aspects (Definition, Precision, Integrity). They use ChatGPT and Wizard-E to generate 2-4 evolved instruction each, then use Wizard-E to rank the quality of those 4-8  generations.

"Interesting in that it introduced the idea of using a [[Process Reward Model]] (PRM) as (one of) the reward model in [[Reinforcement Learning from Human Feedback|RLHF]] via [[Proximal Policy Optimization]] (PPO)."

------

## Introduction
- Large-scale LLMs have recently gathered significant attention (GPT, Claude, LLaMA), and releases of MPT, Falcon, StarCoder, Alpaca, Vicuna, LLM, etc. soon followed.
- Methods like [[Chain of Thought|CoT]] propose to design better prompts to generate step-by-step solutions, which can lead to improved performance.
- Refers to the [[Let's Verify Step by Step]] paper that found that *process supervision* with reinforcement learning significantly outperformed outcome supervision for solving challenging [[MATH]] problems.
- Inspired by [[Evol-Instruct]] and [[Process Reward Model]]-supervised RL (a la Let's verify Step by Step), we aim to enhance the math abilities of [[LLaMA 2]].
- Authors propose a new method named ==Reinforcement Learning from Evol-Instruct Feedback== (RLEIF), which can:
	1. Generate math instructions data by math-specific [[Evol-Instruct]]
		- Includes *downward evolution* and *upward evolution* processes to produce grade-school math and challenging math respectively.
		- We regenerate, filter, and finetune the original math instruction data from [[GSM8K]] and [[MATH]].
	2. Train an ==Instruction reward Model (IRM)== *and* a ==Process-Supervised Reward Model (PRM),== where the former indicates the *quality* of the evolved *instruction*, and the latter receives feedback for *each step* in the *solution*.

Contributions
- We introduce the [[WizardMath]] model, which enhances the mathematical reasoning abilities for open-source pretrained LLMs.
- We propose a new method (*[[Reinforcement Learning from Evol-Instruct Feedback]]* (RLEIF)) to improve LLM reasoning performance.

## Method
- [[Reinforcement Learning from Evol-Instruct Feedback]] integrates [[Evol-Instruct]] and reinforced process supervision methods to evolve GSM8K and MATH, and then finetune the pre-trained LLaMA 2 with the evolved data and reward models.
- Our methods apply three steps:
	1. Supervised fine-tuning
	2. Training an Instruction Reward Model (IRM) and Process-Supervised Reward Model (PRM)
	3. Active Evol-Instruct, and PPO training

Supervised Fine-Tuning
- Following [[InstructGPT]], we first finetune the base with supervised instruction-response pairs
	- We few-shot regenerate 15k answers for [[GSM8K]] and [[MATH]] with an Alpha version of [[WizardLM]] 70B to produce solutions in a step-by-step format, filter to those with the correct answer, and use them to finetune the base LLaMA 2 model.
	- We also sample 1.5K open-domain conversations from WizardLM's training data, then merge it with the above math corpus as the final SFT training data.

[[Evol-Instruct]] principles for Math
- This work attempts to adapt WizardLM to make math instructions with various complexities and diversity to enhance the pre-trained LLMs. We introduce:
	- ==Downward evolution==: Enhances instructions by making questions easier
	- ==Upward evolution==: Derived from the original Evol-Instruct, we deep and generate new and harder questions by adding constraints, concretizing, and increasing reasoning.

[[Reinforcement Learning from Evol-Instruct Feedback]]
Inspired by InstructGPT and PRMs, we train *two reward models* to predict the quality of instructions and the correctness of each step in the answer respectively.
- Instruction Reward Model (IRM): Model aims to judge the quality of evolved instructions on three aspects:
	1. Definition
	2. Precision
	3. Integrity
	- To produce ranking list training training data of IRM, for each instruction, they use ChatGPT and Wizard-E to generate 2-4 evolved instructions respectively. Then we leverage Wizard-E to rank the quality of those 4-8 instructions.
- Process-Supervised Reward Model ([[Process Reward Model|PRM]]): There's no simple way to support highly precision process supervision without professional human-labelers... so ==we use ChatGPT to provide process supervision, asking it to assess the correctness of each step in the solutions generated by our model.==
	- (())
- PPO Training: We evolve the original math (GSM8K + MATH) instructions over 8 turns, increasing the data size from 15k -> 96k.
- ==They use IRM and PRM to generate an *instruction reward* and an *answer reward*, and then we apply the product of these as the final reward $r = r^I \cdot r^A$==  

## Experiment
- (Tables added for the comparison of WizardMath versus other models. It certainly beats other open source models being of the same size, with WizardMath 70B even slightly outperforming some close-source LLMs on GSM8K, like ChatGPT, PaLM 2 540B, etc.).

Authors evaluate each model on GSM8K and MATH using the following zero-shot CoT prompt:
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response: Let’s think step by step.
```

Wizard 7B surpasses most open-source models with parameter counts ranging from 7B-40B

WizardMath 13B outperforms PaLM 1 540B, Minerva 540B, GPT-3.5 on GSM8K, and PaLM 1 540B, GPT-3 175B on MATH. Significantly superior to LLaMA-65B and LLaMA 2 70B on GSM8K and MATH.

WizardMath70B is on-par/superior with Claude Instant, ChatGPT, PaLM 2 on GSM8K. Beats Text-davinci-002 by a margin of 3.6% on MATH benchmark. Surpasses LLAMA 2 70B (81.6 v 56.8) by a significant amount on GSM8K and (22.7 v 13.5) MATH.

## Related Work
- ...



Abstract
> Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of Llama-2, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. WizardMath surpasses all other open-source LLMs by a substantial margin. Furthermore, our model even outperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and model weights are public at [this https URL](https://github.com/nlpxucan/WizardLM) and [this https URL](https://huggingface.co/WizardLM).


# Paper Figures

![[Pasted image 20240720154638.png|500]]
Illustration of the ==Reinforcement Learning from Evol-Instruct Feedback==:
1. SFT
2. Instruction Reward Model (IRM) training and Process-Supervised Reward Model ([[Process Reward Model|PRM]]) training
4. Active Evol-Instruct and Reinforcement Learning via PPO

![[Pasted image 20240720160929.png|400]]


![[Pasted image 20240720164322.png|400]]
Performance of WizardMath compared to alternatives (red in terms of comparison to the "baseline model with similar parameter size")

![[Pasted image 20240720164410.png|600]]
Results of WizardMath 70B on each subset of [[MATH]]

