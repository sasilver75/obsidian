Regularization is a general term for a set of methods that aim to reduce [[Overfitting]] in machine learning models. 

They sometimes make the models less flexible (eg by an objective function penalty term), but that's isn't always the case (eg [[Dropout]]).

Typically, regularization trades a marginal decrease in training accuracy for an increase in generalizability.

[[L1 Regularization]] (Lasso Regularization)
[[L2 Regularization]] (Ridge Regularization)
[[Dropout]]
[[Early Stopping]]