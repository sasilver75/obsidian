---
aliases:
  - Episode
  - Trajectories
---
Sequence of state, action, rewards that are taken/observed during an episode/rollout.

$(S_0, A_0, R_1, S_1, R_2, S_2, A_2, ..., R_T, S_T)$

In an episodic [[Markov Decision Process]] with a known policy, these can be generated by simply sampling from our policy $\pi$ until termination.

Our goal is generally to maximize the $\gamma$-discounted sum of future rewards (the *return*) to be high, when averaged over many trajectories. This expected return for a state is called the [[Value Function]] for a state. Our goal is to learn an optimal policy $\pi_*$ that results in the optimal value function $v_{*}$ for every state $s \in S$.