---
aliases: Text-to-Text Transfer Transformer
---

October 23, 2019 -- [[Google Research]] (Includes [[Noam Shazeer]] and others)
Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) #zotero 
Takeaway: We try a bunch of hyperparameters, etc. regarding training encoder-decoder models on a unified text to text format. We built the T5 model with what works, and it's SoTA!

----


Trained on the [[C4]] dataset (Colossal, Cleaned Common Crawl), which was also developed and released in this paper. Supposed a great paper to read -- they did a 

T5 combines insights from a wide-ranging and detailed experimental study of optimal training objectives/architectures/datasets/hyperparameters, and combines that with unprecedented scale.

Details
- 220M parameter model pretrained on 34B tokens, very standard *Attention is All You Need* structure.
- [[SentencePiece]] tokenization to create [[WordPiece]] tokens; vocabulary size 32k


Results
- Encoder-Decoder architecture performed better on a denoising objective than the LM or Prefix LM did.
- 

See also: [[FLAN-T5]], [[Pile-T5]]
- The FLAN-T5 models are [[Instruction-Tuning|Instruction-Tuned]].

Abstract
> Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, ==we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format==. Our ==systematic study== ==compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks==. By combining the insights from our exploration with scale and our new [[C4|Colossal Clean Crawled Corpus]] (C4) dataset, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.

![[Pasted image 20240426162100.png]]
Above: With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.

![[Pasted image 20240425180847.png]]
![[Pasted image 20240426171903.png]]
![[Pasted image 20240426172742.png]]
![[Pasted image 20240426172930.png]]
![[Pasted image 20240426174027.png]]
![[Pasted image 20240426175811.png]]

![[Pasted image 20240426175345.png]]
Experiment varying corruption rate
![[Pasted image 20240426175407.png]]
Experiment varying corruption span lengths; turns out spans of 2-5 helped.
![[Pasted image 20240426180209.png]]
![[Pasted image 20240426180224.png]]
Interesting that WebText did so well; WebText is Reddit posts with more than 3 upvotes, I believe. The main lesson is that pretraining on in-domain unlabeled data can improve performance on downstream tasks. They also find that pretraining on the full dataset is better than (eg) training on half the dataset, with two epochs.

![[Pasted image 20240426182417.png]]
Interesting that [[Gradual Unfreezing]] was mentioned here; pity it didn't do especially well.

![[Pasted image 20240426182633.png]] 
![[Pasted image 20240426183526.png]]



# Non-Paper Figures
![[Pasted image 20240619172726.png]]