---
aliases: Text-to-Text Transfer Transformer
---

October 23, 2019
Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)

Trained on the [[C4]] dataset (Colossal, Cleaned Common Crawl), which was also developed and released in this paper. Supposed a great paper to read -- they did a detailed study of various training objectives/architectures/datasets/hyperparameters.

See also: [[FLAN-T5]], [[Pile-T5]]
- The FLAN-T5 models are [[Instruction-Tuning|Instruction-Tuned]].

Abstract
> Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, ==we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format==. Our ==systematic study== ==compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks==. By combining the insights from our exploration with scale and our new [[C4|Colossal Clean Crawled Corpus]] (C4) dataset, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.

![[Pasted image 20240425180847.png]]
