---
aliases:
  - MoE
---
Related:
- [[Expert Capacity]]
- [[Capacity Factor]]

Examples (roughly chronological):
- Hinton/Jordan's "Adaptive Mixtures of Local Experts"
- Shazeer's 2017 "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
- [[GShard]]
- [[Switch Transformer]]
- [[GLaM]]
