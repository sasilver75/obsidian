
December 31, 2020 -- [[Eleuther]]
Paper: [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027
Website: [The Pile](https://pile.eleuther.ai/)

An 825 GiB Dataset for open source language modeling. Consists of 22 smaller, high-quality datasets combined together, such as [[Common Crawl]], [[OpenWebText]], books, and papers.

Abstract
> Recent work has demonstrated that ==increased training dataset diversity improves== general cross-domain knowledge and downstream generalization capability for large-scale ==language models==. With this in mind, we present the Pile: an ==825 GiB English text corpus== targeted at training large-scale language models. The Pile is ==constructed from 22 diverse high-quality subsets== -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.

![[Pasted image 20240419210949.png]]
