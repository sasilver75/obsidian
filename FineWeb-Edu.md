FineWeb Blogpost: [Link](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
HuggingFace Dataset: [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)

Takeaway: Authors used LLaMa3-70B-Instruct to annotate 500k samples from [[FineWeb]], then trained a small classifier to predict these scores. They they applied the classifier to 15T tokens of FineWeb, requiring 6,000 H100 GPU hours and yielding 1.3T tokens. The result is a very high-performance dataset.

Note: Authors also release a version of FineWeb that retained more data, using a threshold of 2/5, rather than 3/5 on the Likert scale. This dataset has 5.4T tokens and is available [here](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu-score-2)
- The trained classifier is available [here](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier)

---


A dataset from [[HuggingFace]] with ==1.3T tokens==, containing only web pages with "very high educational content." We used¬†[Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)¬†==to annotate 500k samples from üç∑ FineWeb==, scoring each for their educational quality on a scale from 0 to 5.

> *"Outperforms all openly released web-scale datasets by a significant margin on knowledge- and reasoning-intensive benchmarks like MMLU, ARC, and OpenBookQA."*
> - Thomas Wolf, Cofounder @ HuggingFace

Based on a new approach that has recently emerged for filtering LLM datasets: ==using synthetic data to develop classifiers for identifying educational content!==
- This was notably used in the training of [[LLaMA 3]] and [[Phi-3]]

> "*Our training data consists of heavily-filtered publicly-available web data (according to the 'educational level') from various open internet sources, as well as synthetic LLM-generated data.*"
> - Phi-3 Paper

> *"We found that previous generations of LLaMA are good at identifying high-quality data, so we used LLaMA 2 to help build the text-quality classifiers that are powering LLaMA 3.*
> - LLaMA 3 Blog Post

Here, HuggingFace develops their own educational quality classifier using *annotations* generated by LLAMA-3-70B-Instruct.
- They had experimented with Mistral-8x7B-Instruct and Mixtral-8x22B-Instruct, as well as ensembles of these, but found that LLaMA3 alone gave the best results.

Authors explored various prompt formats to extract educational scores using an LLM and found that the additive scale used by Yuan et al. in the [[Self-Reward]]  paper worked best.
- This scale allows the LLM to reason about each additional point awarded, unlike the single-rating Likert scale which fits samples into predefined boxes.
- We set a threshold of 3 (on a scale of 0 to 5) during the filtering process, and retain high-level educational pages.

![[Pasted image 20240605140528.png]]
Above: The prompt used! It's interesting to me that (assumedly) a 5-point document should tick all these boxes, but the second one (eg) says "It might...offer a superficial overview." I guess a lot is weighing on that "might", right?

They used these annotations to train a small classifier, available [here](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier)
- The used a Snowflake-arctic-embed embedding model with a classificiation head with a single ***regression output*** on top of it. 
- They trained it with 450,000 annotations for 20 epochs with a learning rate of 3e-4, freezing the embedding and encoder layers. They used the checkpoint with the highest F1 score on the held-out validation set of 45k samples, treating LLaMA 3 annotations as ground-truth.
- After training, they rounded the scores to integers from 0 to 5.
- They then converted the problem to a binary classification task by using a fixed threshold to determine if a file is educational. With a threshold of 3, the model achieved an F1 score of 82% on the validation set.

They they applied the classifier to 15T tokens of FineWeb, requiring 6,000 H100 GPU hours.

Conclusion
- üìö FineWeb-Edu¬†**surpasses üç∑ FineWeb and all other open web datasets, with remarkable improvements on educational benchmarks**¬†such as MMLU, ARC, and OpenBookQA.
- It achieves the same performance with significantly less data, requiring 10x fewer tokens compared to C4 and Dolma to match MMLU results.
- This demonstrates the effectiveness of using classifiers trained on LLM annotations for large-scale data filtering.

