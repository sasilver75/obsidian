March 10, 2022
[[DeepMind]]
Paper: [Internet-augmented language models through few-shot prompting for open-domain question answering](https://arxiv.org/abs/2203.05115)
#zotero 

Takeaway: A system for [[Open-Domain]] [[Question Answering]] with a retrieval system that uses Google Search (rather than Wikipedia) on a mix of single-hop and multi-hop tasks.
- It seems like they fetch a bunch of relevant passages from Google (search, then scraping and chunking the top 20 links), and then use few-shot prompting to set up the question, using k=15 (Evidence, Question, Answer) pairs.
	- Not sure how they make the Question/Answer, given a chunk of Evidence
	- Then they sample some subset of retrieved passages (n=1?) and generate responses, and then use a scoring mechanism to rate which one to trust.

----

Notes
- The approach that they propose to use few-shot prompting as a flexible and robust way to condition pre-trained LMs on external evidence has three steps:
	1. Given a question, we *retrieve* a set of relevant documents from the web using a search engine.
	2. Use the retrieved evidence to condition the LM through [[Few-Shot Prompting]]
	3. Generate multiple *candidate answers* from each evidence and *rerank them using scores computed from the same LM*, to find the best answer.

Abstract
> In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. ==Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence,== we ==use few-shot prompting to learn to condition LMs on information returned from the web using Google Search==, a broad and constantly updated knowledge source. Our approach ==does not involve fine-tuning or learning additional parameters,== thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. ==Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance== and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.