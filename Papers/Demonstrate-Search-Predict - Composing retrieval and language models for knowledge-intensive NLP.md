Dec 28, 2022
[[Omar Khattab]], Mattei Zaharia, [[Christopher Potts|Chris Potts]], [[Percy Liang]] et al.
Paper: https://arxiv.org/abs/2212.14024
#zotero 
Takeaway: ...

---

Notes:
- The Demonstrate-Search-Predict (DSP) framework for in-context learning relies entirely on passing natural language text between a frozen RM and a frozen LM. It introduces a number of composable *functions* that:
	1. Bootstrap training examples (Demonstrate)
	2. Gather information from a knowledge corpus (Search)
	3. Generate grounded outputs (Predict)
- Although DSP programs implement behaviors like query generation, it requires no hand-labeled examples of these intermediate *transformations*; instead, the Demonstrate stage uses labeled question-answer pairs to implement a form of [[Weak Supervision]] that programmatically annotates the transformations that are invoked within Search and Predict.
- In our pipelines, we reliably rewrite questions to resolve conversational dependencies and iteratively decompose complex queries with summarization of intermediate hops, before generating grounded responses from multiple passages with self-consistency.
- Language Model
	- The LM generates all of:
		- Exemplar queries that illustrate how to produce queries for questions in the training set (Demonstrate stage)
		- The intermediate "hop" queries to find useful information for the input question (Search stage)
		- The final answer to the input question (in the Predict stage)
- Retrieval Model
	- DSP programs invoke a frozen retrieval model to retrieve the top $k$ most "relevant" text sequences for a given query. The RM accepts free-form textual inputs and specializes in estimating the relevance of stored text sequences to a query.
	- RM is response for:
		- Retrieving passages for each query generated by the LM during the Search stage.
		- Also for passages that are used within demonstrates in the Demonstrate stage.
- DSP framework
	- Let's talk about the core datatypes and composable functions provided by the Python framework:
		- `Example` datatype
			- To conduct a task, a DSP program manipulate one or more instances of the `Example` datatype; these behave like Python dictionaries with multiple fields.
			- ![[Pasted image 20240517143025.png]]
			- We typically assume that human-labeled training data does *not* include labels for intermediate transformations (eg queries for individual hops), and so it's the job of the DSP program to discover these strategies via in-context learning.
		- A DSP `program`
			- Takes the input (eg a question) and outputs the system output (its short answer)
			- ![[Pasted image 20240517143138.png]]
			- Starts by creating an Example for the input question, and assigning the train field to the training set from the previous snippet. We then use some DSP primitives (built-in-functions) to build the demonstrate/search/predict formations that define the program.
		- A `transformation`
			- is a function that takes an `Example` as input and returns an `Example` , populating new fields (or modifying existing fields) in it. Above, we used `multihop_demonstrate, multihop_search, multihop_predict`. Transformations may themselves invoke other transformations.
- Demonstrate
	- In DSP, a *demonstration* is a training example that's been prepared to illustrate specific desired behavior to the LM.
	- A Demonstrate transformation takes an input `x` of type `Example` and prepares a list of demonstrations in `x.demos`m typically by selecting a subset of the training examples in `x.train` and *bootstrapping* new fields in them.
	- Examples in the training set typically consist of input text and target output of a task; the Demonstrate stage can augment a training example by programmatically bootstrapping annotations for intermediate transformations that illustrate:
		- How to break down the input question to gather information for answering it.
		- How to use information gathered in an earlier "hop" to ask follow-up questions
		- How to use the information gathered to answer complex questions.
	- The `annotate` primitive accepts a user-defined transformation `fn` and applies it over a list of training examples -- whenever `fn` returns an example (rather than `None`), `annotate` caches the intermediate predictions (i.e. the generated queries and retrieved passages). These serve as successful demonstrations for the later pipeline transformations (invoking the Search/Predict stages).
	- ![[Pasted image 20240517144737.png]]
	- Above: `multihop_demonsrate` invokes `annotate`, which bootstraps missing fields in training examples by caching annotations from `attempt_example`.
	- Demonstrate transforms a training question-answer pair into a fully-populated demonstration, including fields like hop1 and hop2 (queries for multi-hop search) as well as psg1 and psg2 (retrieved passages).
	- By bootstrapping pipelines, Demonstrate makes it easy to explore complex strategies in Search and Predict without writing examples for every transformation.
	- It's not always possible to fit all training examples into the context window of an LM; DSP provides three primitives for selecting a subset of training examples, namely using the `sample`, `knn`, and `crossval` strategies.
- Search
	- The search stage gathers passages to support transformations conducted by hte LM.
	- Assuming a large knowledge corpus divided into text passages, Search involves our LM directly querying the RM, requesting the top-k passages that match an input question. 
	- ![[Pasted image 20240517145710.png]]
	- In many scenarios, we ned more sophisticated search strategies that empower the RM to find relevant passages using (eg) multi-hop reasoning.
	- Supported with automatic annotations from Demonstrate, the Search stage allows us to simulate many such strategies and many others in terms of passing queries, passages, and demonstrations between the RM and LM.
	- Search facilitates our vision of advanced strategies in which the LM and RM co-operate to incrementally plan a research path during which the RM gathers information and the LM identifies the next steps.
	- Note that [[Self-Ask]] can be thought of as a simple instantiation of DSP's Search stage! DSP can express ideas like Self-Ask and many other, more sophisticated pipelines -- DSP offer a number of intrinsic advantages that lead to large empirical gains: 80-290% over Self-Ask.
	- For improved recall and robustness, ew can also *fuse* the retrieval across multiple generated queries! Fusion has a long history in IR; DSP uses a variant of CombSUM (Fox and Shaw, 1994), assigning each passage the sum of its probabilities across retrieval lists.
	- We can also equip our pipelines with a query rewriting step, which produces a query that encompasses all of the relevant conversational context.
- Predict
	- The predict stage generates the system output using demonstrations (eg in x.demos) and passages (eg in x.context). It has the function of systematically aggregating information among a large number of demonstrations, passages, and candidate predictions.
	- Generating Candidates
		- Generally, Predict has to produce 1+ candidate predictions for the end-task. 
		- The basic primitive is `generate`, which accepts a `Template` and (via currying) an `Example` and queries the LM to produce one or more completions.
		- A corresponding primitive that uses the RM in this stage is `rank`, which accepts a query and one or more passages and returns their relevance scores.
		- A `Template` is an object that can produce prompts; it can map an `Example` to a string, and extract fields out of completions. 
			- It can produce:![[Pasted image 20240517152535.png]]
		- Above: The LM is asked to generate a CoT rationale and an answer, and the generated text will be extracted back into the `rationale` and `answer` keys of each completion.
	- After producing multiple candidates, we can simply extract the most popular prediction -- this is the [[Self-Consistency]] method, which seeks to identify predictions at which multiple distinct rationales arrive.
		- DSP generalizes this in two ways:
			- We can sample multiple "pipelines of transformations" (PoT) within the program.
			- When sampling our CoTs or PoTs provides multiple candidates, we can select the top-k predictions and then compare them directly. We can also invoke the RM via `rank` to find the prediction that's most grounded in a retrieved contexts, or, given an RM that can score completions, simply the prediction that has the highest score given the prompt.
- Conclusion
	- The promise of in-context learning is that we can build complex systems from pretrained components using only natural language as the medium for giving systems instruction, and, as we argue for, allowing components to communicate with eachother. In this new paradigm, the building blocks are pretrained models, and the core operations are natural language instructions.
	- DSP consists of a number of simple, composable functions for implementing in-context learning systems as deliberate *programs* instead of end-task prompts.

Abstract
> Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “==retrieve-then-read==” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose ==DEMONSTRATE-SEARCH–PREDICT (DSP),== a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can ==express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably==. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and ==delivering 37–120%, 8–39%, and 80–290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively==. We release DSP at https: //github.com/stanfordnlp/dsp.


# Paper Figures
![[Pasted image 20240517140135.png]]
Above: Simple "single retrieve-then-read" is often not enough to answer questions, if your documents don't have the entire answer in them outright -- you need multi-hop! 

![[Pasted image 20240517140321.png]]
Above: 
- Given an input question and 2-shot training set, the ==Demonstrate== phase annotates intermediate transformations on the training examples using a form of weak supervision, creating *demonstrations*. ((It colors in the middle of what the search/predict iterations between Q/A might look like?))
- Learning from the resulting demonstrations, the ==Search== phase decomposes the complex input question and retrieves supporting information over two retrieval hops.
- The ==Predict== stage uses the demonstration and retrieved passages to answer the question.