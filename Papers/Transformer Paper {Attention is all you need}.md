---
tags:
  - paper
---

Paper: https://arxiv.org/abs/1706.03762
Date: June 2017
Authors: Ashish Vaswani, Noam Shazeer,  Niki Parmer, Jakob Uszkoreit, Lilion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin


See prior work:
- _**Neural Machine Translation by Jointly Learning to Align and Translate**_Â **(2014)**, which introduced an attention mechanism for RNNs; motivated this paper.

Summary: The paper introduces the original [[Transformer]] architecture, with an [[Encoder-Decoder Architecture]] that will become additionally relevant as separate modules later. Introduces scaled dot product attention, [[Multi-Head Attention]] blocks, and positional input encoding.

-------
