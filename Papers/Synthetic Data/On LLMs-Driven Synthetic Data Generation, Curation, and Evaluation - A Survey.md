June 14, 2024
Zhejiang University and Harbin Institute of Technology
[On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](https://arxiv.org/abs/2406.15126v1)
#zotero 
Takeaway: ...

Not a very good paper. Didn't really teach me anything. Ignore.

---

## Introduction
- A large amount of high-quality, diverse data remains the foundation for building robust models.
- Fulfilling such data reliance with human data is unrealistic due to high costs, data scarcity, privacy concerns... and human-generated data might not even be optimal for training/evaluation!
- LLMs ability to act as a repository of knowledge/linguistic comprehension *and* their ability to follow instructions make LLMs highly-promising synthetic data generators.
- [[TinyStories]] and [[Phi-1]] demonstrated that data quality is crucial fore effective model learning.
	- [[Alpaca]], [[Vicuna]], [[OpenHermes2.5 Dataset]], [[Openchat 3.5]] leveraged high-quality synthetic data for training.
- Generating synthetic datasets with high correctness and sufficient diversity involves a lot of tricks, making LL-driven synthetic data generation a non-trivial problem.
	- This survey investigates recent studies and organizes them according to the topics of generation, curation, and evaluation, which are all closely-related.

## Preliminaries
- Data annotation as a specialized paradigm of synthetic data generation has particularly extensive applicability, including [[Reinforcement Learning from from AI Feedback|RLAIF]] and LLM-based evaluation.
- Goal is to generate data that closely aligns with evaluation metrics. 
- There are two general requirements that are seen to be challenging:
	- ==Faithfulness==: generated data must be logically and grammatically correct. This becomes harder when generating long, complex, or domain-specific data.
	- ==Diversity==: Diversity captures the variation among generated data, reflecting differences in text length, topic, or writing style. It's important for generated synthetic samples to mimic the diversified nature of real-world data, thereby preventing overfitting and bias during model training or evaluation.

## Generic Workflow
- We systematically summarize some of the common practices for data generation with LLMs, which can be roughly divided into *prompt engineering* and *multi-step generation*.

- An effective prompt generally contains three elements:
	- Task Specification ("What I need you to do")
		- specifies necessary contexts, like task purpose, data explanation, roleplay
		- "Suppose you are a {xxx}" can significantly improve LLM performance
	- Generation Conditions ("How I need you to do it")
		- format clarification, knowledge augmentation
	- In-Context Demonstrations ("Examples")
- Directly prompting a LM for examples is going to result in poor diversity, often with highly repetitive outputs, even with a high decoding temperature.
- ==Conditional prompting== is a widely-adopted strategy, which explicitly and concretely communicates to the LLM the specific type of data desired.
	- eg have one prompt that selects generates a number of emotions, and then have another prompt that generates a movie review, conditioned on some emotion selected from the previous response.
	- Allows for better control over diversity.
- Focus on Conditional Prompting centers on the following two subjects:
	- Conditioning Scope (Should we use finger-grained attributes (topics, lengths, style) for more diversified generation? Should we incorporate three randomly chosen words into a generated story?)
	- Conditioning Values (There are instances where an instance pool of classes/labels to sample from isn't available; Ding et al 2023b constructed a concept tree to delve into different subtopics, ensuring coverage of sampled conditioning values...)
- In-context Learning
	- A straightforward yet effective strategy is to provide several demonstrations, which serve as a form of guidance.
	- In scenarios where ground-truth data isn't easily available, approaches like [[Self-Instruct]] and Slef-Prompting instead leverage ICL with synthetic demonstrations generated by LLMs.
	- Quality of in-context samples significantly affects the effectiveness of in-context learning. 
		- Some papers prioritize diversity of in-context examples, while other explore using consistent samples as demonstrative examples based on cosine similarity in embedding space.
- Multi-Step Generation
	- It's unrealistic to expect LLms ot generate the entire desired dataset within a single reference, especially when targeting data with complex structures or semantics.
	- A common strategy is multi-step generation, through which the overall generation process is manually-decomposed into a chain of simpler sub-tasks.
	- Each intermediate output is often used in subsequent generations, in some way.
	- ==Sample-Wise Decomposition==: For things like dialogues and entity-relation triplets, a straightforward approach is to divide samples into smaller chunks and generate only a portion of each sample at a time. So we might prompt the LLM to alternate acting between the assistant and user, replying to eachother based on the context, ultimately producing a complete conversation transcript (using separate instructions for each side).
	- ==Dataset-Wise Decomposition==: 
		- Generating a series of such data that can eventually form a dataset with good diversity and domain-coverage requires long-term scheduling!
		- Dataset-wise task decomposition dynamically adjusts conditions used at each stage of multi-step generation to ensure the overall dataset grows in the right direction.
		- We might target the most frequently-mislabeled categories at each iteration, according to performance of the downstream model trained on previously generated data.
- Data Curation
	- After preceding steps, one may excessively generate overflowing and theoretically unlimited data.
	- These datasets often comprise a considerable portion of noisy, worthless, or even toxic samples, stemming from both LLM hallucinations and ineffective prompts with ambiguous descriptions.
	- Many data curation approaches have been studied, which mostly fall into the categories of *==high-quality sample filtering==* and *==label enhancement==*.
	- ==High-Quality Sample Filtering==:
		- Sample filtering aims to weed out undesired low-quality samples; typically relying on heuristic criteria or re-weighting functions to rerank samples for filtering.
		- Heuristic metrics: The key is to design criteria based on the learning dynamics, like confidence score, influence function, and generation ability. SuperGen employs estimated generation probability to identify samples most related to the desired label. For model-based, they call out [[AlpaGasus]] training on much smaller but curated dataset, surpassing the original Alpaca cross several benchmarks.
		- Sample Reweighting: Believes all data is valuable, but with varying importance; they assign larger weights to correctly-annotated of influential samples during downstream utilization
	- ==Label Enhancement==:
		- Strive to rectify the the potentially erroneous annotations in generated samples. Relyo n either human intervention, or incorporate a student model for human-free knowledge distillation.
		- Human Intervention: Include human-efforts to relabel corrupted samples; perhaps selecting samples with the lowest confidence for human relabeling. Important to compare annotations from humans and LLMs guided by the same codebook/instructions. Can be unrealistically expensive.
		- Auxiliary Model: These methods rely on the weakly-supervised abilities of student models, and hypothesize that a student distilled from the teacher can produce superior labels. ((This doesn't really make sense to me))
	- Data Evaluation: It's important to evaluate the quality and application effectiveness of the data (re: downstream tasks). There are indirect and direct ways of evaluating this:
		- Direct Evaluation: 
			- For open-ended data, human-based evaluation is needed. We might provide some generated samples to human experts who rate them, helping us estimate overall generation quality. A reliable auxiliary model can be leveraged for a more cost-effective and comprehensive evaluation of generated data.
			- Quantification of data diversity primarily employs vocabulary statistics (eg N-gram frequency) and sample relevance calculations... but these struggle to capture semantic information of a dataset. The most common measures of sample correlation are based on cosine similarity and sample distance, which can better capture the contextual and semantic diversity of the dataset.
		- Indirect Evaluation
			- Performance on downstream benchmarks can reflect generation quality.
			- For open-ended benchmarks, evaluation by humans or auxiliary models is needed due to the absence of standardized answers.


## Future Directions
Complex Task Decomposition
- There's still a lack of systematic investigation on how to activate reasoning and planning capabilities for synthetic data generation.
- Agentic data generation
Knowledge Enhancement
- LLM knowledge is long-tailed and biased... without specific domain knowledge, they'll hallucinate. 
- Developing automated prompt conditioning controls directly on mature domain knowledge bases will significantly improve efficiency of knowledge enhancements.
- Enhanced domain knowledge (via retrieval) is also good for *assessing* the quality of generated data.
Synergy between small and large LMs
- Looking forward to more diversified collaboration between large and small models ((They talk about some sort of sidecar arrangement sort of like speculative decoding)) to improve the quality of generated data.
Human-Model Collaboration
- A human-friendly interactive system, to involve necessary human knowledge for annotation and verification is vital and irreplaceable.


## Conclusion
meh


Abstract
> Within the evolving landscape of deep learning, the dilemma of data quantity and quality has been a long-standing problem. The recent advent of Large Language Models (LLMs) offers a data-centric solution to alleviate the limitations of real-world data with synthetic data generation. However, current investigations into this field lack a unified framework and mostly stay on the surface. Therefore, this paper provides an ==organization of relevant studies based on a generic workflow of synthetic data generation==. By doing so, we ==highlight the gaps within existing research and outline prospective avenues for future study==. This work aims to shepherd the academic and industrial communities towards deeper, more methodical inquiries into the capabilities and applications of LLMs-driven synthetic data generation.


## Paper Figures

![[Pasted image 20240719150002.png|250]]
lol

![[Pasted image 20240719161637.png]]
Some example data "pipelines"

![[Pasted image 20240719163742.png]]
Two methods of filtering data: ==High-Quality Sample Filtering== aims to weed out undesired low-quality datapoints, and ==Label enhancement== strives to *rectify* potentially erroneous annotations in generated samples.
