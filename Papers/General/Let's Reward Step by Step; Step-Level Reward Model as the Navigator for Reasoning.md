October 16, 2023 (5 months after [[Let's Verify Step by Step]])
National University of Singapore (NUS), ByteDance, Shanghai Jiao Tong University (Ma et al.)
[Let's Reward Step by Step: Step-Level Reward Model as the Navigator for Reasoning](https://arxiv.org/abs/2310.10080)
#zotero 
Takeaway: ...


Builds on the work from [[Let's Verify Step by Step]] (2023) and *Uesato et al. (2022)* in the PRM space.

----

## Introduction
- When it comes to reasoning, even LMs using [[Chain of Thought|CoT]] are still prone to a cascade of errors in their generation process -- as the number of reasoning steps increases, these LLMs face challenges in providing and integrating effective feedback, resulting in one error leading to another.
- To address this, prior research has focused on feedback mechanisms and supervisory techniques. Works have incorporated self-reflection  or external observations (eg [[ReAct]]) during reasoning, or novel methods like [[Tree of Thought]] and RAP (Wei et al., 2023) that have integrated traditional search algorithms to convert multi-step reasoning into a path search problem. These work, but introduce a new set of problems -- there's a risk of LLMs falling into repetitive thought loops, and self-reflection can exhaust/exceed a model's context window constraints, or it can just result in too much computational overhead!
- Enhancing reasoning capabilities of LMs involves a few important elements:
	1. ==Step-by-Step Reasoning: Helps in breaking down problems incrementally and guiding model toward the solution.==
	2. ==Incorporation of feedback, or otherwise self-reflection==
	3. ==Implementing heuristic search algorithms==

- *Uesato et al (2022)* and *Lightman et al (2023)* ([[Let's Verify Step by Step]]) introduced the [[Process Reward Model]] (PRM), which can provide step-level feedback for multi-step reasoning results generated by larger models.
	- But PRMs have traditionally been used in [[Reinforcement Learning from Human Feedback|RLHF]] via [[Proximal Policy Optimization|PPO]] (introduced as the reward model for PPO in [[WizardMath]]) or [[Rejection Sampling]].
	- We hypothesize that fine-grained feedback could similarly improve *reasoning path searching!*
- We propose a ==heuristic greedy search algorithm== for LLM reasoning tasks that harnesses the PRM for step-level feedback.
	- ==Each new reasoning step generated by the LLM undergoes evaluation by the reward model, which determines whether to accept the given step or continue generating a new one.==
	- ==If a valid new step is unattainable, a backtrack occurs until a complete reasoning path is identified.==
	- ==Compared to [[Chain of Thought]] (zero shot? few shot how many? unclear), performance improvements on [[GSM8K]] 63.2% -> 65.4% and [[MATH]] 10.4% -> 13.7% ... which seem like a pretty modest improvement, but an improvement.==

## Method
- 


## Experiment


## Related Works


## Conclusions and Future Work


Abstract
> Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The ==Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling.== Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, ==we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs==. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, ==we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks==. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.



