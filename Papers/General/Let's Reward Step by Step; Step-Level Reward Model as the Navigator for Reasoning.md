October 16, 2023 (5 months after [[Let's Verify Step by Step]])
National University of Singapore (NUS), ByteDance, Shanghai Jiao Tong University (Ma et al.)
[Let's Reward Step by Step: Step-Level Reward Model as the Navigator for Reasoning](https://arxiv.org/abs/2310.10080)
#zotero 
Takeaway: ...


Builds on the work from [[Let's Verify Step by Step]] (2023) and *Uesato et al. (2022)* in the PRM space.

----

## Introduction
- When it comes to reasoning, even LMs using [[Chain of Thought|CoT]] are still prone to a cascade of errors in their generation process -- as the number of reasoning steps increases, these LLMs face challenges in providing and integrating effective feedback, resulting in one error leading to another.
- To address this, prior research has focused on feedback mechanisms and supervisory techniques. Works have incorporated self-reflection  or external observations (eg [[ReAct]]) during reasoning, or novel methods like [[Tree of Thoughts]] and RAP (Wei et al., 2023) that have integrated traditional search algorithms to convert multi-step reasoning into a path search problem. These work, but introduce a new set of problems -- there's a risk of LLMs falling into repetitive thought loops, and self-reflection can exhaust/exceed a model's context window constraints, or it can just result in too much computational overhead!
- Enhancing reasoning capabilities of LMs involves a few important elements:
	1. ==Step-by-Step Reasoning: Helps in breaking down problems incrementally and guiding model toward the solution.==
	2. ==Incorporation of feedback, or otherwise self-reflection==
	3. ==Implementing heuristic search algorithms==

- *Uesato et al (2022)* and *Lightman et al (2023)* ([[Let's Verify Step by Step]]) introduced the [[Process Reward Model]] (PRM), which can provide step-level feedback for multi-step reasoning results generated by larger models.
	- But PRMs have traditionally been used in [[Reinforcement Learning from Human Feedback|RLHF]] via [[Proximal Policy Optimization|PPO]] (introduced as the reward model for PPO in [[WizardMath]]) or [[Rejection Sampling]].
	- We hypothesize that fine-grained feedback could similarly improve *reasoning path searching!*
- We propose a ==heuristic greedy search algorithm== for LLM reasoning tasks that harnesses the PRM for step-level feedback.
	- ==Each new reasoning step generated by the LLM undergoes evaluation by the reward model, which determines whether to accept the given step or continue generating a new one.==
	- ==If a valid new step is unattainable, a backtrack occurs until a complete reasoning path is identified.==
	- ==Compared to [[Chain of Thought]] (zero shot? few shot how many? unclear), performance improvements on [[GSM8K]] 63.2% -> 65.4% and [[MATH]] 10.4% -> 13.7% ... which seem like a pretty modest improvement, but an improvement.==

## Method
- They trained their reward model  in two phases:
	1. To equip the their base model with mathematical instruction capabilities and strengthen its mathematical proficiency, they used the training portion of the [[MATH]] dataset. They mapped the "question" and "solution" to "instruction" and "response", and then performed instruction tuning on a [[LLaMA]]-7B model and got LLaMA-7B-SFT.
	2. Building on the instruction-tuned model LLaMA-7B-SFT, they trained a PRM model using the [[PRM800K]] dataset from [[OpenAI]]'s [[Let's Verify Step by Step]] paper. This model is now designed to score each step of a solution on a step-by-step basis with a label of Positive, Negative, Neutral.
- Heuristic greedy search
	- Heuristic search is an algorithm that uses heuristic information in problem-solving to guide search direction, finding solutions more rapidly.
		- A classic example of heuristic greedy search is the $A^*$ algorithm, which combines the benefits of best-first search and Djikstra's algorithm. It finds the shortest path by using a heuristic to estimate the cost to the goal, from a given node.
	- During mathematical problem-solving, each chain of reasoning can be broken down step-by-step; this can be translated into a search problem for the optimal reasoning path within a vast space -- being able to assess the reward of each step (or the value of a specific state) is very valuable!
- Algorithm for Heuristic Greedy Search with PRMs
	- Given an input question x, our algorithm expands to the next potential node and evaluates it using our process reward model.
	- If the reward is positive, the algorithm advances using the new node; if not, it continues to explore the next potential node, assessing it with the reward model until it reaches a ==maximum (local) bandwidth B==.
		- ((Interesting that if the first option is positive, we take it, rather than grading several options before taking one? I guess it *is* a greedy algorithm...))
	- If we reach the maximum bandwidth without finding a positive step from our current position, two scenarios are considered:
		1. If a *neutral* option exists, one is selected.
		2. If all sub-nodes are negative, this indicates that we're at an erroneous step, where we can't determine the correct subsequent step, which prompts a backtrack. 
	- To constrain our search space efficiently and balance exploration with efficiency, we've set max bandwidth B and max iterations T parameters.
	- Once the algorithm reaches the max iteration count, it will generate an answer from the current step.
		- ((This seems like it might not be the correct place? What if at the max iteration we've backtracked some amount? Well... maybe this *is* fine.))
- Authors tried BFS and MCTS, but found that they introduced a more redundant search space and didn't effectively improve the results. They say that heuristic greedy search achieves a better trade-off between exploration and efficiency.
- Process-Supervised Data for Code
	- To extend their approach to the field of code, a step-level reward dataset for *coding* is necessary (basically, a code analogue to [[PRM800K]], which they call ==PRM-Code==
	- As opposed to math problems where it's difficult to assess the correctness of a given reasoning step, coding problems with unit-tests are uniquely suited for automated labeling of step-level reward data.
	- Authors use [[MBPP]] (Mostly Basic Python Problems) as a seed dataset for which the PRM for code dataset is created.
		- ((Interesting that they're using this dataset of Python problems that could be solved by beginners, eg "Write a python function to find the first repeated character in a given string."))
	- Python programs are naturally divided into lines separated by the new-line token; they first collect positive results from the ground-truth data.
		- If they select the n'th step as the current annotated step, the `Previous Data` includes the prompt and steps 0:n-1, the `Step` is the n'th step, and the `Label` is ***positive***.
		- To create the ***neutral*** and **negative*** reward for code, they use a combination of code mutation and unit-testing. In Mutation Testing, operators in a given codeblock are mutated to similar operators (eg addition -> division). Such mutation requires parsing the code block into an AST representation and defining rules of how different nodes on the AST can be mutated.
			- This yields N mutated code variants for each line of code, where each mutated code is executable, and each variant undergoes only one mutation.
			- Mutated code that pass unit tests is labeled as **neutral***.
				- ((If the unit tests aren't good, then mutated code might still pass them, but be ground-truth bad code?... Otherwise, I think the assumption is that the data in the MBPP dataset is the "best" way to answer the question.))
			- If it fails to pass unit tests, it's **negative***.
		- We note that mutation testing primarily focuses on atomic operators like arithmetic operations and array slicings, which limits the generality of the PRM-Code dataset. Authors note that better mutations might be interesting.

## Experiment
- As mentioned Trained their PRM based on [[LLaMA]] 7B, with the training method first involving directive fine-tuning using the [[MATH]] training dataset (yielding an SFT model), followed by process reward model training (yielding the PRM). 
- Authors also train a PRM specifically for code, directly using Code-LLAMA-7B, which had been fine-tuned with code instructions, and then further trained it using their code step-level PRM dataset ==PRM-Code== derived from MBPP.
- The resulting code process reward model is evaluated on [[HumanEval]] (164 original programming questions, with 9.6 test cases assigned to each question)... Added about 3-5% to the HumanEval pass@1 score.
- For complex reasoning problems, it's challenging for LLMs to sample reasoning paths that *don't* contains negative steps (according to a PRM); this is why we use search instead of sampling.

## Related Works
- Key datasets for mathematical reasoning are [[GSM8K]], [[MATH]], AddSub, MultiArith, SingleEQ
- Key code generation evaluations include [[HumanEval]], [[MBPP]], and DS1000 (a data-science-related benchmark).
- ==LLMs for reasoning==: Methods like Chain of Thought, Program-aided Language Models (PAL, where it's like CoT but it has code-interpreter access),  Self-Consistency, etc. all focus on building reliable chains of thought.
- ==Reasoning with Feedback==: Methods and models like CRITIC, [[Self-Refine]], *Teaching LLMs to Self Debug*, [[Reflexion]] demonstrate that models can serve as feedback sources to bolster reasoning.
	- *Decomposition enhances reasoning via self-evaluation guided decoding (2023)* offers a prompting method integrating self-evaluation via stochastic beam search to refine multi-step inference.
	- [[Tree of Thoughts]] enables models to explore varied reasoning paths, evaluate decisions, and adjust strategies.
	- *Reasoning with language model is planning with world model (2023)* transforms LLMs to act as both a world model and logical agent, using a planning method inspired by [[Monte-Carlo Tree Search|MCTS]].
	- *Leti: Learning to generate from textual interactions (2023)* uses textual feedback from code errors for better code generation.
	- *ReAct (2023)* presents a framework blending advances in reasoning and actions for broader spectrum of linguistic challenges.
- ==PRMs and ORMs==:
	- Uesato et al (2022): First introduced PRMs, highlighting its advantages over ORM in several applications, from few-shot prompting to reward modeling.
	- Lightman et al (2023) ([[Let's Verify Step by Step]]): Released [[PRM800K]], a dataset based on [[MATH]] annotations, showcasing the reliability of process supervision over outcome supervision.
	- Luo et al (2023) ([[WizardMath]]): introduced ==Reinforcement Learning from Evol-Instruct Feedback (RLEIF)== using PRMs as reward models within the [[Proximal Policy Optimization|PPO]] framework.
	- ==These Studies have focused mostly on PRMs for math, and there's a noticeable gap in PRM research for coding.==

## Conclusions
- Experimental results on [[MATH]] and [[GSM8K]] datasets validate the effectiveness of our proposed HGS-PRM method, and explore the potential of PRM in decoding.
- We use the Mutating Testing technique for code PRM data generation, enabling rapid scaling of step-level reward data for code applications ((booo)).


## Conclusions and Future Work


Abstract
> Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The ==Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling.== Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, ==we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs==. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, ==we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks==. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.



# Paper Figures
![[Pasted image 20240717134459.png|600]]
Comparing (eg) multi-step reasoning (eg CoT) with multi-step search (ToT) with multi-step search with PRM (this paper).

![[Pasted image 20240717135801.png|600]]
An example of using process reward models to guide a heuristic greedy search.
Given an input x, we expand to the next potential node and evaluate it using a reward model. If the reward is positive, the algorithm advances using this new node. If not, it continues to explore the next potential node, assessing it with the reward model until it reaches maximum bandwidth B. If a node expands to the maximum bandwidth without find a positive step, two scenarios are considered... If neutral sub-nodes exist, one is selected. If all subnodes are negative, this indicates an erroneous step, prompting a backtrack. Once we reach the maximum iteration count, we'll generate an answer frmo the current step.
- Uses max bandwidth B and max iterations T parameters.
- Authors tried BFS and MCTS but found they introduced a more redundant search space that didn't improve results.

![[Pasted image 20240717141953.png|600]]
An example of the heuristic greedy search guided by PRM (HGS-PRM). When considering the third step, they must have generated these options left-to-right, because we greedily take a positive step when we come across it. I'm not sure exactly what counts as an "iteration" in the algorithm yet, but it seems we must have hit the iteration count at the "Therefore" one? Because we generate the answer from the current position after reaching the iteration count, I believe.

![[Pasted image 20240717155657.png|600]]
Given some "gold" line of code (the normal stuff from [[MBPP]], they mutate the ast representation of it; if it still passes unit tests, it's Neutral; if it doesn't pass unit tests, it's Negative). This seems... okay.

![[Pasted image 20240717160338.png|500]]
Showing how HGS-PRM improves performance on [[HumanEval]] compared to CoT. I wonder how it would compare to ToT, which is definitely a move exhaustive/computationally-expensive method.
The accuracy improvement we observed in code is generally higher in code than in mathematics.

![[Pasted image 20240717161509.png|400]]
Sampled some reasoning paths (CoTs), which are going to contain "negative" thoughts, versus using HGS-PRM, which definitionally shouldn't contain negative thoughts.

![[Pasted image 20240717171056.png|400]]

![[Pasted image 20240717171122.png|400]]

![[Pasted image 20240717171130.png|400]]

![[Pasted image 20240717171146.png|400]]

# Non-Paper Figures