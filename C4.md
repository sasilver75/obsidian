---
aliases:
  - Colossal Clean Crawled Corpus
---


A cleaned subset of the CommonCrawl dataset. (*Colossal, Cleaned, Common Crawl*)
Developed for use in the [[T5]] model by Google in Oct 2019.
C4 is made by Google using a few heuristics.
Authors chose to create the C4 dataset from scratch due to the lack of high-quality, publicly-available pretraining datasets for language models.

It's about 10% of CommonCrawl, and tries to filter by different ways:
- Uses an open source list of dirty/bad/naughty words, created by Shutterstock.
- Lots of stuff didn't get filtered out; certain pieces of 4Chan, Kiwiforums, 

Lots of models will use both C4 and CommonCrawl as their training.
Many models ([[Gopher]], [[Chinchilla]], [[MPT]], and more) use C4 as a subset of their pretraining data, since the quality is quite high.