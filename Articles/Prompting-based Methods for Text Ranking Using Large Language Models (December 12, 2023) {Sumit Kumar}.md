Link: https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/

---

There's been a growing interest in applying LLMs to zero-shot text ranking in the context of LLMs. LEt's describe the paradigm and a few approaches!

## Text Retrieval with LLMs
- Text Retrieval refers to the task of identifying and ranking the most relevant documents, passages, sentences, etc. in response to a given use query.
- Often these systems are implemented as multi-stage ranking pipelines consisted of a ==retriever== and ==reranker==(s).
	- Popular retrievers include [[BM25]] (traditional zero-shot lexical retriever), and [[Contriever]] (unsupervised dense retriever.). Retrievers optimize to be high recall, with fast performance.
	- Something like [[UPR]] as a ranker might be combined with these to form a SoTA zero-shot multi-stage ranking pipeline. Rerankers optimize to be high precision, and are generally more expensive than the retriever.

- Recent works like [[InPars]], [[Promptagator]], [[HyDE]] use LLMs as auxiliary tools to generate synthetic queries or documents to augment the training data for retrievers or rerankers.
	- ==In contrast, the focus of THIS article will be on methods that *directly use LLMs as rerankers* in the multistage pipeline.==


## Prompting Strategies for LLM-based Ranking
- Ranking strategies for using LLMs in ranking tasks can be categorized into three main approaches:
	1. ==Pointwise Ranking==: Rerankers takes both the query and candidate document to generate an independent relevance score. 
	2. ==Pairwise Ranking==: A pair of candidate items, along with the user query, serve as prompts to guide the LLM to determine which is most relevant to the given query.
	3. ==Listwise Ranking==: Generalizes the pairwise paradigm; in listwise ranking, a set of documents is fed to the LLM. The LLM is then instructed to generate a ranked permutation of these documents.

### (1/3) Pointwise Ranking
- In pointwise reranking, the reranker takes the query and a specific candidate document to independently directly generate a relevance score. These independent scores assigned to each document $d_i$ are then used to reorder the candidate set $D$.
- Can be further classified into two popular approaches, based on how the ranking score is calculated:
	1. ==Instructional Relevance Generation==
		- LLM is prompted to output either "Yes" or "No" to determine the relevance of a candidate to a query. The generation probability is then converted to a relevance score. 
	2. ==Instructional Query Generation==
		- Use LLMs to generate a query based on the document, and measure the probability of generating the actual query.
		- An example of this approach is [[UPR|Unsupervised Passage Re-Ranker]] (UPR), which applies an off-the-shelf LM and a prompt of *"Please write a question based on this passage"* to the document $d_i$ tokens, and computes the likelihood of query generation conditioned on the passage. ![[Pasted image 20240528174123.png]]
- ((Neither of these seem that good to me! It's strange that in these examples we don't have the LLM generate any sort of rationale before giving a judgement, as we see in LLM-as-a-Judge papers like [[Prometheus]], [[Shepherd]])).

### (2/3) Pairwise Ranking
- A pair of candidate items ($d_i, d_j$) along with the user query serve as prompts to guide the LLMS to determine which document (of the two) is most relevant to the given query. ![[Pasted image 20240528174203.png]]
- For ties in aggregated scores, pairwise ranking has been proven to be more effective than pointwise and listwise methods... but it's also ==inefficient and hence unsuitable for inference in large-scale industrial systems.==
	- Making k^2 calls can be prohibitive in terms of cost! But some comparisons can be redundant, in that they can be predicted from those of other comparisons. Options here include:
		- Global Random Sampling
		- Neighborhood Window Sampling
		- Skip-Window Sampling (Best, with greedy aggregations)
	- Papers ASAP (Active Sampling for Pairwise comparisons) and PRP (Pairwise Ranking Prompting) might be interesting to look at, who both present methods to improve pairwise ranking efficiency.
- Once you have all of your pairwise rankings, you have to aggregate them into a ranking. Several approaches work well in practice:
	- Sorting via KwikSort
	- Additive Aggregation
	- Regression-based Aggregation
	- Greedy Aggregation
	- Graph-based Aggregation

### (3/3) Listwise Ranking
- Generalizes the pairwise paradigm; here, we get a set of candidate documents fed to the LLM and we output a ranked permutation of these documents (By some document identifier, eg `[2] > [3] > [1]`)
- Inefficient, because the substantial number of required tokens in the output as each additional token generated by the LLM requires an extra inference step. ((? I don't see how it's less efficient in terms of number of output tokens to do {all of ranking}))
- Relevance scores for each candidate is simply defined as the reciprocal of its rank, so $s_i = 1/r_i$ .

#### Sliding Window Strategy
- Due to the limitations of LLM input contexts, LLM can only rank a limited number of passages. To overcome this, we use a sliding window strategy to allow the LLM to rank an arbitrary number of passages.![[Pasted image 20240528174956.png]]
- Window size and step size are two hyperparameters that you can play with.
- It seems smaller models lack the capability to effectively reorder a list of input documents (wrong format, repetition, missing documents); it's been most effective with Claude and GPT4.

