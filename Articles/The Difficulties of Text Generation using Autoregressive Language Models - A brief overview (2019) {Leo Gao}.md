---
tags:
  - article
---

- Interest in text-generating models has been rekindled in the past year (2019), in large part due to GPT-2, which primarily demonstrates the effectiveness of using the Transformer architecture with bigger models/data/compute.
- Notably, GPT-2 achieved SOTA on several language modeling datasets *without even training on the dataset!*
	- But GPT-2 and similar models also exhibit some flaws that may not be fixable purely using the bigger-model paradigm. Let's talk about some of these flaws!


### What's an autoregressive language model anyways? Why does it matter?
- The core problem of language modeling is approximating the distribution of natural language sequences using some parametrized function. 
- The make modeling more manageable, this autoregressive language model formulation factors the ideal language model p*(x) into:
	- ![[Pasted image 20231224191153.png]]
- In other words, in order to make the modeling problem more tractable, we instead train the parametrized function $\hat{p}_\theta(x)$  to predict the next token, conditioned on the previous token(s) context... and then repeat this forward, using the newly generated tokens appended to the original context as the new context.
	- This lets us obtain an estimate of the likelihood of any given sequence by taking the *product* across these conditional probabilities, for each word in the sequence!

Many problems can be formulated as autoregressive models or would benefit from a strong pretrained language models.

### [[Beam Search]] and repetition
![[Pasted image 20231224201058.png]]
- In the GPT-2 samples provided, the authors decided to sample with top-k filtering and temperature, rather than with [[Beam Search]], which would be expected to return much higher-quality samples by maximizing likelihood.
	- It was surprising then, that in the paper "The curious case of neural text degeneration" (Holtzman et al., 2019), it was shown that GPT-2 samples with higher predicted likelihood (ie found by beam search) actually have much *lower quality*, and tend to be extremely repetitive!
	- The authors argued that this modeling problem is due to maximum-likelihood being a fundamentally incorrect sampling objective, and propose nucleus sampling, a method that truncates low-likelihood token predictions (similar to top-k), while preserving "broad" (tail-heavy) distributions. It could be argued, however, that since sampling a maximum-likelihood sample from the ideal text model would by definition provide the most likely English text, it would *already take into account* the unlikelihood of extremely bland repetitive text in English... so the fault must lie with the training objective, not with the sampling objective.
	- Another tempting solution is to just penalize repetition. While empirically successful, there's no good theoretical reason why less repetition would better model the underlying distribution.


## Exposure Bias
![[Pasted image 20231224202430.png]]
- One major problem of maximum-likelihood training of autoregressive models is [[Exposure Bias]]!
	- Autoregressive models are only trained and evaluated on samples drawn from the target language distribution, but at evaluation time, they're fed samples that are themselves generated by the model!
	- This error compounds extremely quickly, and it's been observed (anecdotally) that GPT-2 exhibits a sharp drop-off in quality after a certain number of steps!

## Future Work
-  This problem bears a striking resemblance to many problems in reinforcement learning; indeed, existing works like "SeqGAN: sequence Generative Adversarial Nets with Policy Gradient" (Yu et al., 2016), ... use RL for various components of the training pipeline, from propagating the Generator gradient in a GAN to using Inverse Reinforcement Learning (which is itself deeply connected toGANs)
- There's still a long way to go before RL-baesd options become practical for models as large as GPT-2.

## Conclusion
- Recent work has demonstrated an immense improvement in the quality of NN text generation due to the increase in model sizes, the problem of exposure bias still persists for long sequence of generated tokens.

















