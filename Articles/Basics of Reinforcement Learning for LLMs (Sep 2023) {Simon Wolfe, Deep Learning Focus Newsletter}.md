#article 
Link: https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning

----

Recent AI research has revealed that [[Reinforcement Learning]], and more specifically, [[Reinforcement Learning from Human Feedback]] (RLHF) is a key component of training a SotA LLM.

Despite this, most open-source models have relied heavily on supervised learning strategies, like [[Supervised Fine-Tuning]]. 
- The lack of emphasis on reinforcement learning can be attributed to several factors, including the necessity to curate a large amount of human preference data.
- There's underlying skepticism towards reinforcement learning generally, because people aren't as familiar with it as they are with supervised learning -- people like to stick with the approaches they know best!

In this theories, we'll start with some basic definitions and approaches, and work our way towards more modern algorithms (e.g. [[Proximal Policy Optimization|PPO]]) that are used to finetune language models with [[RLHF]].

![[Pasted image 20240208213922.png]]

# What is reinforcement learning (RL)?
- At the highest level, RL is just another way of training a machine learning model. In prior overviews, we've seen a variety of techniques for training NNs, but the two most-commonly-used techniques for language models are supervised and self-supervised learning.

- In (Self)Supervised Learning, we have a dataset of inputs (eg a sequence of text), with corresponding labels (e.g. a classification or completion of the input text), and we want to train our model to accurately predict those labels from the input.
	- Example: Maybe we want to fine-tune a language model to classify sentences that contain explicit language. In this case, we can obtain a dataset of sentences with binary labels indicating whether the sentence contains explicit language or not.
		- Then, we can train our model to classify this data correctly by iteratively:
			1. Sampling a mini-batch of data from the dataset
			2. Predict the labels with the model
			3. Compute the loss (e.g. CrossEntropy)
			4. Backpropagate the gradient through the model
			5. Performing a weight update

- ==SELF-supervised learning is similar to the setup explained above, but there are no explicit labels without our dataset -- instead, the "labels" we use are already present within the input data==! It's basically a =="hack"== to use parts of the existing data as a label (eg by masking an intermediary word, masking a patch in an image, rearranging image patches, trying to predict the next token).

When is RL useful?
- Although RL is just a useful way of training a neural net, the training setup is different compared to supervised learning!
- Similarly to humans, RL trains neural networks through trial and error. The NN will produce an output, receive some feedback about the output, and learn from the feedback.
	- In the case of a language model, the neural network will produce an output, receive some feedback about this output, then learn from the feedback.
	- In our context, when we finetune a language model using [[Reinforcement Learning from Human Feedback]], the language model produces some text, receives a score/reward from a human annotator, and then uses RL to finetune the language model to generate outputs with high scores.


![[Pasted image 20240208221014.png]]
In a RL environment, the environment is not differentiable like it is in a supervised learning context.

In this case, ==we can't apply a loss function that trains the LM to maximize human preferences just using supervised learning== -- why?
- ==The reason is that the score that we get from the human is a little bit of a black box -- there's no easy way for us to explain this score or connect it mathematically to the output of the neural network!==
	- In other words, ==we can't backpropagate a loss applied to this score through the rest of the neural network.== This would require that we are able to differentiate the system that generates the score, which is a human that subjectively evaluates the generated text.

Big picture: This begins to provide us with insight as to why RL is such a beautiful and promising learning algorithm for neural networks!
- ==REINFORCEMENT LEARNING MEANS THAT WE CAN LEARN FROM ARBITRARY FEEDBACK; THAT WE CAN LEARN FROM SIGNALS THAT ARE NON-DIFFERENTIABLE, AND THEREFORE NOT COMPATIBLE WITH SUPERVISED LEARNING.==

In the case of RLHF, we can score the outputs generated by a language model using whatever principle we might have in mind, then use RL to learn from the scores, no matter how we choose to define them. In this way, we can teach a language model to be helpful, harmless, honest, more capable through tool use, and more.


# A formal framework for RL

![[Pasted image 20240208223352.png]]
The agent acts and receives rewards (and a new state) from the environment.
((Sam: Given a state, take an action and receive both a reward and a new state resulting from your actions))

Problems that are solved with RL tend to be structured in a similar format to above; namely, we have ==an agent that is interacting with an environment==. The agent has a *state* in the environment and produces *actions*, which can modify the current state and yield a reward.

As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. ==The agent's goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent.==
- It's true that ==rewards may have a long horizon==, meaning that it might take several "correct," consecutive actions to generate positive reward.
	- Making a good move in the middle of a chess game may end up causing the win, but it takes a while for it to play out, assuming you don't make any more mistakes.


## Markov Decision Processes ([[MDP]])
- To make things more formal and mathematically sound, let's formulate the system as a [[Markov Decision Process]] (MDP). Within an MDP, we have states, actions, rewards, transitions, and a policy; see below:

![[Pasted image 20240208224152.png]]
- States and actions have discrete values, while rewards are real numbers
- In an MDP, we define two types functions; ==transition functions== and ==policy functions==.
- The ==policy function takes a state as input, and outputs a probability distribution over possible actions==. Given this output, we can then make a decision for the action to be taken from a current state.
	- The ==transition function== then outputs the next state, based upon the previous state and the chosen action.

![[Pasted image 20240208224901.png]]
((Above: So given a state `s`, we determine a probability distribution of actions we can take with our `policy function`, given that state. We choose some action `a` from that distribution. Given our `(s,a)`, we use out `transition function` to determine our next state `s`. We are possibly given a reward `r` for reaching the new state.))

One thing we might be wondering after looking at the 


























