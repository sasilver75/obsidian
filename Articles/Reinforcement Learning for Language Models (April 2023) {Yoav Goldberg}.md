#article 
Link: https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81

----

## Why RL?
- With all the talk about [[Reinforcement Learning from Human Feedback|RLHF]], why is RL better than learning from demonstrations (aka supervised learning) for training language models? Shouldn't learning from demonstrations (eg "supervised fine-tuning") be sufficient?
- The author acme to realize that ==there's an argument which not only *supports the case of RL training, but also requires in, in particular with models like ChatGPT!*==
	- This is spelled out in the first half of a [talk](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81) by [[John Schulman]] from OpenAI

This post pretty much repeats his arguments in more words, and adds some things that John didn't explicitly say (but which the author is sure he thought about)

