https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training?utm_source=post-email-title&publication_id=1174659&post_id=147749119&utm_campaign=email-post-title&isFreemail=true&r=764e6&triedRedirect=true&utm_medium=email

[[Sebastian Raschka]]

----

![[Pasted image 20241126143049.png]]

There are hundreds of papers a month that propose new techniques and approaches for post-training.

Let's look at the pre-training and post-training pipelines of the most recent SoTA models!


Let's focus on the following pre-training and post-training pipelines of the following models:
1. [[Qwen 2]]
2. [[Apple Intelligence Foundation Language Models|Apple Foundation Model]]
3. [[Gemma 2]]
4. [[LLaMA 3.1]]

These models are presented in order based on the publication dates of their respective papers.

# Alibaba's Qwen 2
- Comes in 5 flavors
	- Dense 0.5B
	- Dense 7B
	- Dense 62B
	- MoE 57B (14B active)

One of the standout features is good multilingual capabilities in 30 languages:
- Large, 151,642 token vocabulary
	- For comparison [[Llama 2]] had a 32k vocab and [[LLaMA 3.1]] has a 128k vocabulary
- "==As a rule of thumb, increasing the vocab size by 2x reduces the number of input tokens by 2x, so the LLM can fit more [text] in the same input?=="
	- I think this is true if you're just raising your vocab count using 




# Apple's Apple Foundation Model
- 


# GDM's Gemma 2
- 


# Meta's LLaMA 3.1
- 




























