#article 
Link: https://huggingface.co/blog/rlhf

Note that this article is 1.5 years old, so it might be slightly out of date.
Didn't really tell me anything that I didn't already "know"

-----

What makes "good" text is inherently hard to define, since it's subjective and context-dependent. There are many applications of writing where you want ==creativity==, and others where you'd want ==truthfulness==, and others where you want ==executability!==

Writing a loss function to capture these attributes seems intractable, and most language models are still trained with a simple NTP loss.
- To compensate for the shortcomings/misalignment of the loss itself, people define metrics designed to better-capture human preferences, like [[BLEU]] or [[ROGUE]].

While being better-suited than the loss function itself at measuring performance, these metrics simply compare generated text to references with simple rules and are thus also limited.

What if we take it one step further and just use human feedback as the loss to optimize the model? That's the idea behind [[Reinforcement Learning from Human Feedback]] (RLHF); use methods from RL to directly optimize a language model, using human preference scores as feedback!


# RLHF: Let's take it step by step

- RL from human feedback is a challenging concept because it involves a multiple-model training process and different stages of deployment.
- In this blog, we'll break down the training process into three core steps:
	1. Pretraining a language model (LM)
	2. Gathering data and training a reward model
	3. Fine-tuning the LM with reinforcement learning

### Pretraining Language Models
- OpenAI used a smaller version of GPT-3 for its first popular RLHF model, [[InstructGPT]].
- Anthropic used transformer models from 10M to 52B parameters trained for this task
- DeepMind has documented using up to 280 billion parameter model [[Gopher]] for this task.

- The initial model CAN be fine-tuned on additional text or conditions, but doesn't necessarily need to be. OpenAI fine-tuned on human-generated text was "preferable", and Anthropic generated their initial LM for RLHF by distilling an original LM on context clues for their "helpful, honest, and harmless" criteria.

In general, there isn't a clear answer on "which model" is the best starting point for RLHF.

### Gathering data and training a reward model

Generating a Reward Model calibrated with human preferences is where relatively new research in RLHF begins; the underlying goal is to get a model or system that ==takes in a sequence of text and returns a scalar reward which should numerically represent a human preference.==

The output being a ***==scalar reward==*** is crucial for existing RL algorithms being integrated seamlessly later in the process.

The LMs for reward modeling can be both *another fine-tuned LM* or an LM trained from scratch on the preference data.
No one base model is considered the clear best choice for reward models.

The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (hopefully, this will be prompts similar to those that it will receive in prod); the prompts are passed through the initial language model to generate new text.
- Human annotators then rank the generated text outputs from the LM

One may ==initially think that humans should apply a scalar score== directly to each piece of text, in order to generate a reward model -- but ==this is difficult to do in practice==, because the different values of humans cause these scores to be uncalibrated and noisy. ==Instead==, ==rankings are used to compare the outputs of multiple models and generate a much better-regularized dataset.==

There are multiple methods for ranking the text. One method that's been successful is to have users compare generated text from two language models conditioned on the same prompt.
By comparing model outputs in head-to-head matchups, an ==Elo== system can be used to generate a ranking of the models and outputs relative to eachother! ==These different methods of ranking are normalized into a scalar reward signal for training.==
((I'd like to know more about this part of the process))

An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation
- OpenAI: 175B LM, 6B reward model
- Anthropic used LM and reward models from 10B to 52B
- DeepMind uses 70B Chinchilla models for both LM and reward
==An intuition would be that these preference models need to have similar capacity to understand the text given to them, as a model would need to *generate* said text.==
- ((I can make up plausible sounding intuitions all day! Either of "equal intelligence" or of "greater intelligence", I would think the reward model should be.))

At this point, we have an initial language model that can be used to generate text, and a preference model that takes any text and assigns it a score of how well humans perceive it. Next, we use a reinforcement learning (RL) algorithm to optimize the original language model with respect to the reward model.

### Fine-tuning the LM with reinforcement learning
- Training an LM with RL was, for a long time, something that people thought was impossible both for engineering and algorithmic reasons.
- What multiple organizations seem to have gotten to work is fine-tuning some or all of the parameters of the initial LM with a [[Policy Gradient]] algorithm, [[Proximal Policy Optimization]] (PPO). 
- Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive, depending on the scale of the model and the infrastructure available.

The exact dynamics of how many parameters to freeze, or not, is considered an open research problem.

There are tons of guides on how PPO  works; the relative maturity of this method has made it a favorable choice for scaling up to the new application of distributed training for RLHF.

Let's first formulate this fine-tuning task as an RL problem
- First: the ==policy== is just a language model that takes a prompt and returns a sequence of text.
- The ==action space== of the policy is all the tokens corresponding to the vocabulary of the language model (eg 50k tokens)
- The ==observation space== is the distribution of possible input token sequences (size of vocabulary & length of input token sequence)
- The ==reward function== is a combination of the preference model and a constraint on policy shift.

The reward function is where the system combines all of the models we've discussed into one RLHF process:
1. Given a prompt $x$ from the dataset, the text $y$ is generated by the current iteration of the fine-tuned policy.
2. Concatenated with the original prompt, that text is passed to the preference model, which returns a scalar notion of "preferability" $r_{\theta}$ .
3. In addition, per-token profitability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them. This has been designed as a scaled version of a [[Kullback-Leibler Divergence]] between these sequences of distributions over tokens, $r_{KL}$.

The ==KL divergence term== penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch. This is useful to make sure that the model outputs reasonably coherent text snippets; ==without this penalty, the optimization can start to generate gibberish text that the reward model gives high reward==.

The final reward send to the RL update rule is $r = r_{\theta} - {\lambda}r_{KL}$ 

Some RLHF examples have additional terms added to the reward function.
- For example, OpenAI mixes in additional pre-training gradients (from the human annotation set) into the update rule for PPO.

Finally, the ==update rule== is the parameter update from PPO that maximizes the reward metrics in the ***current batch of data*** (PPO is [[On-Policy]], meaning that the parameters are only updated with the ***current batch*** of prompt-generation pairs).
- PPO is a trust optimization algorithm that uses constraints on the gradient to ensure that the update step does not destabilize the learning process.

![[Pasted image 20240423131159.png]]
_Technical detail note: The above diagram makes it look like both models generate different responses for the same prompt, but what really happens is that the RL policy generates text, and that text is fed into the initial model to produce its relative probabilities for the KL penalty. This initial model is untouched by gradient updates during training_.

Optionally, RLHF can continue from this point by iteratively updating the reward model and the policy together.
- As the RL policy updates, users can continue ranking these outputs versus the model's earlier versions.
Anthropic discusses this option as *Iterated Online RLHF*, where iterations of the policy are included in the ELO ranking system across models. This introduces complex dynamics of the policy and reward model evolving, which represents a complex and open research question.


## Open-Source tools for RLHF
- In 2019, OpenAI did RLHF on LMs in TensorFlow
- Today, there are a few active repositories for RLHF in Pytorch...


## What's next for RLHF
- These techniques are extremely promising and impactful, and have caught the attention of the biggest research labs in AI, but there are still clear limitations.
- The models are better, but can still output harmful or factually-inaccurate text without any uncertainty.
- When deploying a system using RLHF, gathering the human preference data is quite expensive, due to the direct integration of other human workers outside the training loop.
- RLHF performance is only as good as the quality of its human annotations, which takes two varieties:
	- Human-generated text (such as fine-tuning the initial LM in InstructGPT)
	- Labels of human generated text between model outputs

Generating well-written human text answering specific prompts is very costly.
Thankfully, the scale of data used in training the reward model for most applications of RLHF is not as expensive (50k labeled preference samples) -- but it's higher than most academics would be able to afford.

Only one large-scale dataset for RLHF on a general language model ([[Helpful and Harmless|HH]]) exists, from Anthropic -- though a couple of smaller-scale task-specific datasets ([[TL;DR]] from OpenAI) exist.

Another challenge is that ==human annotators often disagree, adding substantial potential variance to the training data without ground truth.==

With these limitations, huge swaths of unexplored design options could still enable RLHF to take substantial strides.
- Many of these fall within the domain of improving the RL optimizer.
- PPO is a relatively old algorithm, but there are no structural reasons that other algorithms could not offer benefits/permutations on the existing RLHF workflow.

One large cost of the feedback portion of fine-tuning the LM policy that every generated piece of text from the policy needs to be evaluated on the reward model.

To avoid these costly forward passes of a large model, offline RL could be used as a policy optimizer. 

Other core trade-offs in the RL process, like exploration-exploitation balance, have also not been documented. 


### Further reading
- Here's a list of the most prevalent papers on RLHF to date.
- The field was recently popularized with the emergence of DeepRL (around 2017) and has grown into a broader study of the applications of LLMs from many language technology companies.

... Some paper recommendations ...


