#article 
Link: https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early

------

Research on language modeling has a long history that dates back to GPT-1 or RNN-based techniques like ULMFit.

Despite this longish history, language models have only become popular relatively recently; the first rise in popularity came with the proposal of ==GPT-3, which showed that impressive few-shot learning performance could be achieved across many tasks==, with a combination of self-supervised pre-training and in-context learning.

After this, GPT-3 inspired the proposal of a swath of large language models (MT-NLG, Chinchilla, Gopher, more). After, ==research on language model alignment led to the creation of even more impressive models like InstructGPT and its sister ChatGPT==.

Despite being incredibly powerful, these models were all *closed source!*















