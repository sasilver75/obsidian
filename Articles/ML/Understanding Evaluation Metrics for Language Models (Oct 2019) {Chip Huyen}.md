#article 
Link: https://thegradient.pub/understanding-evaluation-metrics-for-language-models/
( This is an old article, but it's mostly about metrics like perplexity, cross-entropy, etc. Evergreen topics )

-----

![[Pasted image 20240221205802.png]]

Traditionally, language model performance is measured by [[Perplexity]], [[Cross Entropy]], and [[Bits Per Character]] (BPC). 
As language models are increasingly used as pre-trained models for other NLP-related tasks, they're often evaluated based on how well they perform on specific downstream tasks.

The [[GLUE]] benchmark score is one example of a broader, multi-task evaluation for language models.

Counterintuitively, having *more metrics* makes it *more difficult* to compare language models -- especially as many of these benchmarks are unreliable (leaked answers, etc).

==One of Chip's favorite interview questions is to explain [[Perplexity]] or the difference between [[Cross Entropy]] and [[Bits Per Character]]==


# This is boring for me right now, but continue later!