#article 
Link: https://www.interconnects.ai/p/agi-is-what-you-want-it-to-be?utm_source=post-email-title&publication_id=48206&post_id=143916547&utm_campaign=email-post-title&isFreemail=false&r=764e6&triedRedirect=true&utm_medium=email

Ok article -- "There are a bunch of definitions of AGI, and they're all bad. We're curious to see what the (first) legal definition will be in future OpenAI/Microsoft cases"

------

AGI doesn't need to involve the idea of agency; the three letters only indicate a general level of capability -- but no one can control what others think it means.

The biggest problem in AGI debates is that people have different ideas about what the end goals/value systems should be -- there are constraints on the Overton window of acceptable AGI definitions towards more advanced capabilities.

Let's recap some definitions of AGI

Wikipedia
> A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named ==AIXI==, the proposed AGI agents ==maximize the ability to satisfy goals in a wide range of environments==. This type of AGI, characterized by the ability to maximize a mathematical definition of intelligence, rather than exhibit human behavior, was also called ==universal artificial intelligence==.
- The AIXI framework is described on LessWrong as: "Not computable, and so does not serve as a design for a real-world AI."

OpenAI definition of AGI
> Highly autonomous systems that outperform humans at most *economically valuable work*.

This sort of definition is what's convenient for them, and definitely not the only acceptable definition of AGI. 

Nato is actually fine with calling a system like ChatGPT-4 AGI, but that might loop him too much in with the *Sparks of AGI* paper from Microsoft, which claimed:
> Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet incomplete) version of an AGI system.

The claim above is reasonable IF ACCOMPANIED with a sufficiently-mellow definition of AGI as an AI system that can match human capabilities on a broad suite of tasks.

==It is clear though, regardless of definition, that GPT-4 fits many *colloquial* definitions of AGI.==
- Nato's definition: An AI system that's *generally useful*. ((? Does this include being able to understand pictures? Generate images? If certain systems are more generally useful than others, there must be a threshold of generality where we call it an AGI, right? Where is that?))

The media backlash following the *Sparks of AGI* paper mostly stems from the methods in the paper and the definitions of AGI relative to OpenAI.
- Given OpenAI's dominance of mindshare and their relationship to Microsoft, the AI community mostly struck out at the authors of over-claiming.

Microsofts' researchers were making an extremely aggrandizing case of what AGI is that weakened their own marketplace; Satya, the CEO, was probably not thrilled at the time.

# RL still rules the AGI discourse

- ==The narratives of RL are more effective than the methods.==
- RL researchers are obsessed with ==agency== and ==feedback== -- these two terms create an intruiging form of intelligence, but both of them are really needed! Feedback alone is just a while loop, and Agency alone has no prospect of consistent self-improvement.

David Silver's Deep RL tutorial @ ICML 2016 had a slide claiming that AI is at its essence a combination of RL and deep learning...
![[Pasted image 20240424140022.png]]
Although ==this attitude is far from the technical reality, but it's still the cultural reality.==

The other form of RL's AGI argument is the 2021 *Reward is Enough* paper from Silver, Sutton, et al
- This paper mostly says that we can solve any problem with an RL-like approach -- it seems like AGI is guaranteed, then.

----
==Reward is Enough==, 2021 Abstract

In this article we hypothesise that ==intelligence, and its associated abilities, can be understood as subserving the maximisation of reward==. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. ==This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives==. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.

----

If some form of agency is required for AGI, how do we determine this?
- We've seem very little evidence that LLMs can autonomously improve -- mostly due to how resource-intensive and sensitive the training runs are.
- ==The crucial question of what determines if a powerful AI system is "autonomous" is the next debate for whether GPT-N is AGI==


# Modern AGI tests

- The Employment Test (Nilsson)
	- A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.
	- By some arguments we've already crossed this boundary (but does it have to be 100% of the job? 99%? 95%?)
- The Ikea Test (Marcus)
	- Also known as the Flat Pack Furniture Test
	- An AI views the parts/instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.
	- Lol, this is a Gary Marcus idea; moving along.
- The Coffee Test (Wozniak)
	- A machine is required to enter an average American home and figure out how to make coffee; find the coffee machine, find the coffee, add water, find a mug, brew the coffee by pushing the proper buttons. This has not yet been completed.
- The Modern Turing Test (Suleyman)
	- An AI model is given $100,000 and has to obtain $1 million

Perhaps the final example is the best definition? It's actionable, ambitious, and interpretable.
((I still think it fucking sucks))

# Agency and shifting goalposts
- The AI agent coding assistant Devin is a recent example of "Glimpses of AGI" -- Within this, trust is a big issue! This is a limiter to adoption.
- Another relevant constraint was a topic in the recent Dwarkesh interview with Mark Zuckerberg; How will we even power the data centers needed to train GPT-5 class models and onward. Moving from 50MW-100MW up to 1GW datacenters.

==Favorite part of the Microsoft-OpenAI deal rests so strongly on the definition of AGI, eventually a jury or judge will get to decide what the definition of AGI is.==

Over the decades we're going to keep seeing the definition of AGI changes as what it means to be a human changes in the face of AI. 
















