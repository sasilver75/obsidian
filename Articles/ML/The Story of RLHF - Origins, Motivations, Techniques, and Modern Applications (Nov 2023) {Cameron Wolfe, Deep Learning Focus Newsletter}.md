#article 
Link: https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations

------
- Language models experienced a surge of interest in 2018 with the proposal of [[Bidirectional Encoder Representations from Transformers|BERT]], which demonstrated that the [[Transformer]] architecture, self-supervised pre-training, and supervised transfer learning form a powerful combination.
- [[T5]] saw that supervised transfer learning was effective in this domain as well.
- Despite these accomplishments, those models pale in comparison to the generative capabilities of models like [[GPT-4]] that we have today! In order to create a model like this, we need training techniques that go far beyond supervised learning.

- Modern generative language models are the combined result of numerous notable advancements, including:
	1. Decoder-only transformers
	2. Next-token prediction as a pre-training task
	3. Prompting
	4. Neural scaling laws
	5. More!
One of the biggest factors that made the most recent models a commercial and usability-success have been alignment of models via reinforcement learning from human feedback (RLHF).

Using this approach, we can teach LLMs to surpass human writing capabilities, avoid harmful outputs, cite their sources, and much more. Fundamentally, ==RLHF enables the creation of AI systems that are more safe, capable, and useful.==

# Where did RLHF come from?
Let's understanding some basic ideas that preceded and motivated the development of RLHF, such as:
- Supervised learning (and how RLHF is different)
- The LLM alignment process
- Evaluation metrics for LLMs (and the ==ROGUE== score, in particular)










----------
# Comments

- 



































