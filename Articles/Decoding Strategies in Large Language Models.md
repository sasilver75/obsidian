https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html
A Guide to Text Generation from Beam Search too Nucleus Sampling

Not a particularly useful article in my opinion.

---

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load our Tokenizer and Model (in eval mode, on GPU)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
model.eval()

# Let's tokenize the following text
text = "I have a dream"
input_ids = tokenizer.encode(text, return_tensors='pt').to(device)
# tensor([[  40,  423,  257, 4320]])

# I think this is asking to generate 5 additional tokens on the end, in sequence.
outputs = model.generate(
	input_ids,
	max_length=len(input_ids.squeeze())+5
) # tensor([[  40,  423,  257, 4320,  286,  852,  257, 6253,   13]])
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Generated text: {generated_text}")
# 'I have a dream of being a doctor.'
```

There's a common misconception that LLMs like GPT-2 directly produce text -- but this isn't the case! Instead, LLMs themselves calculate logits, which are just scores assigned to every possible token in the vocabulary. These logits are converted to a probability distribution over the vocabulary using a softmax function that can be considered as separate from the mode.

![[Pasted image 20240627221404.png]]

Autoregressive models predict the next token in a sequence based on the preceding tokens. P(of | I have a dream) = 17. 
![[Pasted image 20240627223103.png|400]]
We do this to calculate the conditional probability for every token in the vocabulary.

So how do sample from these probabilities to generate text? There are many options.


----


# Greedy Search
- [[Greedy Decoding]] is a decoding method that takes the most probable token at each step.
- This might sound intuitive, but it's important to note that greedy search is short-sided: It only considers the most likely token at each step without considering the overall effect on the sequence.
- It's fast and efficient, since it doesn't need to keep track of multiple sequences like [[Beam Search]] does.

Given our generated sequence `I have dreams of being a doctor`
![[Pasted image 20240627234617.png|100]]
Although each token in the sequence is the likeliest token at the time of prediction, the selected tokens of "being" and "doctor" were both assigned relatively low probabilities -- this might suggest that "of", our first predicted token, may not have been the most suitable choice, since it leads to "being," which is quite unlikely.
- But we didn't have the ability to "look ahead" at the time, which is a reason why techniques like [[Beam Search]] exist.

# Beam Search
- Unlike greedy search, which only considers the most probable of the next tokens, beam search is able to take into account the *n most likely tokens*, where *n* represents the number of beams.
- This procedure is repeated until a predefined maximum length is reached, or an EOS token appears -- at this point, the sequence with the highest overall score if chosen as the output.

We maintain the sequence score logP(w), which is the cumulative *sum* of log probabilities of every token in the beam. We normalize this score by the sequence length to prevent bias towards longer sequences (this normalization factor can be adjusted).

`Generated text: I have a dream. I have a dream`

The best sequence seems a little surprising, but it's a common response from GPT-2.
To be honest, it's not a particularly compelling generation. Let's look at two other sampling methods: Top-k Sampling and Top-p Sampling (Nucleus Sampling).

# Top-k Sampling
- [[Top-K Sampling]] is a technique that leverages the probability distribution generated by the language model to select a token randomly from the *k most likely options*.

If we had k=3 and four tokens: A, B, C, D
With respective probabilities P(A)=30, P(B)=15, P(C)=5, P(D)=1
(I don't know why in his example these don't add to 1, but it's not important)

In top-k sampling, the token D is disregarded, and the algorithm will output A 60% of the time, B 30% of the time, and C 10% of the time.
(So we sort of renormalize the probabilities, considering just the top-k tokens)

==This approach ensures that we prioritize the most probable tokens, while introducing an element of randomness in the selection process.==

Another way of introducing randomness is the concept of [[Temperature]] $T$, which is a parameter in $[0,1]$, which affects the probabilities generated by the softmax function.
- In practice, it simply consists of dividing the input logits by a value we call temperature.

![[Pasted image 20240628000459.png]]

Here's a chart demonstrating the impact of temperature on probabilities for a given set of logits `[-1.5, -1.8, 0.9, -3.2]`. We've plotted three different temperature values:
![[Pasted image 20240628000557.png]]

Note that a ==temperature of 1.0== is equivalent to the default softmax with no temperature at all. On the other hand, a low temperature setting (eg 0.1) significantly alters the probability distribution, making it much more peaked.

By adjusting the temperature (along with the sampling method, we can influence the extent to which the model produces more diverse or predictable responses.

`Generated text: I have a dream job and I want to`

This definitely feels more natural than `I have a dream. I have a dream.`


# Nucleus Sampling/Top-P Sampling
- [[Top-P Sampling|Nucleus Sampling]] takes a different approach from top-k sampling: Rather than selecting the top-k most probable tokens, nucleus sampling chooses a cutoff value $p$ such that the *sum of probabilities of selected tokens exceeds p*.
	- This forms a "nucleus" of tokens from which to randomly choose the next token.

In other words, the model examines its top probable tokens in descending order, and keeps adding them to the "considered list" until the total probability of the considered last surpasses our threshold p.
- Unlike in [[Top-K Sampling]], the number of tokens included in the nucleus can vary from step to step.

`Generated text: I havea  dream. I'm going to`
This feels like a notable enhancement in semantic coherence, compared to greedy sampling.
((But then again, the generation is *actually* probabilistic in this one, unlike greedy sampling, so there's no guarantee that the next generation would look as good.))

