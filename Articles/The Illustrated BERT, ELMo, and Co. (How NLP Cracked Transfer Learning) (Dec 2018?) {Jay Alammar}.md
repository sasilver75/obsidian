#article 
Link: https://jalammar.github.io/illustrated-bert/

Review: This was not a good article.

------

The year 2018 has been an inflection point for machine learning models handling text. Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving.

![[Pasted image 20240401211802.png]]

One of the latest milestones in this development is the release of [[BERT|BERT]], an event described as marking the beginning of a new era in NLP.
BERT is a model that broke several records for how well models can handle language-based tasks.

This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component.

![[Pasted image 20240401211935.png]]
Above:
- You can download the model already pre-trained in step 1, and only worry about fine-tuning it for step 2.

BERT builds atop a number of clever ideas that have been bubbling up in the NLP community recently:
- Self-supervised sequence learning (Andrew Dai, Quoc Le)
- [[ELMo]] (Matthew Peters, AI2+UW)
- [[ULMFiT]] (Howard, Ruder)
- [[Transformer]] (Vaswani et al)

There are a number of concepts one needs to be aware of to properly wrap one's head around what BERT is

## Example: Sentence Classification
- The most straight-forward way to use BERT is to use it to classify  a single piece of text. It looks like this:
![[Pasted image 20240401212339.png]]
To train such a model ,you mainly have to train the classifier, with minimal changes happening to the BERT model itself during the training phase.
- This process is called Fine Tuning and has roots in semi-supervised sequence learning and ULMFiT

For people not versed in the topic, since we're talking about classifiers, then we are in the supervised-learning domain of machine learning.

Other examples for use-cases include:
- Sentiment analysis
	- Input: Movie/product review, Output: Pos/Neg sentiment
- Fact-checking
	- Input: Sentence, Output: "Claim" or "Not Claim"
	- Input: Claim sentence, Output: "True" or "False"


Model Architecture
- Now that you have an example use-case in your head for how BERT can be used, let's take a closer look at how it works:
- Paper introduces two model sizes for BERT:
	- BERT BASE
	- BERT LARGE

Bert is basically a trained Transformer *Encoder* stack!

![[Pasted image 20240401212812.png|350]]

Both BERT model sizes have a large number of encoder layers (called blocks) -- Twelve for the Base version, and 24 for the large version.

### Model Inputs
- The first input token is supplied with a special \[CLS\] token, standing for "Classification"
- Just like the vanilla encoder of the transformer, BERT takes a sequence of words as input, which keep flowing up the stack.
- Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.

This is identical to the OG Transformer encoder stack up until this point -- it's at the output that we first start seeing how things diverge.

### Model Outputs
- Each position outputs a vector of size *hidden_size (768)*. 
- For the sentence classification example we've looked at above, we focus on the output of only the first position (that we passed the special \[CLS\] token to.)
- That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by using a single-layer neural network as the classifier.

((They're just saying that we use the embedding stack of a transformer to output some representation of the input data, and then use some sort of classifier on top of that data.))

#### Parallels with ConvNets
- For those with a background in CV, the vector hand-off is reminiscient of what happens at the back-end of a network like [[VGGNet]] and the fully-connected classification portion at the end of the network.
![[Pasted image 20240401213559.png]]

# A new age of embedding
- Methods like Word2Vec and GloVe have been used for word-embedding tasks. Let's recap how those are used before pointing to what has now changed.

### Word embedding Recap
- For words to be processed by ML models, they need some sort of numeric representation that models can use in their calculations.
- [[Word2Vec]] showed that we can use a vector to properly represent words in a way that captures *semantic* or meaning-related relationships, as well as syntactic, or grammar-based relationships.
- ==The field quickly realized that it's a great idea to use embeddings that were pre-trained on vast amounts of text data, instead of training them alongside the model on what was frequently a small dataset.==
	- It became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe.

### [[ELMo]]: Context Matters
- If we're using the GloVe representation, then the word "stick" would be represented by this vector *no matter what the context was!*
	- eg: "Paris" for the city and "Paris" for the woman's name are represented in the same way.
- Why don't we set embeddings based on the context it's used in? ==To capture the word meaning in that context== as well as other contextual information?
	- Contextualized word embeddings were born!
- ==Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding==. It uses a *bi-directional LSTM* trained on a specific task to be able to create those embeddings.

![[Pasted image 20240401215511.png]]
- ELMo provided a significant step towards pre-training in the context of NLP
- The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.

What's ELMo's secret?
- It gains its language understanding form being trained to predict the next word in a sequence of words -- a task called *Language Modeling!*
![[Pasted image 20240401215856.png]]
- It uses a language modeling objective as a way to adjust the embeddings!? Do the embeddings train alongside the LSTM?

ELMO goes a step further a trains a bi-directional LSTM - so that its language model doesn't only have a sense of the next word, but also the previous word!
- ELMo comes up with the contextualized embedding through grouping together the hidden state (and initial embedding) a certain way (concatenation followed by weight summation).


# ULM-FiT: Nailing down transfer learning in NLP
- ULMFiT introduced a language model and a process to effectively fine-tune that language model for various tasks.
- The transformer: Going beyond LSTMs:
	- The release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs.
	- ==The encoder-decoder structure of the transformer made it perfect for machine translation, but could you use it for sentence classification?==

# OpenAI transformer: Pre-training a Transformer Decoder for Language Modeling
- The decoder is a good choice because it's a natural choice for language modeling since it's built to mask future tokens -- a valuable feature when it's generating a translation word by word.
- The model stacked twelve decoder layers. Since there's no decoder in this setup, the layers would not have the encoder-decoder attention sublayer (cross attention) that the vanilla transformer decoder layers have -- it will still have the self-attention layer, however (masked attention, so it doesn't peek at future tokens)

With this structure, we can proceed to train the model on the same language modeling task: predict hte next word using massive (unlabeled) datasets.


# Transfer learning to downstream tasks

![[Pasted image 20240401220702.png]]

# BERT: From Decoders to Encoders
- The OpenAI transformer gave us a fine-tunable pre-trained model based on the Transformer's decoder stack -- but something went missing in this transition from LSTMs to Transformers.
- ELMo's language model (an LSTM) was bidirectional, but the openAI transformer only trains a forward language model.
- Could we build a transformer model whose language model looks both forward and backwards?

Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon -- "is conditioned on both the left and right context")

### Masked Language Model
- BERT said that we'll use Transformer Encoders, but also use masks.
- Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a "masked language model" concept from earlier literature -- also called a ==Cloze== task.
- Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes.

## Two-sentence Tasks
- If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you'll notice that some tasks require the model to say something intelligent about two sentences.
	- Are they simply paraphrased versions of each other? Given a wikipedia entry as an input, and a question regarding that entry as another input, can we answer that question?
- To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task:
	- Given two sentences (A and B), is B likely to be the sentence that follows A, or not?

# Task specific models
![[Pasted image 20240401222434.png|450]]

BERT for Feature Extraaction
- The finetuning approach isn't the only way to use BERT -- like ELMo, you can use the pretrained BERT to create contextualized word embeddings. Then you can feed those embeddings to your existing model -- a process the paper shows yields results not far behind fine-tuning BERT on a task like named entity recognition.

























