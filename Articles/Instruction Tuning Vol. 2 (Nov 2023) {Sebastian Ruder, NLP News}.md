#article 
Link: https://newsletter.ruder.io/p/instruction-tuning-vol-2
Previously: [[Instruction Tuning Vol. 1 (October 2023) - {Sebastian Ruder, NLP News blog}]]

----

# Characteristics of Instruction-Tuning Data
Let's get on with the instruction-tuning! There are a few things to consider when using these recent datasets:
1. ==Data source==: How was this data obtained? Many have been generated using ChatGPT, meaning they inherit biases of the source model, or might noisy. Human-written examples are more expensive to obtain but are more high-quality.
2. ==Data quality==: Was any filtering done to improve the quality of the generated data? In most cases, filtering is based on simple heuristics or a pre-trained model, which can result in noisy data.
3. ==Domain and language coverage==: Most datasets cover general QA-style use cases and are in English, but similar methods can be used to obtain data in other domains or languages.
4. ==Number of dialog turns==: A "dialog turn" is an utterance by one speaker. Most datasets are single-turn; they consist of a prompt and a single response. Multi-turn may be necessary to train a more conversational model.
5. ==License terms==: Data generated using OpenAI models is subject to OpenAI's terms of use, which prohibit using the data to develop competing models. So look for data with a more permissive license to avoid legal complications!


# The Latest Instruction-Tuning Datasets


## [[Alpaca]] Data (2023)
- (Released with the Alpaca paper)
- 52k synthetic English instruction examples generated using OpenAI's text-davinci-003 with [[Self-Instruct]].
- The authors applied some modifications to simplify the data generation pipeline and lower costs -- the final data costs less than $500 to generate!

## [[Evol-Instruct]] (2023)
- A ***rewritten*** set of 250k English instruction-response pairs based on the Alpaca data.
- Instructions are rewritten to:
	- Make them more complex
	- Create a new, more-specialized instruction by prompting ChatGPT.
- In the next step, ChatGPT is used to generate the corresponding responses.
- Low-quality instruction-response pairs are filtered using heuristics.
- The process is repeated three times.

## [[Vicuna]] ShareGPT data (2023)
- 70k English conversations shared by users and scraped from [[ShareGPT]]
- Pre-processing includes converting HTML to markdown, filtering low-quality samples, and splitting lengthy conversations into smaller segments.
- Compared to single-turn datasets, the ShareGPT conversations often consist of *multiple turns* and are thus more useful for training models to leverage the context of the conversation.
- The conversations uploaded to this site may still be owned by the users, so their use is potentially problematic.


## Baize data (2023)
- 54k and 57k English multi-turn dialog examples (3.4 turns on average) generated with ChatGPT using questions from Quora and StackOverflow as seeds.
- ChatGPT simulates both the human and AI participants of the conversation.
- In addition, they generated 47k dialogs in the medical domain based on MedQuAD questions.
![[Pasted image 20240424015209.png]]

## Databricks [[Dolly]] (2023)
- 15k English instruction-following examples written by Databricks employees.
- Both instructions and answers are human-generated.
- This is in contrast to the other datasets above, where instruction and/or responses are generated by ChatGPT.
- Examples cover 7 use cases (openQA, closedQA, information extraction and summarization of Wikipedia data, brainstorming, classification, creative writing).
- Permissively licensed.
![[Pasted image 20240424113026.png]]

## [[oasst1|OpenAssistant Conversations]] (oasst1) (2023)
- 11k crowd-sourced multi-lingual instruction-following conversations, with human-generated messages for both the assistant and human participant.
- Multilingual (42.8% English, 31.4% Spanish, the rest in other languages)
- Annotators annotated the quality of prompts and responses, after annotators were provided with detailed guidelines for both writing the prompts and acting as the assistant.

# [[LIMA]] data (2023)
- 1k training and 300 test answer-response pairs mostly sampled from StackExchange, wikiHow, and the Pushshift Reddit dataset with around 400 written by the paper authors.
- A nice observation of this study is that ==training on small sets of highly-curated high-quality instruction data outperforms training on the much larger, noisier Alpaca data==

![[Pasted image 20240424114335.png]]

Takeaways:
- ==Quality > Quantity==: As [[LIMA]] showed, training on sets of (smaller, even) high-quality data outperforms instruction-tuning on larger, noisier data. Using more diverse prompts and quality filtering both improve performance too.
- ==Imitation != Mastery==: Models that are instruction-tuned on ChatGPT-generated data mimic ChatGPT's style (and thus might fool human raters), but *not* its factuality (The False Promise of Imitating Proprietary LLMs, 2023), performing worse on standard benchmarks. Using stronger base models is the best way to address this.
- ==The stronger the base, the better==: More powerful base models also produce stronger instruction-tuned models ([[Tulu]] using a 65B model)
- ==The combination wins!==: Combining multiple instruction-tuning datasets results in the best average performance across tasks ([[Tulu]]). Dataset mixing and developing modular instruction-tuned models are thus important research directions.


# Future Directions
- We still lack a clear understanding of what makes a good instruction and good instruction-response pairs. There's a lot of anecdotal knowledge when it comes to creating good model prompts, but it's unclear (?) how to create more high-quality instruction-following data in a more principled manner.

- To improve model performance, we still need to develop more reliable methods to identify high-quality examples and filter out undesirable ones. In a similar vein, it's important to develop methods that allow us to identify how a particular instance affects model behavior and alignment at test time.

- In light of the biases of both human and automatic evaluations, there's no clear gold standard for how to evaluate instruction-tuned models.
- Evaluating a model on a set of tasks that can be efficiently and automatically evaluated is one way to side-step this issue (LMentry, M2C, IFEval), but these are restricted to a certain set of use cases.
