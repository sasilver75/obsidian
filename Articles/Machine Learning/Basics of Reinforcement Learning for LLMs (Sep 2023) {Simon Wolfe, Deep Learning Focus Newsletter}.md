#article 
Link: https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning

----

Recent AI research has revealed that [[Reinforcement Learning]], and more specifically, [[Reinforcement Learning from Human Feedback]] (RLHF) is a key component of training a SotA LLM.

Despite this, most open-source models have relied heavily on supervised learning strategies, like [[Supervised Fine-Tuning]]. 
- The lack of emphasis on reinforcement learning can be attributed to several factors, including the necessity to curate a large amount of human preference data.
- There's underlying skepticism towards reinforcement learning generally, because people aren't as familiar with it as they are with supervised learning -- people like to stick with the approaches they know best!

In this theories, we'll start with some basic definitions and approaches, and work our way towards more modern algorithms (e.g. [[Proximal Policy Optimization|PPO]]) that are used to finetune language models with [[RLHF]].

![[Pasted image 20240208213922.png]]

# What is reinforcement learning (RL)?
- At the highest level, RL is just another way of training a machine learning model. In prior overviews, we've seen a variety of techniques for training NNs, but the two most-commonly-used techniques for language models are supervised and self-supervised learning.

- In (Self)Supervised Learning, we have a dataset of inputs (eg a sequence of text), with corresponding labels (e.g. a classification or completion of the input text), and we want to train our model to accurately predict those labels from the input.
	- Example: Maybe we want to fine-tune a language model to classify sentences that contain explicit language. In this case, we can obtain a dataset of sentences with binary labels indicating whether the sentence contains explicit language or not.
		- Then, we can train our model to classify this data correctly by iteratively:
			1. Sampling a mini-batch of data from the dataset
			2. Predict the labels with the model
			3. Compute the loss (e.g. CrossEntropy)
			4. Backpropagate the gradient through the model
			5. Performing a weight update

- ==SELF-supervised learning is similar to the setup explained above, but there are no explicit labels without our dataset -- instead, the "labels" we use are already present within the input data==! It's basically a =="hack"== to use parts of the existing data as a label (eg by masking an intermediary word, masking a patch in an image, rearranging image patches, trying to predict the next token).

When is RL useful?
- Although RL is just a useful way of training a neural net, the training setup is different compared to supervised learning!
- Similarly to humans, RL trains neural networks through trial and error. The NN will produce an output, receive some feedback about the output, and learn from the feedback.
	- In the case of a language model, the neural network will produce an output, receive some feedback about this output, then learn from the feedback.
	- In our context, when we finetune a language model using [[Reinforcement Learning from Human Feedback]], the language model produces some text, receives a score/reward from a human annotator, and then uses RL to finetune the language model to generate outputs with high scores.


![[Pasted image 20240208221014.png]]
In a RL environment, the environment is not differentiable like it is in a supervised learning context.

In this case, ==we can't apply a loss function that trains the LM to maximize human preferences just using supervised learning== -- why?
- ==The reason is that the score that we get from the human is a little bit of a black box -- there's no easy way for us to explain this score or connect it mathematically to the output of the neural network!==
	- In other words, ==we can't backpropagate a loss applied to this score through the rest of the neural network.== This would require that we are able to differentiate the system that generates the score, which is a human that subjectively evaluates the generated text.

Big picture: This begins to provide us with insight as to why RL is such a beautiful and promising learning algorithm for neural networks!
- ==REINFORCEMENT LEARNING MEANS THAT WE CAN LEARN FROM ARBITRARY FEEDBACK; THAT WE CAN LEARN FROM SIGNALS THAT ARE NON-DIFFERENTIABLE, AND THEREFORE NOT COMPATIBLE WITH SUPERVISED LEARNING.==

In the case of RLHF, we can score the outputs generated by a language model using whatever principle we might have in mind, then use RL to learn from the scores, no matter how we choose to define them. In this way, we can teach a language model to be helpful, harmless, honest, more capable through tool use, and more.


# A formal framework for RL

![[Pasted image 20240208223352.png]]
The agent acts and receives rewards (and a new state) from the environment.
((Sam: Given a state, take an action and receive both a reward and a new state resulting from your actions))

Problems that are solved with RL tend to be structured in a similar format to above; namely, we have ==an agent that is interacting with an environment==. The agent has a *state* in the environment and produces *actions*, which can modify the current state and yield a reward.

As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. ==The agent's goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent.==
- It's true that ==rewards may have a long horizon==, meaning that it might take several "correct," consecutive actions to generate positive reward.
	- Making a good move in the middle of a chess game may end up causing the win, but it takes a while for it to play out, assuming you don't make any more mistakes.


## Markov Decision Processes (MDP)
- To make things more formal and mathematically sound, let's formulate the system as a [[Markov Decision Process]] (MDP). Within an MDP, we have states, actions, rewards, transitions, and a policy; see below:

![[Pasted image 20240208224152.png]]
- States and actions have discrete values, while rewards are real numbers
- In an MDP, we define two types functions; ==transition functions== and ==policy functions==.
- The ==policy function takes a state as input, and outputs a probability distribution over possible actions==. Given this output, we can then make a decision for the action to be taken from a current state.
	- The ==transition function== then outputs the next state, based upon the previous state and the chosen action.

![[Pasted image 20240208224901.png]]
==((Above: So given a state `s`, we determine a probability distribution of actions we can take with our `policy function`, given that state. We choose some action `a` from that distribution. Given our `(s,a)`, we use out `transition function` to determine our next state `s`. We are possibly given a reward `r` for reaching the new state.))==

One thing we might be wondering after looking at the diagram above: *"What is the difference between the agent and the policy?* The distinction is a bit nuanced, but ==we can think of the agent as *implementing* the policy in the environment.==

As an agent interacts with the environment, we form a "==trajectory==" of state/action pairs that are chosen throughout this process. Then, given the reward associated with each of these states, we can get a total reward as given by the equation below:

![[Pasted image 20240208230341.png]]
Above: ==This return is the summed up rewards across the agent's full trajectory, but rewards achieved at later time steps are exponentially discounted by the factor Â¥;== see below for more.

The goal of RL is to maximize this return!

As shown by the equation below, we can characterize this by finding a policy that maximizes the return over trajectories that are sampled from the final policy.

![[Pasted image 20240208230801.png]]
((I think the way to read the first thing is "Maximize the following function by varying the policy"))

As a simplified example of the setup described above, let's consider training a NN to navigate a 2x3 grid from an initial state to a final state:
![[Pasted image 20240208230952.png]]
- The state will be given by the current position in the grid; let's represent this as a [[One-Hot Encoding]] vector. 
- Let's implement our policy using a feed-forward neural network that takes the current one-hot position as input and predicts a probability distribution over potential actions (ie move up, move down, move left, move right).
- For each chosen action, the transition function simply moves the agent to the corresponding next position on the grid and avoids allowing the agent to move out of bounds.
- The optimal agent, after training, learns to reach the final state without passing through the red square. ((I think the optimal agent greedily samples the policy at each point until it hits the end state, and then calculates the reward of its trajectory.))

Note that this problem has an environment that isn't differentiable, and contains long-term dependencies -- it has to perform several sequential actions to get any reward.

How does this apply to language models?

![[Pasted image 20240208231412.png]]

As discussed extensively in prior interviews, LMs specialize in next token prediction. Our LM takes several tokens as input (a prefix) and predicts the next token based on this context. This is all done autoregressively, meaning that the language model:
1. Predicts the next token
2. Adds the next token to the context
3. Repeats

To view this setup through the RL lens, ==we consider the language model to be our policy==!
Our state is just the current textual sequence in the context. Given the state, the language model creates a probability distribution over *the next token* and selects the next action to take (the next token). ==Once a full textual sequence has been produced, we can obtain a reward by *rating* the quality of the language model's output, either with a human or a reward model that's been *trained* by human preferences.==

This setup at first feels different from learning from the gridworld as before, but we begin to see that the problem formulation used for RL is quite generic -- it can solve many different problems.
((Okay, but how do we use the reward from the human to update the language model's parameters? It's not like we can use backpropagation, can we?))
- (((Later Sam Answer: It's because the reward that's actually being provided in RLHF isn't from a black box nondifferentiable human; it's from a separate reward model TRAINED on human feedback -- and that REWARD MODEL is itself differntiable!)))

### Important Terms and Definitions
- ==Trajectory==: The sequence of states and actions that describe the path taken by an agent through an ecosystem.
- ==Episode==: Sometimes the environment that we're exploring has a well-defined end state; e.g. reaching the final destination in our 2x3 gridworld. In that case, we refer to the trajectory of actions and states from start-end state as an "episode."
- ==Return==: Return is just the reward summed up over an entire trajectory -- this sum typically also includes a *discount factor*; intuitively, this means that current rewards are more valuable than later rewards. 
- ==Discount Factor==: The concept of discounting goes *beyond* RL, and refers to the basic idea of determining the current value of a future reward. This disincentivizes random wandering before getting to reward-bearing states.
- ==On vs. Off-Policy==: In RL, we have a target policy that describes the policy that our agent is aiming to learn. Additionally, we have a behavior policy that is being used by the agent to select actions as it interacts with the environment. 
	- The distinction here is subtle, but it lies in whether the behavior policy used to select actions as the agent navigates the environment during RL is the same (on-policy) as the target policy that we are trying to evaluate and improve or not (off-policy).
- ==Îµ-Greedy Policy==: RL trains a NN via interaction with an environment. The policy that the neural network implements takes a current state and produces a probability distribution over possible actions. But given that probability distribution, how do we choose which to use? One of the most common approaches is the Îµ-Greedy Policy, which greedily selects the "most probable" (highest expected return) action 1-Îµ % of the time, and selects a random action otherwise (Îµ % of the time).
	- This approach balances Exploration and Exploitation by allowing the agent to explore new actions in addition to those that it knows to work well.

# Q-Learning: A Simple Introduction to RL
- The [[Q-Learning]] algorithm is simple enough to be a good introduction to the topic of RL algorithms; afterwards, we'll learn about Deep Q-Learning (which is just a system that trains a deep neural network with RL).

### Q-Learning: Modeling Q Values with a Lookup Table
- [[Q-Learning]] is a ==model-free== RL algorithm, meaning that we don't have to learn a model for the environment with which the agent interacts... It means that we don't need to train a model to estimate the transition or reward functions -- these are instead just ==*given to us as the agent interacts with the environment.*== (recall: The transition function tells you what state you end up in as you take an action from a state, and the reward function tells you how valuable it is to be in a position.)
- ==The goal of Q-Learning is to learn the value of any action at a particular state.== We do this through learning a [[Q-Function]], which ==defines the value of a state-action pair as the expected return of taking that action at the current state under a certain policy and continuing afterwards, according to the same policy.==
	- Q(s,a) = expected future accumulated value that you might get by taking action a from state s, under your current policy.

![[Pasted image 20240209011019.png]]

- To learn this Q-function, we create a *lookup table* for the state-action pairs!
	- Each row in this table represents a unique state, and each column denotes the unique actions that you can take from any state. The value within each entry of the lookup table represents the ==Q-Values== (i.e. the output of the Q function0 for a particular state-action pair.
	- These Q-Values are initialized at zero and then updated using the [[Bellman Equation]]! As the agent continues to interact with the environment, these values eventually become optimal.


![[Pasted image 20240209012124.png]]

### The Q-Learning Algorithm
- The first step is to initialize our Q Values at 0 and pick an initial state from which to start hte learning process. Then we iterate:
1. Pick an action to execute from the current state (using an Îµ-Greedy policy)
2. Get a reward and next state from the (model-free) environment
	- Recall that a model-free environment implies that we get both the transition functions (what state taking an action from a state lands you in) and the reward functions (how much reward you get for reaching state s)  for free.
3. Update your Q-value based on the [[Bellman Equation]].

As shown in the figure above, our update to the Q value considers:
1. The reward of the current action
2. The Q value of the current state
3. The Q value of the next state

However... what should the Q value of the next state be? The agent might take any one of multiple actions from the next state, so how can we know its value?

In [[Q-Learning]], we choose to use the maximum Q value, as shown below:

![[Pasted image 20240209012706.png]]
Interestingly, Q-learning uses an Îµ-greedy policy when selecting actions, allowing new states and actions to be explored with a certain probability.

When computing Q value updates, however, we always consider the next next action with the maximum Q value, which may or may not be executed from the next state... In other words, Q-learning estimates the return for state-action pairs by *assuming* a greedy policy that just selects the highest-return action at the next state, though we don't follow such an approach when actually selecting an action. ==For this reason, Q-learning is an off-policy learning algorithm.== (Meaning it explores differently than it ultimately exploits, once it has a learned policy).
- This above update rule for Q-learning is mathematically guaranteed to find an optimal policy for any finite [[Markov Decision Process|MDP]].

#### Deep Q-Learning
- The foundation of [[Deep Q-Learning]] (DQL) lies in the vanilla Q-learning algorithm above. DQL is just an extension of Q-learning for deep reinforcement learning, meaning we use an approach similar to Q-learning to train a deep neural network.
- Now that we're using a more powerful model than a lookup table, Deep Q-Learning can actually be leveraged in interesting (but still relatively simple) practical applications. Let's look at this algorithm and a few related applications that might be of interest.

The problem with Q-learning:
- The size of the lookup table that we use for Q-learnign is dependent on both the total number of states and actions that exist in an environment.
	- ==In a complex environment like a high-resolution video game, or in real life, maintaining a lookup table as in classical Q-learning is intractable, and a more scalable approach is needed!==



==DQL solves this problem by modeling the Q function using a neural network, rather than with a lookup table!==
- This neural network takes the current state as input and predicts the Q values of all possible actions from that state as an output. DQL eliminates the need to store a massive lookup table; we instead just store the parameters of our neural network, and use it to predict Q values.

![[Pasted image 20240209165648.png]]
![[Pasted image 20240209165625.png]]
Above: See 

The Deep Q-Learning algorithm:
- We have ***two*** neural networks: 
	- The ==Q Network==
	- The ==Target Network==
- These networks are identical, but the exact architecture they use may depend on the problem being solved... To train them, we first gather data by interacting with the environment. This data is gathered using the current Q network with an epsilon-greedy policy. This process of gathering interaction data for training the Q network is referred to as ==experience replay==.
- From here, we use that data to train the Q network.
	- During each training iteration, we sample a batch of data and pass it through the Q network and the Target network. 
		- ==The Q network takes the *current state* as the input and *predicts the Q value of the action that is taken* (predicted Q value).==
		- ==The target network takes the *next state* as the input and *predicts the Q value of the best action that can be taken from that state* (target Q value).==

![[Pasted image 20240209170051.png]]
- From here, we use the predicted Q value, the target Q value, and the observed reward ot train the Q network using an MSE loss.
	- The target network is held fixed, but every several iterations, the weights of the Q network are copied to the target network, allowing this model to be updated as well. ((Question: These models are predicting different things, despite having the same architecture. How can transferring over the weights not fuck shit up?))
	- Then we just repeat this process until the Q network converges.

Why do we need the target network?
- The vanilla Q-learning framework leverages two Q values in its update rule:
	- A (predicted) Q value for the current state-action pair and the (target) Q value of the best state-action pair for the next state.
- In DQL, we similarly have to generate each of those two Q values.

This idea of using a separate network to produce a training target for another neural network (referred to as [[Distillation|Knowledge Distillation]]) is heavily used in deep learning.

Practical applications:
- DQL is a deep RL framework that's been used for several interesting practical applications; early on, it was for playing Atari breakout by [[DeepMind]].


## Final Remarks
- With a basic understanding of RL and the associated problem formulation, we start to see how such problems can be solved by algorithms like (deep) Q-learning.
- Although RL is a complex topic, the algorithms and formulations we've studied so far are quite simple.
	- Over the course of coming interviews, we will slowly build upon these concepts, eventually arriving at the algorithms we use today to finetune language models.


