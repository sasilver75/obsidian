Link: https://huyenchip.com/2023/10/10/multimodal.html#part_1_understanding_multimodal

----

For a long time, each ML model operated in one data mode -- text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).

However, natural intelligence isn't limited to a single modality! Humans can read, talk and see. Incorporating additional modalities into LLMs is viewed as a key frontier in AI research and development.

These systems are called LMMs (Large Multimodal Models).

Multimodal can mean one or more of the following:
1. Input and output are of different modalities (text-to-image, image-to-text)
2. Inputs are multimodal (a system processes both text and images)
3. Outputs are multimodal (a system can generate both text and images)

Let's look at:
1. The context for multimodality, including why we should care about modalities, different data modalities, and types of multimodal tasks.
2. The fundamentals of multimodal systems, looking at [[CLIP]] and [[Flamingo]]
3. Active research areas for LLMs, including generating multimodal outputs, and adapters for more efficient multimodal training, covering newer multimodal systems.

# Part 1: Understanding Multimodal



# Part 2: Fundamentals of Multimodal Training


# Part 3: Research Directions for LMMs






