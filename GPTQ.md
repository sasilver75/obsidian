October 31, 2022
ETH Zurich, IST Austria
Paper: [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)

"This merges the name of the OPT model family with the abbreviation for [[Post-Training Quantization]] (PTQ)"
