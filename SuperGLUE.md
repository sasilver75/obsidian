May 2, 2019 (13 months after [[GLUE]]) -- NYU, [[Meta AI Research]], [[DeepMind]]
Succeeded [[GLUE]], which saturated (model > humans in < 1 year). SuperGLUE instead lasted a few years.
Paper: [SuperGLUE: A stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537)

May 2019 benchmark for understanding Language Model performance on downstream tasks. Styled after [[GLUE]], but with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard.

Possible leaderboard: https://super.gluebenchmark.com/leaderboard/
(I believe 87.1 is a human score)

Abstract
> In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available atÂ [this http URL](http://super.gluebenchmark.com/).

![[Pasted image 20240420143159.png]]