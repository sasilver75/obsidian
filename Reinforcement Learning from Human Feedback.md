---
aliases:
  - RLHF
---
March 4, 2022 -- [[OpenAI]]
Paper: [Training Language Models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

![[Pasted image 20240415231812.png]]

See also: [[Reinforcement Learning from Human Feedback with AI Feedback]] (RLAIF)
