---
aliases:
  - KTO
---
February 2, 2024
Stanford (*Ethayarajh et al.*)
[KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306)

> "What I did was, instead of using a learned reward model, I'd just use a dummy reward model with +1 for good examples and -1 for bad examples, and I used this dummy reward model to together with PPO, but in an offline way with no active sampling; it was very comparable to DPO... if you go by an orthodox view that the *rewards* really matter, this shouldn't have worked well; but it did! The quality for my test is basically about the same as DPO up to an LLM size about 30B parameters... This gave some confirmation that there's something more going on here. He played around with the PPO/DPO objective, rewriting them to come up with a common form... and realized that they, along with some of the lesser-known methods that had been proposed, all had some commonalities: There existed a reference point in relation to which the reward of an example is measured, the shape of the function was concave in gains and sometimes convex in losses (meaning you get less sensitive the further you go from a reference point)), and there was usually some element of loss aversion, where there was greater penalization of bad outputs compared to gain for good outputs. If you step back and look like this function, it looked a lot to me like the classic human value functions from Prospect Theory (this is a theory from behavioral economics from K&T where they wanted to understand why people don't maximize their expected value (eg would take a sure $30 over a 1/2 chance at $100 for $50). The three hallmarks of prospect theory are the three things we described above.)." If you use that KT value function and specify the reference point as the sort of average reward, rather than just the reward of the negative example (as in the case of DPO), you get this KTO function. Nice things: You don't need pairs of preferences anymore; in DPO you need (x,accepted,rejected) -- in KTO, you just need a binary label of accepted/rejected, which is useful for cases like email blasts, where either a user clicked on it or didn't. Also, with KTO "you're optimizing for human utility, rather than the likelihood of preferences." You can also have datasets with 1% good and 99% bad. Much more practical. We found that KTO was able to match or exceed DPO in the 1-30B model parameter range we tested (Pythia, Mistral). This held even when you have heavily inbalanced data (where, in the DPO setup, you have evenly-balanced good/pair pairs). 
> Authors of the [[Orca-Math]] find that the [[Kahneman-Tversky Optimization|KTO]]-aligned model is way better than the [[Direct Preference Optimization|DPO]]-aligned model, especially if you use noisy data; the difference could be as big as 20 points in terms of final performance... So Mathematical reasoning is a big one.
> A team at UCLA adapted KTO to diffusion models! Humans preferred KTO-aligned to DPO-aligned for a 70-30% margin.
> "If you took the paired DPO data and broke it up into individual examples and trained using KTO, you can match or exceed DPO performance, even if you adjust for the increased data volume"
> "If your dataset has enough noise (oops or inferred preferenced) or intransitivity, KTO can work better than DPO, because it essentially avoids things that are too hard to learn, whereas DPO will just fit to the noise. This makes it robust to real-world data where data is noisy and intransitive enough that KTO is able to win out."
> "Theoretical advantage is that KTO can ignore things that are too hard to learn... if you have a preference that runs the wrong way of what you'd expect, the KTO update will be very small; but if it's noise and not something you actually want to learn, this lets you ignore it! But that's a double edged sword if you *do* want to learn it; in those cases you can set the risk-aversion Beta hyperparameter to be small and run for more epochs than you usually would to fit this more complex feedback distribution more neatly."
> In real world, you want to take advantage of data that you have lying around, which is where KTO really shines. In academia, we have benchmarks that we want to maximize performance on... and we basically create data using GPT4 or Claude or something; this isn't representative of how alignment is often done in the real world. We'd like to see more realistic alignment datasets, including some with much weaker signals (eg a binary signal, or a Likert Scale from 1-5); this would help bridge the divide from academia to industry with respect to alignment research.

References:
- Video: [Vertex Ventures Neural Notes: KTO - Helping AI make decisions more like a human](https://youtu.be/dDQAYmObK-Y?si=0rs_uM0by_0r9qd1)
	- Discussion with the lead author, Kawin Ethayarajh

While [[Proximal Policy Optimization|PPO]], [[Direct Preference Optimization|DPO]], and [[Identity-Mapping Preference Optimization|IPO]] require pairs of accepted vs rejected generations, KTO just needs a binary label (accepted or rejected), hence allowing us to scale to much more data.