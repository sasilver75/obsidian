---
aliases:
  - QLoRA
---
==This results in massive reductions in memory requirement -- enabling the training/fine-tuning of models as large as 70 billion parameters on just 2x NVIDIA RTX 3090s, which would originally take more than 16x A100-80GB GPUs!==
 It enables a finetuning of a 65B parameter model on a single 48GB GPU, while preserving full 16-bit fine-tuning task performance.

Related: [[Low-Rank Adaptation]]