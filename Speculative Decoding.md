Feb 2, 2023
DeepMind Paper: [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318)

Additional References
- [Video: Speculative Decoding: When two LLMs are Faster than One](https://www.youtube.com/watch?v=S-8yr_RibJ4)
- [Blog: Philkrav's Speculative Decoding post](https://philkrav.com/posts/speculative/)

Note: 
- I currently have [[Speculative Sampling]] as a separate vocabulary term, but I'm not sure if that's appropriate.
- Interestingly, two concurrent papers both introduced these papers(Leviathan et al, 2022 and Chen et al, 2023) from Google Research and Deepmind respectively

----

Notes:
- Notes from Philkrav's Speculative Decoding post
	- Autoregressive sampling is typically memory-bandwidth-bound because tokens are sampled one-by-one; every new token requires a forward pass of the *entire model* on the most recent token.
	- The basic premise in speculative decoding is to produce multiple tokens per forward pass of our model by using a cheaper "draft" model to produce $k$ draft tokens, which may or may not be the correct continuation. Then our main, expensive model will check whether the tokens are right, all at once -- evaluating the likelihoods of a short sequence of $k$ tokens in parallel has a similar latency to sampling a single token.
	- Algorithm:
		- Given: target model distribution is $p$, draft model distribution is $q$. $p(x)$ is shorthand for $p(x_t|x_0...x_{t-1})$ 
		- In a loop:
			- Produce $k$ draft tokens using the draft model $q$. Evaluate their likelihoods under $p$, which produces $k+1$ logits. For each token:
				- If $p(x) > q(x)$ : Accept the token and continue
				- Otherwise, accept the token with probability $p(x)/q(x)$ . If it's accepted, move on to the next draft token.
			- On the first rejection, sample a token from the modified distribution $p'(x) = norm(max(0, p(x) - q(x))$ and exit.
			- If no tokens are rejected, sample an additional token from $p(x)$ (remember, we get k+1 logits, not just k!)
	- Note that $p$ and $q$ are distributions over tokens, and they should be measured *after* top-k/top-p adjustments to LLM token probabilities. 
	- So how do we get a high acceptance rate of our draft model's predictions? The acceptance rate increases when the draft model more accurately matches the target model; to this end, we can use [[Distillation|Knowledge Distillation]] to train our draft model to reproduce the target's model outputs.
		- Something interesting in the blog post about how we should actually use the *reverse* [[Kullback-Leibler Divergence|KL-Divergence]], rather than the usual *forward* KL Divergence; an interesting comment on the student model failing to "cover" the modes of the target distribution.


Abstract
> We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.



![[Pasted image 20240424185642.png]]
Above: Both DeepMind and Google Research ran experiments and found ~2-3x speedups, recommending k values of ~3-5.


((Not sure the below are relevant))
To cover:
- Theshold decoding
- Staged speculative decoded
- Guided generation
- Lookahead decoding 
- Prompt lookup decoding


# Non-paper Figures

![[Pasted image 20240514011212.png]]
![[Pasted image 20240614224334.png]]
Above:
- In speculative decoding, we use a smaller model (like in [[Contrastive Decoding]]) to decide what to generate (unlike in CD, where we decide what *not* to generate)! But occasionally the larger model might decide to go in another direction.
- The smaller model generates blocks of tokens, and the larger model checks its work, and determines if the generation is "in distribution." The larger model can choose to reject proposals and generate a token itself.