---
aliases:
  - Speculative Sampling
---


November 30, 2022
[[Google Research]] (Leviathan et al)
[Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
*and*
Feb 2, 2023
[[DeepMind]] (Chen et al)
[Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318)

A variety of token speculation techniques exist


References
- [Video: Speculative Decoding: When two LLMs are Faster than One](https://www.youtube.com/watch?v=S-8yr_RibJ4)
- [Blog: Philkrav's Speculative Decoding post](https://philkrav.com/posts/speculative/)
- Github Repo Index: [hemingkx's SpeculativeDecodingPapers Repo](https://github.com/hemingkx/SpeculativeDecodingPapers)
- Great animation: https://x.com/GoogleAI/status/1865172246534217941

Papers
- (These Google and Deepmind papers at the top)
- [[Medusa]]
- [[Hydra]]
- [[EAGLE]]
- [[Better and Faster Large Language Models via Multi-token Prediction]]

----

Notes:
- Notes from Philkrav's Speculative Decoding post
	- Autoregressive sampling is typically memory-bandwidth-bound because tokens are sampled one-by-one; every new token requires a forward pass of the *entire model* on the most recent token.
	- The basic premise in speculative decoding is to produce multiple tokens per forward pass of our model by using a cheaper "draft" model to produce $k$ draft tokens, which may or may not be the correct continuation. Then our main, expensive model will check whether the tokens are right, all at once -- evaluating the likelihoods of a short sequence of $k$ tokens in parallel has a similar latency to sampling a single token.
	- Algorithm:
		- Given: target model distribution is $p$, draft model distribution is $q$. $p(x)$ is shorthand for $p(x_t|x_0...x_{t-1})$ 
		- In a loop:
			- Produce $k$ draft tokens using the draft model $q$. Evaluate their likelihoods under $p$, which produces $k+1$ logits. For each token:
				- If $p(x) > q(x)$ : Accept the token and continue
				- Otherwise, accept the token with probability $p(x)/q(x)$ . If it's accepted, move on to the next draft token.
			- On the first rejection, sample a token from the modified distribution $p'(x) = norm(max(0, p(x) - q(x))$ and exit.
			- If no tokens are rejected, sample an additional token from $p(x)$ (remember, we get k+1 logits, not just k!)
	- Note that $p$ and $q$ are distributions over tokens, and they should be measured *after* top-k/top-p adjustments to LLM token probabilities. 
	- So how do we get a high acceptance rate of our draft model's predictions? The acceptance rate increases when the draft model more accurately matches the target model; to this end, we can use [[Distillation|Knowledge Distillation]] to train our draft model to reproduce the target's model outputs.
		- Something interesting in the blog post about how we should actually use the *reverse* [[Kullback-Leibler Divergence|KL-Divergence]], rather than the usual *forward* KL Divergence; an interesting comment on the student model failing to "cover" the modes of the target distribution.


Abstract
> We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.



![[Pasted image 20240424185642.png]]
Above: Both DeepMind and Google Research ran experiments and found ~2-3x speedups, recommending k values of ~3-5.


((Not sure the below are relevant))
To cover:
- Theshold decoding
- Staged speculative decoded
- Guided generation
- Lookahead decoding 
- Prompt lookup decoding


# Non-paper Figures

![[Pasted image 20240514011212.png]]
![[Pasted image 20240614224334.png]]
Above:
- In speculative decoding, we use a smaller model (like in [[Contrastive Decoding]]) to decide what to generate (unlike in CD, where we decide what *not* to generate)! But occasionally the larger model might decide to go in another direction.
- The smaller model generates blocks of tokens, and the larger model checks its work, and determines if the generation is "in distribution." The larger model can choose to reject proposals and generate a token itself.
- ==Speculative decoding has no impact as to the *quality* of the model, because each prediction is  validated by the main model==

From Eugene Cheah/Pico's paper club
![[Pasted image 20240821120734.png|400]]

![[Pasted image 20240821120858.png|400]]
Speculative decoding is harder than it seems!
![[Pasted image 20240821120909.png|400]]
We save time, not compute, actually! (spend more compute time to save wall clock time)
![[Pasted image 20240821120955.png|300]]
Simplification: Your GPU goes through 3 states
- Complex math in L1 SRAM 
	- L1 SRAM is parts of memory typically closest to processor; fastest memory
- Read/Write between L1 SRAM and VRAM
- Wait around for other L1 SRAM/other threads
![[Pasted image 20240821121231.png|400]]
Most time is about waiting for memory!
So you start to see why doing large batches of tokens, thing are much faster (you do the memory fetch once into L1, do all of your computation... between 1 token at a time, and 12 tokens...)
So it is cheaper to process large batches of tokens, versus many tokens one at a time.

![[Pasted image 20240821121748.png|300]]
The point is to be able to use batching to compute multiple tokens at a time (using more compute), 

![[Pasted image 20240821123021.png|450]]
