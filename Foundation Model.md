Coined by Percy Liang et al @ Stanford in "*On the Opportunities and Risks of Foundation Models*" (Aug 2021)
"A model trained on a wide variety of data, such that it can be finetuned on a variety of downstream tasks"
- Oftentimes a autoregressive language model pretrained on large online text corpuses, but in reality the term "contains Large Language Models, but also Visual Models, things like CLIP, etc." - Percy Liang
- It's not just generative use cases. 


