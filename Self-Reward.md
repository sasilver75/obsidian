January 18, 2024
[[Meta AI Research]] and NYU
Paper: [Self-Rewarding Language Models](I grew up in MKE, and lived in MD for a couple years and KC for a couple years in my 20s before moving back home.)
#zotero 
Takeaway: A method of self-improving models where the same model is used for instruction-following generations as well as LLM-as-a-Judge feedback. A prompt is generated from a seed set (using another model), our model creates N generations/responses, rates them, and creates a pairwise preference dataset of `(prompt, best_y, worst_y)` using the best and worst generations from each prompt. We then use DPO to fine-tune the model, improving both the instruction-following and response-evaluating capabilities of the model. This entire process is repeated multiple times.

Comparison: Similar to [[Pairwise Cringe Optimization]] (PCO), except this paper's "reward model" co-evolves with the instruction-following component, whereas the PCO paper uses a fixed/frozen separate reward model.

Questions: IIRC they don't talk too much about when/why the iterative improvement plateaus, do they? 

References:
- [OxenAI Self Rewarding Language Models](https://www.youtube.com/watch?v=jQEo5A-Pdls)
----

Notes:
- Methods like RLHF and DPO are great, but they have two problems:
	1. Bottlenecked by the size and quality of the human preference dataset that was collected.
	2. (In RLHF) Bottlenecked by the quality of the *frozen* reward model
- This paper proposes to train a *self-improving reward model* that, rather than being frozen, is continually updated during LLM alignment. The key is to ==develop an agent that possesses all the abilities desired during training, rather than having a separate reward model and language model.==
	- Incorporating the reward model into the same system allows positive task transfer between the reward modeling task and the instruction-following tasks.
- ==Self-Rewarding Language Models== are able to both:
	1. Act as an instruction-following model, generating responses for given prompts.
	2. Can generate and evaluate new instruction-following examples to add to their own training dataset.
	- Authors then train these models using an Iterative [[Direct Preference Optimization|DPO]] framework as introduced in (Xu et al, 2023, the CRINGE loss paper!)
- ==Process== (for each iteration)
	- Candidate responses are generated by the model for newly-created prompts, and are then assigned rewards by that same model via LLM-as-a-Judge prompting.
	- A preference dataset is then built from the generated data, and the model for the next iteration results from fine-tuning the current model via DPO, using these preference pairs.
- Authors begin with a [[LLaMA 2]] 70B model fine-tuned on [[oasst1]], and then perform the training above. They find that both instruction-following and reward modeling abilities improve, meaning that the model, during iterative training, is able to provide higher quality preference datasets to itself than in the previous iteration.
- We aim to have the model possess two skills simultaneously, so that it can perform self-alignment, allowing it to iteratively train itself w/ AI Feedback:
	1. *==Instruction-following==*: Given a prompt that describes a user request, the ability to generate a high-quality, helpful/harmless response.
	2. ==*Self-Instruction creation*==: The ability to generate and *evaluate* new instruction-following examples to add to its own training set.  (Authors say this also includes LLM-as-a-Judge.)
- We start with a base model and a seed set of human-authored instruction-finetuning dataset, and use it to train the model in a SFT manner ("SFT dataset"). We also assume that we're provided a seed set of (evaluation instruction prompt, evaluation result response) examples which can be used to train the ability to fine-tune in the initial LLM-as-a-Judge function ("EFT dataset").
	- When performing in an LLM-as-a-Judge functionality, the prompt instructs the LLM to evaluate the response using five additive criteria: ==relevance, coverage, usefulness, clarity, and expertise==.
- Using this model, we can then make it self-modify its own training set, making it ==generate additional training data for the next iteration of training:==
	1. ==Generate a new prompt== using few-shot prompting, sampling prompts from the original seed IFT data (Similar to in [[Self-Instruct]] and [[Unnatural Instructions]])
		- ==NOTE==: To generate new prompts, they use a fixed model, LLaMA 2-Chat 70B with 8-shot prompting, following Self-Instruct (6 from seed, 2 from generated).
	2. ==Generate N candidate responses== for the given prompt using our model.
	3. ==Evaluate the candidate responses== using a \[0,5\] scale using the same model's LLM-as-a-Judge ability.
- Recapping the previous two points: Training is initially performed on a base model using seed IFT and EFT datasets. This is then augmented with additional data via AI (Self-)Feedback; They refer to augmenting the seed data with additional examples for training as ==AI Feedback Training (AIFT) data==.
- We construct *preference pairs* of the form `(instruction prompt x_i, winning response y_w, losing response y_l)`, forming the winning and losing pair by taking the *highest and lowest scoring responses* from the N evaluated candidate responses.
	- (This is interesting that we only pay attention to the best and worst generations, when it comes to training data for DPO)
- Our overall training procedure trains a series of models `M1, M2, ..., MT` where each successive model *t* uses augmented training data created by the *t-1'th* model:
	- $M_0$: Base pretrained LLM with no finetuning 
	- $M_1$: Initialized with $M_0$, then finetuned on the IFT+EFT seed data, using SFT.
	- $M_2$: Initialized with $M_1$, then trained with AIFT($M_1$) data using DPO.
	- $M_3$: Initialized with $M_2$, then trained with AIFT($M_2$) data using DPO.
- This iterative training resembles the procedure used in the ==Pairwise Cringe Optimization== paper, and is specifically termed ==Iterative DPO== (from the CRINGE paper).
- Sam Recap:
	- Basically start with an IFT/EFT'd base model
	- Generate a new prompt (using a separate LLaMA 2-Chat 70B model, using a strategy from Self-Instruct)
	- Generate responses for the new prompt using our model
	- Score the responses using our model, and construct a preference pair `(prompt, best_y, worst_y)`, using only the best/worst responses.
	- (repeat multiple times)
	- Run a round of DPO training on our model, improving the IFT and EFT abilities of the model
	- (repeat)
- Authors note that while Self-Reward modeling can substantially improve win rate in most categories, there are some tasks for which the approach *does not improve* (eg mathematics, logical reasoning), indicating that our current training approach mainly allows the models to better-utilize their existing knowledge.
	- ((There are some other strategies for improving mathematical/logical reasoning in a self-play/AIF manner, I believe. EG AlphaGeometry-type stuff))
- Authors note that the LLM-as-a-Judge prompt is very important; they tried the prompt proposed in the ==Self-Alignment with [[Instruction Backtranslation]] (2024)== paper (which describes the options as multiple choice options in a range of quality buckets, whereas this prompt describes the points as additive, covering various aspects of quality). There's a ==huge difference between the performance of these two prompts==, with *ours* having 65.1% pairwise accuracy, and only 26.6% pairwise accuracy for theirs!

Abstract
> We posit that to achieve superhuman agents, ==future models require superhuman feedback in order to provide an adequate training signal==. Current approaches commonly train reward ==models from human preferences, which may then be bottlenecked by human performance level==, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the ==language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training==. We show that ==during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself==. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.

# Paper Figures
![[Pasted image 20240507141407.png]]
Above: Two Steps (that are then repeated)
1. Given new prompts, *generate a number of responses* and *rate/rank them*, using the same model for both operations.
2. Use these preference pairs to DPO fine-tune the model

![[Pasted image 20240507150829.png]]
Above: The prompt used for the LLM-as-a-Judge functionality of our model. Recall that it generates N responses to the previously-generated prompt, then rates each of them using this prompt prior to constructing a (prompt, best_response, worst_response) preference pair, using only the best/worst (according to this prompt, which gives a \[0,5\] score) of the N generations that the model creates. See that they're basically using [[Chain of Thought|CoT]], by asking the model to justify its score prior to giving a rating. It's interesting that the scoring is so strongly guided by the prompt. Think: Does this leave nuance on the floor, or is it actually better to give our judge a shorter leash, like this?

![[Pasted image 20240507154058.png]]

![[Pasted image 20240507154541.png]]

![[Pasted image 20240507154557.png]]

