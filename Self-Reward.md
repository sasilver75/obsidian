January 18, 2024
[[Meta AI Research]] and NYU
Paper: [Self-Rewarding Language Models](I grew up in MKE, and lived in MD for a couple years and KC for a couple years in my 20s before moving back home.)
#zotero 
Takeaway: A method of self-improving models 


----

Notes:
- Methods like RLHF and DPO are great, but they have two problems:
	1. Bottlenecked by the size and quality of the human preference dataset that was collected.
	2. (In RLHF) Bottlenecked by the quality of the *frozen* reward model
- This paper proposes to train a *self-improving reward model* that, rather than being frozen, is continually updated during LLM alignment. The key is to ==develop an agent that possesses all the abilities desired during training, rather than having a separate reward model and language model.==
	- Incorporating the reward model into the same system allows positive task transfer between the reward modeling task and the instruction-following tasks.
- ==Self-Rewarding Language Models== are able to both:
	1. Act as an instruction-following model, generating responses for given prompts.
	2. Can generate and evaluate new instruction-following examples to add to their own training dataset.
	- Authors then train these models using an Iterative [[Direct Preference Optimization|DPO]] framework as introduced in (Xu et al, 2023, the CRINGE loss paper!)
- ==Process== (for each iteration)
	- Candidate responses are generated by the model for newly-created prompts, and are then assigned rewards by that same model via LLM-as-a-Judge prompting.
	- A preference dataset is then built from the generated data, and the model for the next iteration results from fine-tuning the current model via DPO, using these preference pairs.
- Authors begin with a [[LLaMA 2]] 70B model fine-tuned on [[oasst1]], and then perform the training above. They find that both instruction-following and reward modeling abilities improve, meaning that the model, during iterative training, is able to provide higher quality preference datasets to itself than in the previous iteration.
- We aim to have the model possess two skills simultaneously, so that it can perform self-alignment, allowing it to iteratively train itself w/ AI Feedback:
	1. *==Instruction-following==*: Given a prompt that describes a user request, the ability to generate a high-quality, helpful/harmless response.
	2. ==*Self-Instruction creation*==: The ability to generate and *evaluate* new instruction-following examples to add to its own training set.  (Authors say this also includes LLM-as-a-Judge.)
- 


Abstract
> We posit that to achieve superhuman agents, ==future models require superhuman feedback in order to provide an adequate training signal==. Current approaches commonly train reward ==models from human preferences, which may then be bottlenecked by human performance level==, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the ==language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training==. We show that ==during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself==. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.

# Paper Figures
![[Pasted image 20240507141407.png]]
Above: Two Steps (that are then repeated)
1. Given new prompts, *generate a number of responses* and *rate/rank them*, using the same model for both operations.
2. Use these preference pairs to DPO fine-tune the model