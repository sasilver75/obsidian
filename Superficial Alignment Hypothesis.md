---
aliases:
  - SAH
---
Proposed in the [[LIMA]] (May 2023) paper

Says that a models' capabilities are almost entirely learnt during pre-training, and alignment teaches it which subdistribution of formats should be used when interacting with users.

SAH simply states that alignment can be learned in a data-efficient manner given a set of examples with sufficient quality and diversity.