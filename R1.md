---
aliases:
  - R1-Zero
---


January 20, 2025
[[DeepSeek]]
Paper: [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)


References:
- [Jay Alammar's The Illustrated DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

--------

# Paper Figures/Notes



# Non-Paper Figures/Notes

![[Pasted image 20250128115738.png]]
They used a very large number (~600k) of "Long CoT Reasoning Examples", which are hard to come-by and expensive to label with humans at this scale. This data was allegedly created by a *precursor* to R1 (an unnamed sibling) specialized in reasoning ("This sibling is inspired by a third model called R1-Zero").

Creating reasoning models with large-scale RL happens in two steps:
![[Pasted image 20250128115840.png]]
1. Large-Scale Reasoning-Oriented RL (R1-Zero): RL is used to create the interim reasoning model. This is then used to generate the sFT reasoning examples. This model is possible because of an earlier experiment called ==DeepSeek R1-Zero==.
	- This model is special because it's able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through an RL trained process with no SFT step, and is still competitive with O1:
![[Pasted image 20250128120020.png]]

Points to two things:
- Modern base models have crossed a certain threshold of quality and capability. The [[DeepSeek V3]] model used was trained on 14.8 T high-quality tokens.
- (Some) reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. 

> Write Python code that takes a list of numbers, returns them in sorted order, but also adds 42 at the start.

- A software linter can check if the completion is proper Python code or not
- We can execute the Python code to see if it even runs
- Other modern coding LLMs can create unit tests to verify the desired behavior (without themselves being reasoning experts)
- We can measure execution time and make the training process prefer more performant solutions over other solutions, or use readability metrics (e.g. KC) to write more human-legible code, shaping the reward.

![[Pasted image 20250128120621.png]]
We can generate multiple such solutions, and then automatically check to see that the first one isn't even code, the second one is but doesn't solve the problem, the third one is plausible but fails the unit tests, while the fourth is correct. These are all signals that can be used to improve the model.

![[Pasted image 20250128120635.png]]
Above: We see that as the number of training steps increases, performance increases on [[AIME]] problems.

![[Pasted image 20250128120800.png]]
Correspondingly, we see that over steps of RL training, the model generates more thinking tokens to process the problem.

Despite [[R1|R1-Zero]] performing well on these reasoning problems, other issues make it less usable than desired:
> ==DeepSeek-R1-Zero struggles with challenges like poor readability and language mixing==.

The full [[R1]] model is meant to be more usable, so instead of relying *completely* on the RL process, it's used in *two places* as we've mentioned earlier in this section:
1. Creating an ==interim reasoning model== to generate SFT data points (inspired by R1-Zero)
2. Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers)

![[Pasted image 20250128121031.png]]

Creating SFT reasoning data using the interim reasoning model
- To make the interim reasoning model more useful, it goes through a supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems. (some of which are generated and filtered from [[R1|R1-Zero]]) -- the paper confusingly refers to this as ==Cold Start Data==.

> Cold Start
> Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.
> To collect such data, we've explored several approaches:
> - Using few-shot prompting with a long CoT as an example
> - Directly prompting models to generate detailed answers with reflection and verification
> - Gathering DeepSeek-R1-Zero outputs in a readable format and refining the results through post-processing by human annotators

![[Pasted image 20250128122540.png]]
But wait, if we have this data, why are we relying on the RL process?
- It's because of the scale of the data; the dataset might be 5,000 examples, but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows us to synthetically generate that extremely valuable data.

![[Pasted image 20250128122624.png]]
Above: We use special prompting, human annotation, and filtered responses from R1 Zero to genreate some SFT training data from which we create (via SFT) an interim reasoning model. This model is then used to create a larger amount of synthetic datas, which, after filtering, were about 600k examples.
- This filtered reasoning data was then used to bootstrap the LARGER R1.

![[Pasted image 20250128122800.png]]
Above: Example of what instruction data looks like

General RL Training Phase
- This general RL training phase (from the SFT checkmark resulting from training on the 600k interim-model-generated-examples) then proceeds:
![[Pasted image 20250128122915.png]]
Since this extends to non-reasoning applications, it utilizes a helpfulness and safety [[Reward Model]] for prompts that belong to these types of applications.

### Architecture
- Just like previous models from the dawn of GPT2, R1 is a stack of Transformer decoder blocks; 61 of them.
- The first 3 are dense, but the rest are [[Mixture of Experts]] layers.

![[Pasted image 20250128123815.png]]
More details are in their [[DeepSeek MoE]] and [[DeepSeek V3]] papers

















