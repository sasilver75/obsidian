---
aliases: []
---
Taking an existing pretrained model and pretraining it further on new domain data -- like taking a LLaMA 3 8B base model that's been trained on a general text corpus that we then want to adapt to finance/medical/legal/other domains. So it's more self-supervised learning on a different domain, but not IFT.