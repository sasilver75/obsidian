---
aliases:
  - DPO
---
May 2023
Paper: [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)


See the [[Zephyr]] paper "Zephyr: Direct Distillation of LM Alignment"