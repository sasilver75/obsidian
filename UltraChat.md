May 23, 2023
Tsinghua University (Ding et al)
[Enhancing Chat Language Models by Scaling High-Quality Instructional Conversations](https://arxiv.org/abs/2305.14233)
#zotero 
Takeaway: A totally-synthetic dataset of 1.47M multi-turn dialogues covering a wide range of topics/instructions in the context of user-agent interactions. Uses a top-down three-sector approach and separately-prompted User/Assistant GPT-3.5s.


A dataset of systematically designed, diverse, informative instructional conversations aiming to capture the breadth of interactions that a human might have with an AI assistant, including multi-turn conversations.
- Contains ==1.5 million high-quality multi-turn dialogues== covering a wide range of topics and instructions -- it's ==synthetically-generated data== covering technology, art, entrepreneurship, and many more. Superior in metrics like scale, average dialogue length, diversity, and coherence.
- Building on the UltraChat dataset, they finetune a [[LLaMA]] model into "UltraLLaMA" and show that it outperforms other open-source models. including [[Vicuna]], the previously recognized SoTA open-source model (as of this time).

In the Zephyr Paper, they talk about UltraChat (this should give us pause about the dataset quality):
> UltraChat is a self-refinement dataset of 1.47M multi-turn dialogues generated by GPT-3.5-Turbo over 30 topics and 20 types of text material. Initially after running dSFT over the whole corpus, they got a model that would preface most of its answers with "I don't have personal experiences," and "as a LLM," even for questions where it doesn't make sense to do that. We applied filters to focus on grammatical error (5% of the dataset), helpfulness and remove undesired model responses, resulting in a dataset containing ==only 200k examples==. 



Sam Note: This is a great and inspiring paper regarding thinking about making multi-turn user-assistant data in a top-down manner.

---

## Introduction
- Experimental evidence strongly suggests that chat language models can be effectively trained through instruction fine-tuning.
- In an attempt to capture the breadth of interactions that a human might have with an AI assistant, the authors don't use tasks like Question-Answering or Summarization to construct the data, instead curating three sectors:
	1. ==Questions about the World==
	2. ==Creation and Generation==
	3. ==Assistance on Existing Materials==
- Then we employ ==meta-information==, ==in-context expansion==, and ==iterative prompting== to scale up the number of instructions.
- To construct informative and realistic multi-turn conversations, ==two separate ChatGPT Turbo APIs are used to generate conversations, with one playing the role of the user, and the other generating the response==.
	- They construct the user model with carefully-designed prompts to mimic human user behavior and call the two APIs iteratively.
- Authors finetune [[LLaMA]]-13B into [[UltraLLaMA]], which consistently outperforms comparable open-source baselines.

## Related Work
- In instruction tuning, the [[T5]] and [[FLAN]] series of models did pioneering work on instruction tuning, expanded upon with [[FLAN-T5]]
- Authors later proposed to use [[Reinforcement Learning from Human Feedback|RLHF]]/[[Proximal Policy Optimization|PPO]] to language models after first learning a human reward model directly from annotated human feedback.
- In terms of data augmentation, papers like [[Self-Instruct]] and [[Alpaca]] put synthetic data for LMs on the map, generating 52k high-quality instruction-response pairs based on 175 seed tasks, and "distilling" Text-Davinci-003. A slew of papers followed.
	- Besides scaling data size, these works also diverge in their ways of prompt engineering to gather data (eg in CAMEL, designed a multi-agent role-play environment to for LLMs to solve a given complex task, producing 115k instruction-response pairs that simulate real human conversations).

## Design and Data Construction
- LLMs are believed to be better annotators than human-beings in many scenarios. Directly using LLMs like ChatGPT to generate multi-turn conversations can be satisfactory, but not informative, as it can't enjoy the benefit of reinforcement learning with human feedback in the alignment process.
- Two key points to ensure quality of data:
	1. ==An *opening line* directly determines the topic of the dialogue. Opening lines should be highly-diverse and encompass any task that a human user may request a chat model to perform.==
	2. ==A *user* determines the plot of the dialogue==, and the output should be tailored to the current topic with diverse language styles and requests.
- UltraChat aims to cover a tremendous range of instruction and questions, composed of three sectors (Unlike other datasets that use specific tasks, such as QA, rewriting, and summarization to construct the data):
	1. ==Questions about the World==
		- Focuses on querying existing information about the world. Includes a wide range of topics.
		- Approach to gathering data involves two perspectives -- one centered around *topics and concepts* and the other centered around *real-world entities.* 
			- Topic and Concepts: They ask ChatGPT to generate 30 comprehensive topics that encompass "various aspects of our daily lives", then prompt to generate 30-50 subtopics or related topics for each, then prompt to generate 10+ questions for each original question.
			- Real-World Entities: Derived from Wikidata entities, refined by considering their frequencies in Wikipedia articles, specifically focusing on the 10,000 most-frequently-occurring entities. For each entity, they create 5 meta-questions, followed by 10 more specific questions and 20 extended questions.
		- Authors filter and sample approximately 500k questions as opening lines.
		- During the construction of each dialogue, we prompt the user model to respond concisely and meaningfully, taking into account the ongoing dialogue history.
	2. ==Creation and Generation==
		- Creation of new information with human-input conditions. 
		- These text materials are categorized into 20 different types (eg Articles and Blog Posts, Screenplays, Emails, Programs and Code), and a ChatGPT model is used to produce a diverse range of instructions for each type of writing, and then 80% of the generated instructions are *further fed* back into the ChatGPT model to generate more-detailed instructions.
		- These instructions serve as opening lines for dialogue generation.
		- The user prompt continually reinforces the primary objective of the conversation, which is to generate and refine a piece of writing.
	3. ==Assistance on Existing Materials==
		- Modification of existing information, such as through rewriting, continuation, summarization, or inference.
		- We gather text pieces from the C4 corpus; each piece within the C4 corpus is associated with a source URL.
		- To ensure a diverse range of text content and styles, we adopt 20 material types (eg Articles and Blog Posts, Screenplays, Emails, Programs and Code) from the previous section and *manually curate keywords for each type*.
		- We collect 10,000 text pieces from the C4 corpus, and for each piece prompt ChatGPT to generate 5 distinct instructions. We combine the text pieces with specific instructions... ultimately resulting with a concatenated set of 500,000 pieces that serve as the opening lines for the generated dialogues.
- Maintaining the desired behavior of the user model is crucial for achieving successful automatic dialogue generation. 
	- It's been observed that when the user model is solely provided with the current dialogue history, it tends to assume the role of an AI assistant!
	- We include prompts explicitly instructing the model to adopt various user personalities. A prompt is employed to remind the model of the primary purpose of the dialogue.
		- ((==Note:== not clear to me if they're using a different prompt for the (eg) first and second+ generations, for either user or assistant sides))
- Once data generation is complete, a further filtration step is performed to ensure overall data quality, where we exclude polite statements like "Thank you," "Thanks," and "You're welcome."
- Authors train [[UltraLLaMA]] off of a [[LLaMA]]-13B base. They break each dialogue into smaller sequences, limiting them to the maximum length of 2048 tokens.
	- ((This sucks and is a huge red flag, no? Like you can't really benefit from coherent multi-turn interactions if you yourself don't have a context window to even accommodate them.))

## Evaluation


## Conclusion




Abstract
> Fine-tuning on ==instruction data== has been widely validated as an effective practice for implementing chat language models like ChatGPT. ==Scaling the diversity and quality of such data==, although straightforward, ==stands a great chance of leading to improved performance==. This paper aims to improve the upper bound of open-source models further. We first provide a ==systematically designed, diverse, informative, large-scale dataset of instructional conversations==, ==UltraChat==, which ==does not involve human queries==. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains ==1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions==. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. ==Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA==. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released.


# Paper Figures
![[Pasted image 20240720181724.png|600]]
The three sectors of data the authors choose are:
1. Questions about the World
2. Creation and Writing
3. Assistance on Materials

![[Pasted image 20240720190623.png|600]]
First sector: Questions about the World

![[Pasted image 20240720190640.png|600]]
20 types of text materials used for sector 2 and 3 (Creation and Writing and Assistance on Materials)

![[Pasted image 20240720191057.png|600]]
Totally inscrutable, but used in Sector 3  ((Weird ESL))
It seems like they're independently generating instructions (using the content to generate instructions), and then making 500k combinations of content (from [[C4]]) and instructions using this template (not a prompt).

![[Pasted image 20240720192931.png|500]]
Some statistics about the UltraChat dataset.

![[Pasted image 20240720194433.png|600]]
Some examples of opening lines from different parts of the evaluation set.

![[Pasted image 20240720194605.png|600]]

![[Pasted image 20240720194615.png|500]]

![[Pasted image 20240720194634.png|600]]
When the model is prompted to provide "helpful and detailed responses"

![[Pasted image 20240720194802.png|600]]

![[Pasted image 20240720194819.png|600]]




# Non-Paper Figures
![[Pasted image 20240420234903.png]]

