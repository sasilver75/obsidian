https://www.youtube.com/watch?v=sGuiWX07sKw&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=9

Some of you have probably been thinking, regarding [[Epsilon-Greedy]] exploration... "Surely there's something that we can do that's better than taking a random action with probability $\epsilon$"

One of the fundamental problems is an agent trying to figure out how to get the most value as possible, while also reasonably exploring the state space, while *also* operating under a fixed compute budget ðŸ˜Ÿ 

Agenda:
1. Introduction
2. [[Multi-Armed Bandit]]s (Starting with a simplified version of a RL problem; pick an action, get a reward, and that's the end of your episode. Motivating.)
3. [[Contextual Bandit]]s (Touching on how to bring back the full complexity of the full RL problem. We'll bring back states, and say not only is there an action to choose, but also some state information. Probably one of the most successful current applications of Machine Learning is using Contextual Bandits to decide how to do advertisement banner selection for online advertising.)
4. [[Markov Decision Process|MDP]]s (Coming back to full MDPs)

---
# Introduction
![[Pasted image 20240701154659.png|300]]
Every time we're decision-making online, the same choice is present: Should I take the action that I *think* is optimal (exploit), or should I try a different action (explore)?
- The best long-term strategy might involve giving up reward in the short-term, because we think we might be able to get reward back later (by learning about the environment). "What's behind that unknown door? Maybe there's a dragon, or maybe there's a pot of gold."


![[Pasted image 20240701154838.png|400]]
Examples of explore-exploit trade-off

![[Pasted image 20240701160125.png]]
We'll focus on three different families approaches (out of many) for the exploration-exploitation problem:
1. ==Random Exploration:==
	- Maybe, with some probability, we do a random action (eg epsilon-greedy, or a softmax over our value function)
2. ==Optimism in the face of uncertainty==
	- We estimate our *uncertainty* about the value of something...a nd if we're *uncertain* about something, we should probably try the action. If there's an action that will certainly give us 10, and another that will give us something between 5-20, we should learn more about the second option.
	- We of course need some way to measure our uncertainty on values (eg frequentist, bayesian)
3. ==Information state space== (perhaps the most correct/theoretically careful, but most computationally difficult)
	- Consider the agent's information itself as part of its state -- "I'm in a state where I've tried left 3 times and right 1 time; that's a state! We can ask about how good it is to move into states where the agent has accumulated information"
	- Might be in a state where I've never seen what's beyond that door over there; that's very different from being in a state where we *know* what's behind a door.
	- If we can bring that information into our state description, it *can* be helpful.

![[Pasted image 20240701160206.png]]
There are two different spaces we can explore:
- In the state-action space
	- I know that I've been in this state and taken a left before... so maybe if I come back to this state, I'll go to the right, the next time around. Using knowledge about the state-action space to explore more effectively -- there are actions we have tried and actions we haven't tried.
- In the parameter space
	- Our policies have some parameters that describe our agent's behavior in the environment.
	- We can try some parameters for a while, see how they do... and then maybe "Exploration" means trying out some different parameters!


# Multi-Armed Bandits
- [[Multi-Armed Bandit]]
![[Pasted image 20240701160631.png]]
We have a set of actions A, and a reward function R -- we've thrown away the state space, and we've thrown away the transition function, and really simplified things.
- We get to choose to pull one of these arms for these machines; the machines have different payouts, and we don't know in advance which will produce the most money.
- How do we know which one to pull next?
There's a whole strategy to your exploration/exploitation tradeoff.
Rewards for action $a$ are drawn from a distribution $\mathcal{R}^{A_t}$ 
The goal is to maximize cumulative reward!

You can think of it as a one-step MDP; there's one state and one step; You pick an action, get a reward, and that's the end of the episode. 

![[Pasted image 20240701161006.png]]
Let's talk about opportunity loss, know as *==regret==* -- rather than reward. Instead, we think "How much worse have we done than the best that we could have possibly done."
Definitions
- We define our action-value as the mean reward for an action $a$ 
	- This is the true payout of one of these machines
- The optimal value $v_*$ is the best we could possibly do if we knew which machine had the best payout.
- So the regret is how much worse we do than $v_*$
	- And the total regret is the sum of these differences.
We can maximize our cumulative reward by minimizing our total regret.
- This helps us in understanding how well an algorithm could possibly do...

![[Pasted image 20240701161312.png]]
One more slide to understand what's good and bad about regret
- The "count" is the number of times we've pulled an arm
- The "gap" is the difference in value between any action a and the optimal action (between the best machine and some specific sub-optimal machine).
So regret is the sum of gaps over count iterations... or a sum of action-counts times action-gaps, over all actions.
We want to pull machines with large gaps the smallest number of times, and machines with small gaps more often.

![[Pasted image 20240701161756.png]]
So the real question: What does this regret look like over time?
- For most naive algorithms (eg greedy, epsilon greedy): total regret increases linearly.
	- ((I don't really get why epsilon greedy would have a linear line here, because the "policy" being followed can change as the expected reward of some epsilon-explored action can possibly eclipse that of the currently-greedily-chose action))
If we pick at random, that will always incur some regret...
Can we achieve sublinear total regret?


![[Pasted image 20240701162304.png]]
Let's start with the Greedy Algorithm
- We estimate the value of each action via Monte Carlo evaluation of each arm, and then we select the action with the highest value.
Unfortunately, Greedy can lock onto a suboptimal action forever, especially if our monte-carlo sample of each arm isn't large enough. That means that at every step forever, we incur the gap associated with that action. So Greedy has a linear total regret, because with some probability we can make the same mistake forever.

![[Pasted image 20240701162546.png]]
What if we try to be clever using Optimistic Initial Values?
- A well-known algorithm, and a good idea -- quite hard to beat it
- We initialize our values to the maximum possible amount.
	- We start by assuming the best of all our actions; this is the simplest version of optimism in the face of uncertainty -- "Everything is good until proven otherwise"
So we initialize our estimates to that value, and act greedily then-onwards.  We might not want to erase our optimism immediately upon getting a bad result.. so maybe use some non-stationary/moving average of some sort.
- If you're unlucky about something a few times... we can still end up locking out some action forever, so we never explore that (actually) good action again.

So we still need some sort of continuous exploration to *guarantee* that we'll do better than linear. We want to find algorithms that make FEWER mistakes over time.


![[Pasted image 20240701163550.png]]
Epsilon Greedy, we flip a coin every timestep and pick a random arm with probability epsilon; else we behave greedily.
- We can be absolutely certain that we'll continue making mistakes over time, because we explore randomly. So we still have linear regret.

![[Pasted image 20240701163615.png]]
BUT if we just do something very simple, which is to *decay* our epsilon over time... then you can get sublinear regret.
- ((But if we decay epsilon, I think we lose the convergence guarantee in terms of finding the best action before we basically converge to greedy, in theory?))
If we knew the size of our gaps, we could invent a schedule, where, every step, we only care about the size of the smallest gap between best action and second-best action. If we knew that, we could craft a schedule (when the gaps are very small, we want to explore more often, if the gaps are less small, we want to explore less often).
- This new approach actually has LOGARITHMIC asymptotic total regret, if we decay our epsilon correctly.
	- But this schedule requires advance knowledge of gaps 

can we find an algorithm with sublinear regret for any multi-armed bandit WITHOUT knowledge of the rewards in advance? (v_star, gaps, etc)

One more slide about the theory:
![[Pasted image 20240701164116.png]]
It turns out that there's actually a lower-bound on the regret; that no algorithm can possible do better than a certain curve on that previous graph. We want to push our algorithms down closer and closer.
- This lower bound is actually logarithmic in the number of steps.
What makes a bandit "hard" or "easy?"
- Consider how similar the "best" arm is to the other arms?
- An easy problem is one where one arm is obviously good, and the other is obviously bad.
- A hard problem is two arms that look very similar, but there's a lot of noise in the distributions and it takes you a long time to figure out which is better.
	- Basically: Similar-looking arms with different means
![[Pasted image 20240701164331.png]]
Formally, the total regret is at least logarithmic in the number of steps... multiplied by this term that is basically proportionate to the *gap*, and divided by the difference between the distributions.

![[Pasted image 20240701164504.png]]
Imagine that we have three arms (blue, red, green).... 

We'll use ==Upper Confidence Bounds== (UCB)
![[Pasted image 20240701165010.png]]
We don't only estimate the *mean*, but we also estimate some upper confidence...
For each color, we estimate its q value (its mean), but also some kind of addition that characterizes how big the tail of the distribution is.
- We consider the thing with the highest q + {tail thing}
We add on less and less of this bonus, the more we use an action
Eventually we end up just using the means

So the algorithm is simple:
- Select the action that maximizes (Q plus the UCB, U)

Some methods use assumptions about distributions, and others don't.

The distribution-free version:
![[Pasted image 20240701165544.png]]
- ==NOTE:== People in the comments say he wrote this inequality a little incorrectly, oops!
- Hoeffding's Inequality; basically tells us that if we have some random variables, and we keep sampling again and again and again... 
	- What's the probability that we're wrong by more than amount of u, when we estimate the real mean, using a sample mean.
- We can bound this thing, and it's true for *any* distribution...

![[Pasted image 20240701170910.png]]
![[Pasted image 20240701170924.png]]
We pick a probability (like 5%, for picking our 95% interval), solve for our u from the huffman inequality. We solve for it... giving us that U_t(a) upper confidence value.
- We don't need to know anything about gaps or rewards, and now we have a way to pick actions -- we just add on this term here (see previous slides).
- Note that the action counter is in the denominator, so this term gets pushed down to zero as we try actions more, and actions we've tried less have higher (all else equal) bonuses.

The second thing we do is add a *schedule* to our p value, slowly increasing it over time to be more and more confident that we've included the true q value in our interval.

![[Pasted image 20240701171143.png]]
UCB1 Algorithm (there are many exntesions, hence the one)
- At every step, we estimate our Q values using Monte Carlo, and then add this bonus term on that only depends on the number of total pulls of all arms, and the number of times you've pulled *this* arm -- you pick the action with the highest total value.
![[Pasted image 20240701171250.png]]


![[Pasted image 20240701171425.png]]
Epsilon-Greedy performs well if you get the parameters right
UCB1 in contrast systematically performs well in pretty much all situations


![[Pasted image 20240701171533.png]]
People have plugged in a bunch of other inequalities and generated similar methods

---


![[Pasted image 20240701171602.png]]
Bayesian Bandits: We can exploit prior knowledge about the rewards, if we're given knowledge over the Q distribution.
- We have a distribution over the action-value function that's parametrized by some parameter vector w.
- With experience, we update these ws; These w's would be the means and variances of each of our arms.
![[Pasted image 20240701171907.png]]

![[Pasted image 20240701171922.png]]
Probability Matching
Automatically does "optimism in the face of uncertainty" - The more uncertainty there is in something, the more chance there could be for that thing to be the max.
![[Pasted image 20240701172000.png]]
EG it's fairly high that Blue will be the max!
In practice, there's a simple way to do this:

[[Thompson Sampling]]
![[Pasted image 20240701172119.png|400]]
- It's actually the oldest algorithm for bandits! It comes from the 1940s, before bandits were really even studied -- but it turns out that it's asymptotically optimal, in terms of reducing total regret.
-  The idea is to do probability matching in a sample-based way; every step, we sample our posterior (pick a sample from it)...
- ![[Pasted image 20240701172228.png]]
- We randomly sample from these distributions, for example... and maybe we think the value is now (at some point in the distribution).
- Now, all I do is just look at my samples (eg one for each color), and according to my samples, I pick the one that's best among my samples, and go with that action. Very simple!

Thompson Sampling is a very general idea. We just sample from the parameters characterizing our distribution, and then once we have our samples, we pick the action that has the best result, according to those samples.

----

Let's consider our third option, those information-state-space methods

![[Pasted image 20240701172655.png]]

Why is exploration useful?
- It actually *gains* information; if you weren't gaining information by trying actions (eg not being told how much reward we got), there'd be no reason to explore.
- If we can quantify the value of that information we got, we can trade it off perfectly! If we have two rooms, and I know what's inside one of them, if I can quantify how much long-term reward I'd gain by exploring the unknown room, how much is that worth to me in terms of units of future reward? 
How much am I prepared to pay to take some action that I *currently* believe is suboptimal (I know I can get $100 by pulling X lever, but I want to quantify how much I'll gain from pressing some other lever Y over here... )
- The value of information depends on all kinds of things, one of which is the budget.
If we know the value of information, we can trade off the cost of exploration and the possible benefits optimally.

So what's the REAL best way to trade off information and exploitation?
![[Pasted image 20240701174058.png]]
We can think of an information state space; we'll now transform our bandit problem back into an MDP. We'll think about an information state $\tilde{S}$ that summarizes all information accumulated so far (eg "I've pulled this lever 3 times, and this lever 5 times")
- Each time we take an action, we transition to a new information state $\tilde{S}'$

We can define an MDP over information states now!
- State, Action, Transition, Reward, gamma

![[Pasted image 20240701174417.png]]
![[Pasted image 20240701174454.png]]
Bayes-Adaptive RL

![[Pasted image 20240701174541.png]]

![[Pasted image 20240701174625.png]]

Summary

![[Pasted image 20240701174638.png]]


# Contextual Bandits

![[Pasted image 20240701174714.png]]
A [[Contextual Bandit]] takes our [[Multi-Armed Bandit]] formulation and adds states back into the picture. When we pull an arm, we get a payoff.
- The canonical example is banner ad placement on the internet; we want to maximize the probability that a user clicks on an advertisement.
But we're also told some information about the user (eg location, what they've clicked on in the past, some other statistics about the user.)
So this context becomes our state S

Shown a State, we need to pick an Action, and we get a Reward.
This State informs what we do

We extend our reward functino to depend on the state as well as the action
- Every step, we take an action that tries to maximize cumulative reward over many steps, as usual.

# MDPs

Very briefly

How can we extend our ideas we've seen so far that we can extend to our agent in Atari, or our robot in the real world?
In terms of trading off exploration and exploitation

![[Pasted image 20240701175054.png]]
This is to say that all ideas we've seen in previous sections ALL extend to the MDP space

This idea is not quite perfect in MDPs because iet ignores that when we're in an MDP and we're evaluating a currently policy, that policy is likely to IMPROVE, and Q values are going to get better and better; so the uncertainty U should be taking into account how certain our MonteCarlo is (Q), but also how much more our policy could improve
- Q values could be wrong because
	- Haven't evaluated current policy well
	- Because there are a lot of improvements we can still make

The information state space idea also applies to MDPs
![[Pasted image 20240701175443.png]]
We start with our original state space, and we augment it to include information state -- Our new state is the state we were in in our "real" mdp (eg where the robot is in the real world) augmented by the information gathered so far.
- "I'm in a position that's here" and "I've also visited all other positions over there"
- This blows up to a very large MDP, but if we can solve for it, we start to get the optimal tradeoff of exploration/exploitation
We solve for the augmented information state MDP using whatever method we choose (eg MCTS), and get really good results

![[Pasted image 20240701175618.png]]
We've looked through these families of solutions to exploration/exploitation
And progressively more complicated settings
- Bandits
- Multi Armed Bandits
- Contextual Bandits
- MDPs




