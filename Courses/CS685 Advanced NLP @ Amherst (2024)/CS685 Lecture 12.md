
![[Pasted image 20240524113220.png]]
What do we need RL for, given the above objective?
- The $y$ above: We're sampling from the model at multiple timesteps, the next word to generate. This sampling process is not differentiable; we're choosing one word from the distribution over the entire vocabulary over and over, until the end of sequence is finally sampled, which is then when we determine reward, etc. This is what stops us from doing our normal SFT finetune -- we're nto trying to maximize the likelihood of some gold output, we're trying to maximize the reward of some generation of our model. We don't know what part of the $y$ is good or bad; we only observe the reward at the END of all of the next-token-prediction steps ([[Credit Assignment Problem]]).

It can difficult to figure out what part of the output is good or bad.

----

But let's talk about an alternative, [[Direct Preference Optimization]] (DPO):
- No explicit reward model needed
- Not going to sample outputs y|x from the model
	- (In RL terminology, these generations are called "Rollouts")

DPO is one of an emerging class of methods called preference finetuning, a class of techniques for finetuning a model taking into account human preferences over generations.

![[Pasted image 20240524113955.png]]
They introduce a policy that is a function of the reference policy as well as the KL-modulated reward term -- this 1/B(r(x,y)).

Let's introduce a new policy $\pi^*$ that we'll later solve for as the optimal policy that minimizes the objective, and incorporates the reward term as well as $\pi_{ref}$ 

![[Pasted image 20240524114255.png]]
So what they do is take this minimization objective we previously had, and rewrite it to include this z(x) term, using some basic algebra
- Substitute z(x) into our objective. 
- After re-arranging terms, we get:

![[Pasted image 20240524114422.png]]
This is equivalent to the one above, but we've introduced z(x) into the equation.

But wait, if we actually look at the denominator term, we see that we can rewrite it as $\pi^*$ 
![[Pasted image 20240524114513.png]]
That term that's the  ratio of the two policies is (?) the KL divergence of the two distributions (our current policy pi, and our optimal policy pi-star).

![[Pasted image 20240524114745.png]]

We minimize our KL divergence at 0 when 
![[Pasted image 20240527164313.png]]
This is a closed-form solution for this RLHF objective of the KL-regularized maximization of the expected reward.

So how can we use this knowledge of our *optimal* policy to give us a method that doesn't require us sampling from the model to determine alignment?
- We see that there's an $r(x,y)$ term above; so next we want to solve/rewrite the above for our reward term, as a function of our policy and the $\pi_{ref}(y|x)$. 

We get:

![[Pasted image 20240527164521.png]]
Now we've expressed our reward in terms of the optimal policy, the reference policy, and this normalizer term. 
- (Note this Z is intractable to compute, because it requires summing over every output y that you could come up with, given an input x)

We're hoping that we can get a version of this where the Z's cancel out, and we don't have to compute it.

This happens when we substitute this reward $r(x,y)$ into the Bradley-Terrey Preference model we saw in the last class.

[[Bradley-Terry Model]]:

![[Pasted image 20240527164723.png]]
The probability of preferring the "good" output over the "bad" output given the prompt is equal to the exponentiated good reward over the sum of the exponentiated good reward and exponentiated bad reward.

Now we have a expression for our reward; so can we just substitute in the earlier right side of the equation for all the the reward terms in the most recent picture?

![[Pasted image 20240527164809.png]]

If we do this (substitute in the reward function), and we solve for all of these terms... then in fact it turns out that this normalizer *does* cancel out, and we don't have to worry about it moving forward!

We get that the preference is actually the sigmoid function:
![[Pasted image 20240527164943.png]]
We see that when we do this substitution, we no longer have to deal with the normalizer term, and everything is nicely expressed as the ratios of the probability reference and optimal policies.

We want to convert this into a loss function! We try to optimize the negative log likelihood of the probability we have here:

![[Pasted image 20240527165153.png]]
Where $\pi_{\theta}$ is the policy that we're optimizing, and $\pi_{ref}$ is the reference policy from the end of our SFT stage.
- See that this has no y's that are generated by the model in any way, so we don't have issues of having to sample from the model
- Notice it has no reference to the reward model in any way; we don't have to train a reward model, because we've solved for the optimal policy, gotten the expression for the reward model in terms of the optimal policy, and substituted it into our bradley terry model to obtain the loss.

Nice properties of DPO:
- No explicit reward model needed
- No need for rollouts from the policy

![[Pasted image 20240527165755.png]]
Above: 
RLAIF replaces the human preferences with AI-generated preferences; you can use that in this system as well.

See that this loss incorporates both y_w and y_l; they do an analysis of the gradient on this loss, and show that it is increasing the probability of the preferred completion y_w | x, and decreasing the probability of y_l | x. It's taking into account both the preferred and dispreferred completion, as opposed to SFT, which only would take into account y_w, and not y_l; so incorporating "negative feedback"

Because there's no reward model, you might think there's no chance that reward hacking will occur -- but it still takes human-preference-labeled data that can encode strange biases in behavior!

DPO limit: You're only limited to learning from the examples that are in your preference dataset! In RLHF, you can explore the space a little more freely by creating (novel, unseen) generations and determining reward for them.


[[Identity-Mapping Preference Optimization|IPO]] is a more generic form of DPO
[[Kahneman-Tversky Optimization|KTO]] only requires üëç/üëé for single outputs; don't need two outputs for any given prompt.