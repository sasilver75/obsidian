https://www.youtube.com/watch?v=ayzOzZGHZy4&list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5&index=2
---

![[Pasted image 20240625174745.png]]
Neural Networks can learn *total randomness* -- if you give it really bad data, it will produce exactly what it learns, even if that's really wrong.
- So the article on the left should really be: "When algorithms are trained with erroneous (or not enough) data"

Traditional ML is very model-centric.
- When we learn ML in school, a dataset is given to us, usually fairly clean and well-curated. The goal is to produce a best model for this dataset.












