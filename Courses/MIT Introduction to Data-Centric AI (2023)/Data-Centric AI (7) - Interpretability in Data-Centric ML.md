https://www.youtube.com/watch?v=w0Nn-SVYCL0&list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5&index=7

We're going to talk about building models with interpretable features

----

If you start reading into the data-centric literature, you'll notice references to the concept of *interpretable features*; many algorithms claim to generate interpretable features, and users often clamor for the idea of interpretable features.

But what does this actually mean -- why do we care?

Agenda:
- Introduction to Interpretable ML
- WHY do we care about interpretable features?
- WHAT are interpretable features, really?
- HOW do we get interpretable features?

----
# Introduction to Interpretable ML
- In Machine Learning, we take data/features, input them into an ML model that does some math, and we get an output.
- Question: In the black box in the middle, what's happening?



# WHY do we care about interpretable features?



# WHAT are interpretable features, really?





# HOW do we get interpretable features?


