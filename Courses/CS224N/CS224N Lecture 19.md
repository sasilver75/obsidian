#lecture 
Link: https://youtu.be/cd3pRpEtjLs?si=25cedZolfvxGJFUY

Topic: Model Interpretability and Editing, Been Kim

----

LLMs are exciting and maybe a little bit frightening -- we want these technologies to benefit *us*!t

There ares some things that:
1. We know, machines don't know
2. We both know
3. Machines know, we don't know (eg [[AlphaGo]] move 37)

How can we make things from category 3 interpretable to us lowly humans?

Saliency Maps
- Showing, for each pixel in an image, some idea of how "important" that pixel is in the eventual (eg) classification
- It turns out that we really can't conclude much about model behavior from things like this ðŸ˜¢
	- She shows off some complicated methods that end up performing worse than random guessing

Simple tools are good! Let's tryout some simple tools...


((I'm really not super interested in this interpretability-esque lecture))

Something about "Causal Tracing"

(Skipped to end)









