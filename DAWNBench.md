2017 -- Stanford DAWN Lab
Paper: [https://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf]()
Authors include Peter Bailis
#benchmark 

An early benchmark/competition seemingly focused on time-to-achieve-a-SoTA accuracy level.
They closed submissions on April 20, 2018.
[Results here](https://dawn.cs.stanford.edu/benchmark/)

Abstract
> Despite considerable research on systems, algorithms and hardware to speed up deep learning workloads, there is no standard means of evaluating end-to-end deep learning performance. Existing benchmarks measure proxy metrics, such as time to process one minibatch of data, that do not indicate whether the system as a whole will produce a high-quality result. In this work, we introduce ==DAWNBench==, a benchmark and competition ==focused on end-to-end training time to achieve a state-of-the-art accuracy level==, ==as well as inference time with that accuracy==. Using time to accuracy as a target metric, we explore how different optimizations, including choice of optimizer, stochastic depth, and multi-GPU training, affect end-to-end training performance. Our results demonstrate that optimizations can interact in non-trivial ways when used in conjunction, producing lower speed-ups and less accurate models. We believe DAWNBench will provide a useful, reproducible means of evaluating the many trade-offs in deep learning systems.
