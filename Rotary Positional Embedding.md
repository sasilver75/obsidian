---
aliases:
  - RoPE
---
References
- [Rotary Positional Embeddings: Combining Absolute and Relative](https://youtu.be/o29P0Kpobz0?si=GwvIa0fn3rPkZOul)


Used by [[PaLM]] instead of the learned embeddings of [[GPT-3]].

A technique that allows you to significantly expand the context lengths of a model, relative to the usual sinusoidal embeddings of the original Transformer paper.