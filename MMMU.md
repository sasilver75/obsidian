November 27, 2023
Paper: [MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://arxiv.org/abs/2311.16502)

A benchmark to evaluate multimodal understanding of models. 11.5K meticulously-collected multi-modal questions across 6 core disciplines (Art and Design, Business Science, Health and Medicine, Humanities and Social Science, Tech and Engineering).

Includes heterogenous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.

GPT-4V and Gemeni Ultra only achieve accuracies of 56% and 59%, respectively, indicating significant room for improvement.

Abstract
> We introduce MMMU: a new ==benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning==. MMMU includes ==11.5K meticulously collected multimodal questions== from college exams, quizzes, and textbooks, covering six core disciplines: ==Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering==. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.
