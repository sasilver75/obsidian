Prework: 
- Link to Reasoning Self-Correction Document: https://docs.google.com/document/d/1-U91ekhAwZA4XPI7OXZiMIGmJX7XKpeG4mg731L0s0Q/edit#heading=h.clak3mkd5bz0 

----

Likely that Friday is going to be the best for me

To-Do:
- [ ] Discussion
- [ ] Determine topic areas (maths reasoning, instruction following, QA, alignment, etc.)
- [ ] Determine source reasoning benchmarks
- [ ] Generate Taxonomy on Error Types
- [ ] Create few-shot exemplars of corruption for each error type
- [ ] Apply 
- [ ] Inspect/Filter

On-Policy vs Off-Policy
Do we create a HuggingFace space for it, similar to https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard?
- (Unsure if this is too brash for a first pass): Is there a way to make a task for this for the Eleuther test harness so that people can benchmark their own models?
	- How does this dovetail with on-policy errors (since those are different for each model)


Lit Review?
- ...

Because the field moves so fast, there's a weird dual effect
- Bunch of papers are out there, things move fast
- Net effect is that we end up with 3-5 papers 
- Lit review goes on

"Hey isn't this like *that* paper?"
Twitter
Google Scholar
Papers with Code

Familiarize yourself with these tools
Do a lit review of the "obvious papers" (make a thread in slack for relevant papers)
- As you find them, post them 
- Find 3-5 nearest neighbor papers

1:30 EST

Jimin is an MLE on Eddie's team and is the other person I'll have as a resources.
- Working on Synth data, previously worked on Embeddings with Nils
- Masters from CMU in CS
- Been on vacation the last month in Korea.

The google doc is from a long time ago
I remember on the big thoughts

ICML <- Seems right
ICLR <- Too early

Formatting:
- StepbyStep Math dataset?



Section 1: Contrived/off-policy examples.