

It seems for the 250 (500?) examples that I did, we didn't have any successful solutions.
- Could my perturbations be too difficult? IE the weak-completion model was just *too dumb?*
- I noted that it was pretty rare for the strong-completion model to be able to arrive at the correct answer anyways. Would we assume that models have a higher propensity to recover when tackling questions that it can easily handle? Should we consider an easier dataset, like [[GSM8K]]?
- Do I just need to scale my N higher?
- Let's look at some examples in my little UI. It's easy to spin it up on your computer.

Price is pretty expensive for what I'm doing...
- Iterations of weak solution and verification.
- Straight-shot completion and verification.
- Strong completion of weak prefix.
Burned like $20 yesterday, it seems like. Should I worry about that?

For other inference providers, it seems like Together has a chat completions feature similar to the one that Anthropic offered. 
- Asking in their discord if it's doing under the hood what I'd hope for it to do -- or I can just test it out.
- I don't mind covering some small charges, but ... funding for that?