

 Doing some out-loud "literature review" in this thread around LMs/Reasoning/(Self)-Correction (+PRM/Critic) -- LMK if you have missing favorites!
Edit: Let me know if there's a different way that I should be organizing or notating around this :thinking_face:. Unsure at what level of granularity/depth/analysis to be doing the initial collection, these are 75% papers that I've already read that seem relevant to cite (if only in the intro/setup sections of our paper), and others mostly found through those papers' citations + GS (edited) 

# Literature Review

NOTE: I just have the years on here, and they aren't necessarily ordered correctly intra-year. Usually used the v1 arxiv date, but perhaps I wasn't 100% consistent even on that (eg sometimes the journal date). It's mostly for vibes :slightly_smiling_face:
Seminal-ish general papers that are often cited in introductions, etc.
Vaswani et al. (2017) Attention is All You Need
Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Raffel et al. (2019) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
Brown et al. (2020) Language Models are Few-Shot Learners
GPT-3 showing the ability to do few-shot learning
Rae et al. (2021) Scaling Language Models: Methods, Analysis, and insights from training Gopher
Just another language model, unsure if we want this (Deepmind)
Bommasani et al. (2021) On the opportunities and risks of foundation models
Idea being that if language models are going to be used for important things, failures/corrections in reasoning is an important concept to understand.
Ouyang et al. (2022) Training Language Models to follow Instructions with Human Feedback
OpenAI (2022) ChatGPT: Optimizing Language Models for Dialogue
Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways
Just another language model, unsure if we want this (Google)
Zhang et al. (2022) OPT: Open Pre-trained Transformer Language Models
Just another language model, unsure if we want this (Meta)
Chung et al. (2022) Scaling Instruction-Finetuned Language Models
Exploration of effect of instruction-finetuning on language models
Could optionally also include: Longpre et al. (2023) The Flan Collection: Designing Data and Methods for Effective Instruction Tuning
BigScience Workshop (2022) BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
Just another language model, unsure if we want this (HuggingFace)
On the subject of ("Other GPT-3/3.5 fast-follow language models that can be optionally cited," I left out Megatron Turing NLG and Eleuther's GPT-J for no specific reason. There's a seemingly endless number of such models :slightly_smiling_face:) (edited) 

Reasoning-ish papers 
 (And I don't think we need to cover every X-of-Thought strategy under the sun, here). I think I might be a little light on especially Math-related reasoning papers. It's not something that I usually read much into out of vague high school math anxiety :smile:. LMK
Rajani et al. (2019) Explain yourself! Leveraging Language Models for commonsense reasoning
This seems to be very similar to the later CoT paper, authors include Richard Socher
Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
Wang et al. (2022) Self-Consistency improves Chain of Thought reasoning in Language Models
Other inference-time prompting strategies that we probably won't look closely at:
Zhou et al. (2022) Least-to-Most Prompting
"CoT performs poorly on tasks requiring solving problems harder than exemplars shown in prompts; we use a technique that breaks the problem down into simpler subproblems, then solves them in sequence"
Chen et al. (2022) Program of Thoughts Prompting
Interesting in that uses language models to express the reasoning process as a program, and then separately executes that code, which seems like it could help with ameliorate certain types of errors. Can be combined with self-consistency.
Zelikman et al. (2022) STaR: Self-Taught Reasoner Bootstrapping Reasoning with Reasoning
Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models
This one is a more "agentic" reasoning strategy, interacting with an external knowledge source over multiple terms.
Mukherjee (2023) Orca: Progressive Learning from Complex Explanation Traces of GPT-4
		- Similar to STaR in that both involve generating (intrinsically, extrinsically) explanation traces and finetuning on the reasoning traces.
Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models
CoT can still fall short in in tasks that require strategic look-ahead... ToT generalizes over popular CoT approach to prompting LMs... IIRC it's basically beam search + CoT.
Besta et al. (2023) Graph of Thoughts: Solving Elaborate Problems with Large Language Models
Models information generated by an LLM as an arbitrary graph, where LM "thoughts" are vertices, and edges correspond to dependencies between these vertices... I don't know if this is really a legit strategy, I've definitely heard less about it than CoT/ToT, and this is a buzzy space.
Shinn et al. (2023) Reflexion: LanguageAgent with Verbal Reinforcement Learning
A more "agentic" reasoning strategy I think, sort of in a similar place to ReAct.
I think it's maybe kind of BS (relies on stronger evaluator and self-reflection models, iirc, and multiple rounds of iteration/guessing), Subbarao had bad things to say about both this and ReAct.
Zelikman et al. (2024) Quiet-STaR: Language Models can Teach Themselves to Think Before Speaking
Math/Science:
Lekwoycyz et al. (2022) Minerva: Solving Quantitative Reasoning Problems with Language Models (Google Research)
Azerbayev et al. (2023) Llemma: An Open Language Model for Mathematics (Eleuther folks)
Trinh et al. (2024) Solving olympiad geometry without human demonstrations (AlphaGeometry)
Code:
Chen et al. (2021) Evaluating Large Language Models trained on Code (Codex)
Li et al. (2022) Competition-Level Code Generation with AlphaCode
Leblond et al. (2023) AlphaCode 2 Technical Report


(Self)-Critique/Verification Papers
 I sort of mentally group these along two axes of [self...external critique and intrinsic...exogenous information (eg tool use, rubric)], maybe I'm missing some other dimensions :thinking_face:. Ours is even a little different than the usual generation/critique/revise flow, because we're "hoping" to observe it all "in-flight", within a single generation.
Bai et al., (2022) Constitutional AI: Harmlessness from AI Feedback
Self-critique (IIRC?) but using some exogenous information in the form of a set of principles
Manakul et al.  (2023) SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative LLMs
Instead of an approach that needs access to output probability distributions or external databases, this is a simple sampling-based approach, leveraging the idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts, but for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another.
Miao et al. (2023) SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning
Exploring whether LMs can recognize errors in their own CoT without resorting to external resources.
Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback ([[Self-Refine]])
Uses a single LLM as the generator, refiner, and feedback provider
Gou et al. (2023) CRITIC: Large Language Models can Self-Correct with Tool-Interactive Critiquing
Framework to validate and progressively amend their own outputs in a manner similar to human interaction with tools... Given initial output, interact with appropriate tools, then revise output based on feedback obtained during this validation process. Tested on QA/Math/Code/Toxicity Reduction
Huang et al. (2023) Large Language Models Cannot Self-Correct Reasoning Yet
A short survey over self-correction methods (IIRC self-prompting, external oracle, multi-agent debate, ?) and shows that they don't meaningfully improve performance beyond self-consistency, IIRC, on GSM8K/CommonSenseQA/HotpotQA
Other PRM/Critic Model papers that usually rely on a strong external oracle LM in some way
PRM Papers (Granular reward models for reasoning trajectories)
Solving math word problems with process- and outcome-based feedback (DeepMind)
Let's Verify Step by Step (OpenAI)
Let's Reward Step by Step (NUS)
Critic Papers (Similar to PRM, seems related to reasoning in a sense? Eh)
Shepherd
Prometheus
Prometheus 2



-----

# Dataset Snooping




