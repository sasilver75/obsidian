

https://x.com/thesephist/status/1734966611814289756?s=20

- Automatic Commit Message Generation
- Use the streaming LLMs with attention sinks paper to summarize some sort of live data feed, like financial or news information.
- As per [this](https://youtu.be/TIqf4LMNCjU?si=lTLl_4ft-TmCpMNr) video, distillation is usually used to create a smaller model with a similar performance. Can you use distillation (by providing higher quality labels; either distributions or rationales) to train successively better versions of the same size model? What about slightly increasing capacity models? Do you need more capacity to benefit from the slightly better labels?


