- (01/05/2023): So for multi-headed attention, the idea is that each attention head attends to some different aspect of the input, right? What makes that the case? Is it ever the case that multiple attention heads basically attend to the same thing?









