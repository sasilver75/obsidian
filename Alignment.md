Wikipedia: [AI Alignment](https://en.wikipedia.org/wiki/AI_alignment)

Alignment refers to efforts to steer AI systems toward intended goals, preferences, and ethical principles.
An AI system is considered *aligned* if it advances its intended objectives -- *misaligned* systems may pursue some objectives, but not the intended ones.



Relevant Techniques:
- [[Reinforcement Learning from Human Feedback]]
- [[Reinforcement Learning from from AI Feedback]]
- [[Direct Preference Optimization]] (and variants)


Problems:
- Specification gaming and side-effects
- Pressure to deploy unsafe systems (race conditions)
- Risks from advanced misaligned AI
- Power-seeking
- Existential-risk
- Scalable oversight
- Inner/Outer Alignment
- Emergent goals
- Principal-Agent problems