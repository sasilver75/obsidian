---
aliases:
  - PRM
---
Whereas [[Reward Model|Outcome Reward Model]]s (ORMs) only provide feedback on the final generation of a language model, whereas Process Reward Models (PRMs) provide more precise feedback along every reasoning step in a generation (eg one using [[Chain of Thought|CoT]]), which is more interpretable for humans and rewards human-endorsed CoTs (whereas ORMs can sometimes result in correct answers, even with incorrect reasoning paths, which isn't aligned behavior).

Related Papers: 
- "[[Solving math word problems with process- and outcome-based feedback]]" (Uesato et al. 2022, DeepMind)
- "[[Let's Verify Step by Step]]" (Lightman et al. 2022, OpenAI)
- (Possibly?) "[[WizardMath]]: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct" (Luo et al., 2023)
- "Let's reward step by step: Step-level reward model as the Navigators for Reasoning" (Ma et al., 2023, NUS)

