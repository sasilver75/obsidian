April 24, 2023
[[Microsoft Research]]
Paper: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)
#zotero 
Takeaway: A [[LLaMA]] 7B model fine-tuned using synthetic instruction-tuning data generated from the [[Evol-Instruct]] technique, which was introduced in the same paper. We use an evolutionary algorithm to generate diverse and complex instruction data for LLM by doing multiple rounds of evolution of an initial instruction dataset, where we choose from one of 6 possible evolutions of an instruction (5 increasing the depth, and 1 increasing the breadth), which are performed via specialized prompts and a language model. Compared to the other LLaMA 7B fine-tunes of Alpaca and Vicuna, WizardLM is a meaningfully better model.

----
Note also that Microsoft has released datasets used in training WizardLM:
- WizardLM_evol_instruct_70k
- WizardLM_evol_instruct_V2_196k

Notes:
- Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs.
	- Starting from a simple instruction "1+1=?", the method randomly selects either ==In-depth Evolving== or ==In-breadth Evolving== to upgrade a simple instruction to a more complex one, or to a new one (to increase diversity)?
	- ==In-depth Evolving==: Includes five types of operations:
		- Add constraints
		- Deepening
		- Concretizing
		- Increasing reasoning steps
		- Complicate input
	- ==In-breadth Evolving==: Mutation, i.e. generating a *completely new instruction*  based on the given instruction
	- All six of the above operations are implemented by prompting an LLM with specific prompts. 
	- Since the evolved instructions are generated from LLMs, sometimes evolving will fail! So we adopt an *instruction eliminator* to filter the failed instructions, which is called ==Elimination Evolving==.
- Authors fine-tune LLaMA, and compare their dataset to the one used by [[Alpaca]] (generated with [[Self-Instruct]]) and the 70k ShareGPT (shared by real users) used by [[Vicuna]].
	- This is nice, because one of these is a synthetic dataset, and the other a crowd-sourced (and volunteer-biased) dataset.
- Alpaca's training data (generated from only 175 human-created seed instructions) is the initial starting point; They execute four epochs of evolution using OpenAI's ChatGPT API, to finally obtain 250k instructions (from 52k starting). 
- From the 250k, for fairness's sake, they sample 70k examples (same as the number of examples used for Vicuna) and fine-tune a LLaMA 7B model, with the resulting model being called [[WizardLM]].
- We start our evolution from an initial instruction dataset D of N (Instruction, Response) pairs (I, R). 
	- In each evolution, we upgrade *all* of the I_t in D_t to I_t+1, by applying an LLM instruction evolution prompt.
	- We then use the LLM to generate corresponding responses R_t+1 for the newly evolved I_t+1, thus diving us D_t+1.
	- After this, we run through our Elimination Evolving checks (more below)
- A gradual difficulty increase is necessary to avoid filling the instruction set with extremely complex instructions, which would harm the generalization performance of trained models. To control difficulty increase, we make each evolution "a bit harder," and restrict adding a maximum of 10 to 20 words.
- All five of the In-Depth evolving prompts (add constraints, deepening, concretizing, increased reasoning steps, and complicating input) can be implemented without any in-context examples... ((But then for the complicating input task, it seems like they use in-context demonstrations?))
- The In-Breadth Evolving prompt aims to enhance topic coverage, skill coverage, and overall dataset density.
- The authors use the same LLM to generate responses for involved instructions as they did to generate them, with the simple prompt `"<Here is instruction.>"` ((Meaning... they just plop the instruction in there, it seems? This seems like it could be improved, perhaps with DSPy or a similar tool?))
- Regarding ==Elimination Evolving==, we classify instruction evolution failures into four different situations:
	1. ==The evolved instruction doesn't provide any *information gain* compared to the original one==. *We use ChatGPT to make this determination.*
	2. ==The evolved instruction makes it difficult for the LLM to generate a response==. For example, when the generated response contains "sorry" and is of relatively short length, this indicates that the LLM struggles to respond to the evolved instruction, so we can use this rule to make a judgement.
	3. ==The response generated by the LLM only contains punctuation and stop words.==
	4. ==The evolved instruction *obviously* copies some words from the evolving prompt, such as "given prompt," "rewritten prompt", "\#Rewritten Prompt\#"==, etc.
- Once all evolutions are done, we merge the initial instruction dataset with evolved instruction data from *all epochs*, and randomly shuffle the samples to create the final fine-tuning dataset.
- Note: For each round of evolution, we randomly select one evolving prompt from our six total prompts (5 from in-depth, and 1 from in-breadth) with equal probability.


Abstract
> ==Training== large language models (LLMs) with ==open-domain instruction following data brings colossal success==. ==However, manually creating such instruction data is very time-consuming and labor-intensive==. Moreover, humans may struggle to produce high-complexity instructions. In this paper, ==we show an avenue for creating== large amounts of ==instruction data== with varying levels of complexity ==using LLM instead of humans==. ==Starting with an initial set of instructions, we use our proposed [[Evol-Instruct]] to rewrite them step by step into more complex instructions.== Then, we mix all generated instruction data to ==fine-tune LLaMA. We call the resulting model WizardLM==. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that ==instructions from Evol-Instruct are superior to human-created ones==. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public atÂ [this https URL](https://github.com/nlpxucan/WizardLM)

# Paper Figures
![[Pasted image 20240502180347.png]]
Above: The blue lines are the In-Depth Evolving, while the red lines are In-Breadth Evolving. After generation, we use Elimination Evolving to filter "failed generations."

![[Pasted image 20240503151803.png]]
Above: From an initial instruction, there are primarily two components: ==Instruction Evolvers== (In-Depth, In-Breadth) and ==Instruction Eliminator==.

![[Pasted image 20240503152925.png]]
The ==add constraints== prompt (a form of ==In-Depth Evolving==). It seems that maybe the prompt writer isn't an English-native speaker, which is interesting -- it's cool that it still seems to have worked.

![[Pasted image 20240503153523.png]]
The ==In-Breadth Evolving== prompt, which aims to enhance topic coverage, skill coverage, and overall dataset diversity.

![[Pasted image 20240504003210.png]]
Showing that Evol-Instruct is basically better than Alpaca and Vicuna, all having been fine-tunes of LLaMA 7B.

![[Pasted image 20240504003511.png]]

![[Pasted image 20240504003519.png]]

![[Pasted image 20240504003527.png]]

The following screenshots are the "Complicate Input Prompt," which is one of the more difficult ones, and uses a few-shot approach to prompting:
![[Pasted image 20240504003626.png]]
![[Pasted image 20240504003637.png]]
![[Pasted image 20240504003647.png]]
![[Pasted image 20240504003656.png]]
(End of Complicating Input Prompt)

The paper index contains many comparisons between generations of Alpaca/Vicuna/WizardLM


# Non-Paper Figures
- 