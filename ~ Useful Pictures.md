![[Pasted image 20231215171803.png]]
![[Pasted image 20231223182856.png]]
![[Pasted image 20231224000029.png]]![[Pasted image 20231224001343.png]]

![[Pasted image 20231224001556.png]]![[Pasted image 20231224003503.png]]
![[Pasted image 20231224003841.png]]
![[Pasted image 20231224131650.png]]
![[Pasted image 20231224132027.png]]
![[Pasted image 20231224132852.png]]
![[Pasted image 20231224135822.png]]
![[Pasted image 20231224135808.png]]![[Pasted image 20231224135911.png]]
![[Pasted image 20231224140449.png]]


![[Pasted image 20231224155343.png]]![[Pasted image 20231224160119.png]

![[Pasted image 20231224160249.png]]

![[Pasted image 20231224160322.png]]
(Rotations work very well and produce strong features, despite being simple!)

![[Pasted image 20231224160618.png]]![[Pasted image 20231224171000.png]]
![[Pasted image 20231224171628.png]]

![[Pasted image 20240122211056.png]]
![[Pasted image 20240124184241.png]]

![[Pasted image 20240124184228.png]]

Byte Magazine Covers - https://lunduke.substack.com/p/the-truly-epic-byte-magazine-covers

![[Pasted image 20240127145004.png]]
(From Youtube: https://youtu.be/c_jjyIJ4ci8?si=KVWMwUiQnPPxx868)

![[Pasted image 20240129161437.png]]
(From Youtube: https://youtu.be/X-82uXrw76s?si=QRWRNj2E8scJYfP4)

![[Pasted image 20240129230748.png]]
(From Yannic Kilchner Discord; An example of bots: https://discord.com/channels/714501525455634453/834836108000886805/1200085021705388062)
![[Pasted image 20240129230809.png]]
(From Yannic Kilchner Discord; An example of bots: https://discord.com/channels/714501525455634453/834836108000886805/1200085021705388062)


![[Pasted image 20240131205312.png]]
From: https://www.youtube.com/watch?v=KCXDr-UOb9A

![[Pasted image 20240201111336.png]]
From: https://youtu.be/BN6tTdWcIcQ?si=9S3FG70jYseBcPqj

![[Pasted image 20240131144136.png]]
From SnorkelAI Andrew Ng

![[Pasted image 20240205172048.png]]![[Pasted image 20240208123653.png]]
![[Pasted image 20240210020210.png]]
From: https://news.ycombinator.com/item?id=33831759 

![[Pasted image 20240211134130.png]]
From: https://www.youtube.com/watch?v=6lVBp2XjWsg

![[Pasted image 20240214111538.png]]

![[Pasted image 20240214123110.png]]
Re: Length bias in LLM-as a judge in AlpacaEval (Feb 14, 23)

![[Pasted image 20240214124305.png]]
From Latent Space Paper Club chat (LeveragingLarge Language Models for NLG Evaluation: A Survey) "I don't know how they got to that number, it's just what they provided to me."
![[Pasted image 20240218172236.png]]
![[Pasted image 20240219131023.png]]

![[Pasted image 20240220132410.png]]
As an example of how easy it is to get around guardrails (usecase: copywrite)

![[Pasted image 20240221205508.png]]
From XKCD; Re: evaluation of language model next-token-prediction

![[Pasted image 20240222114053.png]]
Above: The "pro-diversity" guardrails seemed to work for the {Profession} case, but not for the {Profession1 and Profession2} case. This is from DALLE-3, but the same thing was observed in Gemeni in Feb 2024, when there was a hullabaloo about Gemeni creating black Nazi soldiers, etc.

![[Pasted image 20240222121537.png]]
Yud on Safety, Feb 22, after the Google Gemeni kerfuffle with the black Nazis, etc.

![[Pasted image 20240222134504.png]]
From Emmett Shear on Twitter

![[Pasted image 20240222145858.png]]
Torment Nexus meme

![[Pasted image 20240222152053.png]]

![[Pasted image 20240222155809.png]]
The Chinese Room Experiment

![[Pasted image 20240228101951.png]]
Feb 2024 OpenAI protest

![[Pasted image 20240228143011.png]]

![[Pasted image 20240309191916.png]]
![[Pasted image 20240311192314.png]]

![[Pasted image 20240311192323.png]]

![[Pasted image 20240311192422.png]]

![[Pasted image 20240311193133.png]]
![[Pasted image 20240327010754.png|300]]
![[Pasted image 20240327012519.png|200]]

![[Pasted image 20240330002147.png|400]]

![[Pasted image 20240330005607.png|400]]

![[Pasted image 20240330010736.png|400]]![[Pasted image 20240331232918.png]]
Above: context being explaining word2vec; "You shall know a word by the company it keeps" is a good reference

![[Pasted image 20240401142942.png]]
Different uses of the word "pike": re: how embeddings should be contextual

![[Pasted image 20240403175421.png]]
![[Pasted image 20240403175714.png]]
(From: https://youtu.be/5q0GN2M1d2c?si=uAqMOuUqf7PsytY7)
![[Pasted image 20240403200819.png|450]]
![[Pasted image 20240403212659.png]]
![[Pasted image 20240403221104.png]]
![[Pasted image 20240403225624.png]]
Above: Showing that the simple addition of CoT pushed model performance above the human average.


![[Pasted image 20240404161038.png]]
![[Pasted image 20240404161005.png]]
Questions re: RAG

![[Pasted image 20240404201323.png]]

![[Pasted image 20240405013204.png]]
![[Pasted image 20240405021834.png]]
![[Pasted image 20240408171501.png]]
Above: Great explanation of the vanilla LSTM from CS224N Lecture 6

![[Pasted image 20240408171933.png]]


You'll probably notice that while we've introduced this "long term" memory in the form of the hidden state (unfortunately named), we still have the problem along long sequences of boiling the whole world into the hidden state, so we don't get to incredibly long sequence lengths -- just longer than a vanilla RNN!

Re: the name: The idea is that there's the concept of "Short Term Memory" from psychology; it was suggested that the hidden state in an RNN is a model of human memory short term memory, and there was something else that would deal with long term memory.
Schidhuber et al were interested in how we could construct models with *long* short term memory (so there's not long term memory, just a *long* version of short-term memory!)

![[Pasted image 20240408172357.png]]

![[Pasted image 20240408172530.png]]

---

[[Residual Network]]s
- Note that the vanishing/exploding gradient problem rears its head in almost all deep neural networks (feedforward, convolutional neural networks) -- any time you have long sequences of chain rules, the gradient can become vanishingly small or explodingly large. Generally, lower/earlier layer are harder to train, as a result.
- There has been effort to come up with different architectures that learn more efficiently in deep NNs!
- The most common way is to ==add more direct connections that allow gradients to flow==
![[Pasted image 20240408172917.png]]
You have two paths that are summed together:
1. An identity path
2. Some that goes through some neural network layers
So the default behavior is to just preserve the input, which might seem a little like what we just saw for LSTMs.

There are other methods:
- DenseNets, where you add skip connections of each layer to all future layers
- HighwayNets from Schmidhuber, where, rather than just having an identity connection, it introduces an extra gate that looks more like an LSTMM, which says how much to send the input through the highway versus how much to put it through a neural net layer; these two are then combined into the output.


![[Pasted image 20240408173053.png]]
Above:
- This problem first arose with recurrent NNs; they're particularly unstable because you have this one weight matrix being repeatedly used throughout the time sequence.


-----

[[Bi-Directional Recurrent Neural Network]] and multi-layer RNN motivation

Say we wanted to do sentiment classification using an RNN
- WE had the thing where we said that we'd run an RNN over it, and take some combination of the hidden states across timesteps as the representation of the sentence, and then feed it into a softmax classifier to classify for sentiment.

![[Pasted image 20240408174209.png]]

Above:
- So the hidden state at each word can be thought of (sort of) as the contextual representation of that word in the sentence (kinda).
- But we calculate this hidden state left to right.
- So we'd look at the one for "terribly" and the RNN will think, ah, this is a bad word! Add more "bad" to the hidden state!
- But we know that terribly in this context actually means "very!", once you see the next word "exciting"!

![[Pasted image 20240408174441.png]]
This motivates the introduction of a bidirectional RNN!
- An easy way to deal with this would be to just have another RNN that runs "backwards" through the sentence! 
- This second RNN has its own completely separate, learned parameters.
- We could run it backwards through the sentence
- We could get an "overall" representation of a word in a sentence by simply ==concatenating the representations from the forward and backward RNNs!==

![[Pasted image 20240408174641.png]]




