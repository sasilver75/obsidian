![[Pasted image 20231215171803.png]]
![[Pasted image 20231223182856.png]]
![[Pasted image 20231224000029.png]]![[Pasted image 20231224001343.png]]

![[Pasted image 20231224001556.png]]![[Pasted image 20231224003503.png]]
![[Pasted image 20231224003841.png]]
![[Pasted image 20231224131650.png]]
![[Pasted image 20231224132027.png]]
![[Pasted image 20231224132852.png]]
![[Pasted image 20231224135822.png]]
![[Pasted image 20231224135808.png]]![[Pasted image 20231224135911.png]]
![[Pasted image 20231224140449.png]]


![[Pasted image 20231224155343.png]]![[Pasted image 20231224160119.png]

![[Pasted image 20231224160249.png]]

![[Pasted image 20231224160322.png]]
(Rotations work very well and produce strong features, despite being simple!)

![[Pasted image 20231224160618.png]]![[Pasted image 20231224171000.png]]
![[Pasted image 20231224171628.png]]

![[Pasted image 20240122211056.png]]
![[Pasted image 20240124184241.png]]

![[Pasted image 20240124184228.png]]

Byte Magazine Covers - https://lunduke.substack.com/p/the-truly-epic-byte-magazine-covers

![[Pasted image 20240127145004.png]]
(From Youtube: https://youtu.be/c_jjyIJ4ci8?si=KVWMwUiQnPPxx868)

![[Pasted image 20240129161437.png]]
(From Youtube: https://youtu.be/X-82uXrw76s?si=QRWRNj2E8scJYfP4)

![[Pasted image 20240129230748.png]]
(From Yannic Kilchner Discord; An example of bots: https://discord.com/channels/714501525455634453/834836108000886805/1200085021705388062)
![[Pasted image 20240129230809.png]]
(From Yannic Kilchner Discord; An example of bots: https://discord.com/channels/714501525455634453/834836108000886805/1200085021705388062)


![[Pasted image 20240131205312.png]]
From: https://www.youtube.com/watch?v=KCXDr-UOb9A

![[Pasted image 20240201111336.png]]
From: https://youtu.be/BN6tTdWcIcQ?si=9S3FG70jYseBcPqj

![[Pasted image 20240131144136.png]]
From SnorkelAI Andrew Ng

![[Pasted image 20240205172048.png]]![[Pasted image 20240208123653.png]]
![[Pasted image 20240210020210.png]]
From: https://news.ycombinator.com/item?id=33831759 

![[Pasted image 20240211134130.png]]
From: https://www.youtube.com/watch?v=6lVBp2XjWsg

![[Pasted image 20240214111538.png]]

![[Pasted image 20240214123110.png]]
Re: Length bias in LLM-as a judge in AlpacaEval (Feb 14, 23)

![[Pasted image 20240214124305.png]]
From Latent Space Paper Club chat (LeveragingLarge Language Models for NLG Evaluation: A Survey) "I don't know how they got to that number, it's just what they provided to me."
![[Pasted image 20240218172236.png]]
![[Pasted image 20240219131023.png]]

![[Pasted image 20240220132410.png]]
As an example of how easy it is to get around guardrails (usecase: copywrite)

![[Pasted image 20240221205508.png]]
From XKCD; Re: evaluation of language model next-token-prediction

![[Pasted image 20240222114053.png]]
Above: The "pro-diversity" guardrails seemed to work for the {Profession} case, but not for the {Profession1 and Profession2} case. This is from DALLE-3, but the same thing was observed in Gemeni in Feb 2024, when there was a hullabaloo about Gemeni creating black Nazi soldiers, etc.

![[Pasted image 20240222121537.png]]
Yud on Safety, Feb 22, after the Google Gemeni kerfuffle with the black Nazis, etc.

![[Pasted image 20240222134504.png]]
From Emmett Shear on Twitter

![[Pasted image 20240222145858.png]]
Torment Nexus meme

![[Pasted image 20240222152053.png]]

![[Pasted image 20240222155809.png]]
The Chinese Room Experiment

![[Pasted image 20240228101951.png]]
Feb 2024 OpenAI protest

![[Pasted image 20240228143011.png]]

![[Pasted image 20240309191916.png]]
![[Pasted image 20240311192314.png]]

![[Pasted image 20240311192323.png]]

![[Pasted image 20240311192422.png]]

![[Pasted image 20240311193133.png]]
![[Pasted image 20240327010754.png|300]]
![[Pasted image 20240327012519.png|200]]

![[Pasted image 20240330002147.png|400]]

![[Pasted image 20240330005607.png|400]]

![[Pasted image 20240330010736.png|400]]![[Pasted image 20240331232918.png]]
Above: context being explaining word2vec; "You shall know a word by the company it keeps" is a good reference

![[Pasted image 20240401142942.png]]
Different uses of the word "pike": re: how embeddings should be contextual

![[Pasted image 20240403175421.png]]
![[Pasted image 20240403175714.png]]
(From: https://youtu.be/5q0GN2M1d2c?si=uAqMOuUqf7PsytY7)
![[Pasted image 20240403200819.png|450]]
![[Pasted image 20240403212659.png]]
![[Pasted image 20240403221104.png]]
![[Pasted image 20240403225624.png]]
Above: Showing that the simple addition of CoT pushed model performance above the human average.


![[Pasted image 20240404161038.png]]
![[Pasted image 20240404161005.png]]
Questions re: RAG

![[Pasted image 20240404201323.png]]

![[Pasted image 20240405013204.png]]
![[Pasted image 20240405021834.png]]




