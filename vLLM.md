(A popular implementation of [[PagedAttention]]?)

> Swyx: "No one 'Professional' uses vLLM, slow as fuck, dies under any concurrency. All the PyTorch pros look at it and lol and rewrite" (But it's the best open-source backend, right?) "Yeah, but that's not saying much -- if you're only serving your own needs, sure. If you need to serve a few hundred other people, trtllm or write your own."