July 16, 2024
HuggingFace
[SmolLM - Blazingly Fast and Remarkably Powerful](https://huggingface.co/blog/smollm)

Takeaway: SmolLM is a family of state-of-the-art small models of size (==135M, 360M, 1.7B parameters==) trained on a meticulously curated high-quality training corpus called ==SmolLM-Corpus==, which includes data from [[Cosmopedia v2]] (synthetic textbooks and stories; 28B tokens), Python Edu (Educational Python samples from The Stack; 4B tokens), and [[FineWeb-Edu]] (deduplicated; 220B tokens).
- 135M and 360M models are each trained on 600B tokens from the SmoLLM-Corpus
	- Adopts a design similar to [[MobileLLM]], incorporating [[Grouped Query Attention|GQA]] and prioritizing depth over width. 
- 1.7B model is trained on 1T tokens from the SmoLLM-Corpus
	- Uses a more traditional architecture.
Both models use embedding tying and a context length of 2048 tokens, and a tokenizer trained on the SmoLLM-Corpus with a vocabulary size of 49152. All models are trained for one epoch on the permissive subset of the WebInstructSub dataset, combined with StarCoder2-Self-OSS-Instruct, followed by [[Direct Preference Optimization|DPO]] for one epoch, using [[HelpSteer]] for the 135M and 1.7B models, and argilla/dpo-mix-7k for the 360M model.
Regarding ==Cosmopedia v2==, they use seed documents from [[FineWeb]] to generate 39 million synthetic documents consisting of 28B tokens of textbooks, stories, articles, and code, with a diverse range of audiences and over 34,000 topics.

----

\
There's an increasing interest in small language models that can operate on local devices. This trend involves techniques like [[Distillation]] and [[Quantization]] to compress large models, as well as train small models from scratch on large datasets. Both of these approaches enable novel applications, while dramatically reducing inference costs and improving user privacy.
- Microsoft's Phi series (eg [[Phi-3]])
- Alibaba's [[Qwen 2]] family
- Meta's [[MobileLLM]] 
All of the above demonstrate that small models can achieve impressive results when designed and trained thoughtfully.

SmolLM is a series of SoTA small langauge models available in three sizes (135M, 360M, 1.7B parameters). These models are built on a meticulously curated high-quality training corpus called ==SmolLM-Corpus==, comprised of:
- [[Cosmopedia v2]] (28B tokens)
- Python-Edu (4B tokens)
- [[FineWeb-Edu]] (220B tokens)

![[Pasted image 20240716234758.png|600]]
Evaluations showing superior performance against similarly-sized models.
((It's interesting that Karpathy's GPT2-1.5B has such a high MMLU score. Note that the largest SmolLM model is compared only to [[Phi-1.5]] because the [[Phi-2]] and [[Phi-3]] families don't have a model smaller than 2.7B/3.8B respectively))

## Data Curation

- [[Cosmopedia v2]]
	- An enhanced version of [[Cosmopedia]], which was the largest synthetic synthetic dataset of over 30M textbooks, blog posts, and stories generated by [[Mixtral 8x7B]]-Instruct-v0.1. In Cosmopedia, we prompt the model to generate content on specific topics using a web page referred to as a "seed sample." We use web samples to increase diversity and expand the range of prompts.
	- To improve the dataset in version 2, we *tried* two strategies:
		- Using ==more capable models== with the same prompts.
			- Authors tried [[LLaMA 3]]-70B-Instruct, [[Mixtral 8x22B]]-Instruct-v0.1, [[Qwen 1.5]]-72B-Chat, but found no significant improvements in results.
			- ![[Pasted image 20240716235934.png|600]]
			- Above: (Example of a Cosmopedia prompt)

		- ==Optimizing the prompts themselves==.
			- Each prompt has three main components (topic, seed sample, generation style \[specifies the intended audience and type of content we want the model to generate\]).
			- In Cosmopedia v1, we ran clustering on FineWeb samples to identify topics and the corresponding web samples. There are two main limitations with this:
				- The topic list reflects FineWeb clusters, which may limit our control over the topics.
				- The web samples within each cluster are not further filtered, potentially including some low-quality samples.
			- Instead of this unsupervised clustering approach, in v2 we start with a predefined list of 34,000 topics using the ==BISAC book classification==, a standard used to categorize books by subject that is both comprehensive and educationally focused. We start with 5,000 topics belonging to 51 categories, and then ask Mixtral to generate subtopics for *certain* topics.
			- ![[Pasted image 20240717000601.png|500]]
			- Above: Distribution of subtopics among topics. See that there are many more subtopics than topics!
			- After we define our 51 topics, and related subtopics, we still need to find webpages related to them. We implemented a search tool to return the most relevant pages for each topic, and ran this tool using BISAC categories and subtopics as queries on the [[FineWeb]] CC-MAIN-2024-10 and CC-MAIN-2023-50 dumps, which together consist of over 520 million samples. For each query, we retrieved 1,000 pages, ensuring we retrieved only the most relevant content.
			- As a result, they compiled 34 million web pages across 34,000 topics!
			- The next step was to determine which generation style worked best.
		- Generation Style
			- We did some ablations, and found that the textbooks based on topics and seed samples from curated sources like Stanford and OpenStax (subsets of Cosmopedia v1) provided the best overall performance.
			- Stories seemed to help with common-sense benchmarks.
			- After implementing new topics and seed sample retrieval methods in v2, we were able to match performance of curated sources using web seeds, confirming the quality of new prompts.
			- We explored which audience style worked best; we generated textbooks targeting two difference audiences: middle school and college students. ==We found that the models trained on textbooks aimed primarily at middle school students gave the best score on all benchmarks except MMLU==. This can be explained by the fact that most of these test basic common sense and elementary to intermediate science knowledge, while MMLU contains some questions requiring more advanced knowledge and expertise.
			- Thus ==for v2, we decided to generate 40% of the content for middle school students, 30% for college students, and 30% as a mix of other audiences and styles including in subsets we borrow from Cosmopedia v1 such as stories and Stanford courses based textbooks. Additionally, we generated 1B code textbooks based on Python seed samples from AutoMathText dataset==.
			- Ultimately, they produce ==39 million synthetic documents consisting of 28B otkens of textbooks, stories, articlse, and code== for a diverse range of audiences and over 34,000 topics.


## FineWeb-Edu
- [[FineWeb-Edu]] was released a few months ago, consisting of 1.3T tokens of educational web pages filtered from the [[FineWeb]] dataset.
- We develop an ==education quality classifier== using annotations generated by LLaMA3-70B-Instruct; we then use this classifier to retain only the most educational web pages from FineWeb, resulting in a dataset that outperforms FineWeb itself on popular benchmarks, and shows the power of classifiers trained on synthetic data.
- SmolLLM-Corpus includes 220B deduplicated tokens from FineWeb.

## Stack-Edu-Python
- Stack-Edu-Python applied the same idea of FineWeb-Edu to Code! They used LLaMA3 to annotate 500k python samples from The Stack and used them to train an educational classifier using the same recipe as the FineWeb-Edu classifier.
- They then applied this classifier to the entirety of the Python subset of the [[StarCoder]]  model's training corpus, resulting in a refining of 40B tokens to 4B tokens.
- Models trained on Python-Edu converge more than 3x faster than the model trained on unfiltered Python code.

---

![[Pasted image 20240717004445.png]]
![[Pasted image 20240717004516.png]]
![[Pasted image 20240717004523.png]]













