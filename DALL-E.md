---
aliases: []
---
February 24, 2021
Paper: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092)

Improved upon by [[DALL-E 2]]

Abstract
> Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. ==We describe a simple approach for this ((text-to-image generation)) task based on a transformer that autoregressively models the text and image tokens as a single stream of data==. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.

