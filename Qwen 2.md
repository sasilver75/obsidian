October 31, 2023 -- [[Alibaba Research]]
Blog: [Alibaba Cloud Launches Tongyi Qianwen 2.0](https://www.alibabacloud.com/en/press-room/alibaba-cloud-launches-tongyi-qianwen-2?_p_lc=1) ("Tongyi Qianwen")
HuggingFace: [Qwen2](https://huggingface.co/docs/transformers/en/model_doc/qwen2)
See also: [[Qwen]]

A second incarnation of the popular Qwen series of models from Alibaba, this time coming in (???) sizes.
Incorporates [[SwiGLU]], [[Grouped Query Attention]], [[Sliding Window Attention]], and more.

Later, a Qwen2MoE was released (14.3B parameters in total and 2.7B activated): https://huggingface.co/docs/transformers/main/en/model_doc/qwen2_moe

Summary
> Qwen2 is the new model series of large language models from the Qwen team. Previously, we released the Qwen series, including Qwen-72B, Qwen-1.8B, Qwen-VL, Qwen-Audio, etc.
> Qwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the ==Transformer== architecture with ==SwiGLU activation==, attention QKV bias, ==group query attention==, ==mixture of sliding window attention and full attention==, etc. Additionally, we have an ==improved tokenizer== adaptive to multiple natural languages and codes.

![[Pasted image 20240419150245.png]]