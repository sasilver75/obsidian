January 25, 2024 -- [[DeepSeek]]
Paper: [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)

Open-weight code-generating models in sizes ranging from 1.3B-33B, trained on 2T tokens.
Trained on high-quality project-level code corpus, with 16K context window; beats [[Codex]].

Abstract
> The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a ==range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens==. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only ==achieves state-of-the-art performance among open-source code models== across multiple benchmarks but also ==surpasses existing closed-source models like Codex and GPT-3.5==. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.