---
aliases:
  - Kullback-Leibler Divergence
---
References:
- https://youtu.be/SxGYPqCgJWM?si=CKmBi34_mv0oayTZ

Captures distances between probability distributions.
Minimizing the Cross Entropy Loss is equivalent to minimizing the KL loss. Since the CE Loss has a simpler form, it's become (one of) the standard loss functions.