---
aliases:
  - Kullback-Leibler Divergence
---
References:
- https://youtu.be/SxGYPqCgJWM?si=CKmBi34_mv0oayTZ
- https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence

Captures distances between probability distributions.
Minimizing the Cross Entropy Loss is equivalent to minimizing the KL loss. Since the CE Loss has a simpler form, it's become (one of) the standard loss functions.





![[Pasted image 20240411185747.png]]
((I'm not sure if this is *really* a good example, because KL divergence is asymmetric, which isn't what I'd intuitively do.))
Other examples for intuition: https://news.ycombinator.com/item?id=37214898
