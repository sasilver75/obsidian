[[Self-Attention]] vs [[Cross-Attention]]
[[Multi-Headed Attention]] vs [[Multi-Query Attention]]
[[Masked Attention]] vs [[Bidirectional Attention]]
[[Sparse Attention]] vs [[Dense Attention]]
[[Parallel Attention]]
[[PagedAttention]]
[[FlashAttention]]
[[Attention with Linear Biases|Attention with Linear Biases]] (ALiBi)
[[Sliding Window Attention]]
[[PagedAttention]]

Note that attention as introduced in *Attention is All You Need* is referred to as "Scaled Dot Product Attention (SDPA)"