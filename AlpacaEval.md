Mid-2023 (?) -- Tatsu Lab
(No paper for AlpacaEval, just a GitHub Repo): [alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)
But inspired by AlpacaFarm paper here: [AlpacaFarm](https://arxiv.org/abs/2305.14387)

An automated evaluation framework measuring how often a strong LLM prefers the output of one model over a reference model. Metrics include win rate, bias, latency, price, variance, etc. Validated to have high human agreement with 20k human annotations. A model must generate a response to 805 questions on different topics, mostly focused on helpfulness, and then scored by GPT-4... but the final metric is the pairwise win-rate against a baseline model ([[text-davinci-003]]).

Useful as a quick and cheap proxy for human evaluation of a simple instruction-following tasks. Useful for running many evaluations quickly, eg during model development.

Github Overview
> Evaluation of instruction-following models (e.g., ChatGPT) typically requires human interactions. This is time-consuming, expensive, and hard to replicate. ==AlpacaEval in an LLM-based automatic evaluation that is fast, cheap, replicable, and validated against 20K human annotations==. It is particularly useful for model development. Although we improved over prior automatic evaluation pipelines, there are still fundamental [limitations](https://github.com/tatsu-lab/alpaca_eval#limitations) like the preference for longer outputs. AlpacaEval provides the following:
> - [**Leaderboard**](https://tatsu-lab.github.io/alpaca_eval/): a leaderboard of common models on the AlpacaEval evaluation set. **Caution**: Automatic evaluators (e.g. GPT-4) may be biased towards models that generate longer outputs and/or that were fine-tuned on the model underlying the evaluator (e.g. GPT-4).
> - [**Automatic evaluator**](https://github.com/tatsu-lab/alpaca_eval#evaluators): an automatic evaluator that has high agreement with humans (validated on 20K annotations). We evaluate a model by measuring the fraction of times a powerful LLM (e.g. GPT-4) prefers the outputs from that model over outputs from a reference model. Our evaluators enable caching and output randomization by default.
> - [**Toolkit for building automatic evaluators**](https://github.com/tatsu-lab/alpaca_eval#analysis): a simple interface for building advanced automatic evaluators (e.g. with caching, batching, or multi-annotators) and analyzing them (quality, price, speed, statistical power, bias, variance etc).
> - [**Human evaluation data**](https://github.com/tatsu-lab/alpaca_eval#data-release): 20K human preferences between a given and reference model on the [AlpacaFarm](https://github.com/tatsu-lab/alpaca_farm/tree/main) evaluation set. 2.5K of these are cross-annotations (4 humans annotating the same 650 examples).
> - [**AlpacaEval dataset**](https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_eval.json): a simplification of [AlpacaFarm's](https://github.com/tatsu-lab/alpaca_farm/tree/main) evaluation set, where "instructions" and "inputs" are merged into one field, and reference outputs are longer. [Details here](https://github.com/tatsu-lab/alpaca_eval#data-release).
> 
> **==When to use AlpacaEval==?** Our automatic evaluator is a ==quick and cheap proxy for human evaluation of simple instruction-following tasks==. It is useful if you have to run many evaluations quickly, e.g., ==during model development==.
> 
> ==**When not to use AlpacaEval==?** As any other automatic evaluator, AlpacaEval ==should **not replace human evaluation in high-stake decision-making**==, e.g., to decide on model release. In particular, AlpacaEval is limited by the fact that (1) the instructions in the eval set might not be representative of advanced usage of LLMs; (2) automatic evaluators may have biases such as favoring style over factuality of the answer; and (3) AlpacaEval does not measure the risks that a model could cause. Details in [limitations](https://github.com/tatsu-lab/alpaca_eval#limitations).
> 


Followed up by a April 6, 2024 paper: [Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators](https://arxiv.org/abs/2404.04475)
It seems this paper is about trying to reduce biases of the [[LLM-as-a-Judge]] as used

Quote
> LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce complex biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for chat LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: "What would the preference be if the model's and baseline's output had the same length?". To achieve this, we first fit a generalized linear model to predict the biased output of interest (auto-annotator preferences) based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS' Chatbot Arena from 0.94 to 0.98. We release the code and leaderboard at [this https URL](https://tatsu-lab.github.io/alpaca_eval/) .
