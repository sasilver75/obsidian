March 13, 2024
Paper: [Gemma: Open Models Based on Gemeni Research and Technology](https://arxiv.org/abs/2403.08295)

Related: [[Gemeni]] (Some say that it's some sort of a training checkpoint?)

A small family of a 2B and 7B [[Decoder-Only Architecture]] language models. 8,000 token context length, coming [[Instruction-Tuning|Instruction-Tuned]] out of the box.

Abstract
> This work introduces Gemma, a family of ==lightweight==, state-of-the art ==open models== ==built from the research and technology used to create Gemini models==. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (==2 billion and 7 billion parameters==), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.

