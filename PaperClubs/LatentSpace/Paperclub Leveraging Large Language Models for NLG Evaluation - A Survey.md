Link: https://arxiv.org/abs/2401.07103
Published: Jan 2024
Covered: Feb 14, 2024

Authors: [Zhen Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Z), [Xiaohan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+X), [Tao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen,+T), [Can Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+C), [Jia-Chen Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu,+J), [Chongyang Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao,+C)

-------
# Pre-reading
- Abstract
	- LLMs are a new way to generate and evaluate generated content for 
		- coherence
		- creativity
		- context relevance
	- We'll provide a survey of how LLMs are being leveraged for NLG evaluation, offering a way to organize existing LLM-based evaluation metrics, and suggesting a way to compare these metrics.
	- We examine unresolved challenges (bias, robustness, domain-specificity, etc.)
- Introduction
	- ==Traditional NLG evaluation metrics== like BLEU (2002), ROUGE (2004), and TER (2006) primarily focus on surface-level text differences and often ==fall short of assessing *semantic* aspects==.
	- ==Other methods use neural embeddings== to calculate the score (2016, 2020, 2020)... but they're ==inflexible== and limited in scope, and often have ==low alignment with human judgement== (and poor interpretability)
	- We need a more nuanced way of evaluating our LLMs!
	- Emergent abilities of LLMs present a promising avenue for LLM-based NLG evaluation, like
		- [[Chain of Thought]](CoT, 2022)
		- Zero-shot instruction following (2022)
		- Better alignment with human preferences (2022)
	- For example, ==LLMs could generate reasonable explanations to support the ultimate score==, and ==RLHF could align LLM's preferences with humans better==. The key strategy involves instructing LLMs with prompts to evaluate generated texts from various aspects, either with references and sources, or not.
	- There's so much work going on here, we need a summary! That's what this is -- the first comprehensive survey of recent advancements in leveraging LLMs for NLG evaluation.




-------
# Discussion
![[Pasted image 20240214120923.png]]
![[Pasted image 20240214120945.png]]
Simply reordering the choices, the model performance shifta  lot!

![[Pasted image 20240214120955.png]]
Let's focus on this; this can be quite helpful in improving responses

![[Pasted image 20240214121007.png]]
General framework for evaluations: The valuation isa function of the text generated by the LLM, the source of text, and ground truth references (what you expect from the LLM). (The second two are optional depending on your use case)

1. Prompt b ased evaluation
![[Pasted image 20240214121210.png]]
- The probabilty based one is interesting: Prompt the LLM to guess how easy it was to come up with the respone to a given query? If it's easy to come up with the response, then it should be less likely to hallucinate. 

You can also do many types of ensembling to come up with a score
![[Pasted image 20240214121303.png]]
Paper used GPT, Gemeni, Anthropic to have a discussion and come up with a score![[Pasted image 20240214121315.png]]
Seemed like it... more closely aligned with human evaluations, compared to using individual (but only a marginal improvement, so...)

2. Fine-tuning based evaluation: Can we get more out of less? Finetune a much smaller model to align with GPT4
![[Pasted image 20240214121353.png]]
Tries to be a GPT-4 copycat judge for evals

![[Pasted image 20240214121412.png]]
It asks: What are the different types of evaluation scenarios?
Once you have them, sample soem tasks from each scenario, and ask GPT4 to answer /evaluate those tasks... and use them to evaluatethe Auto-J model. It seems like the evaluation from Auto-J seemed ![[Pasted image 20240214121445.png]]to correlate highly with GPT-4, compared to other models.

Pause

But the bias of the LLMs!

![[Pasted image 20240214122511.png]]
- LLMs tend to favor specific positiosn when it comes to multiple choices; this is called ==*order bias*== (this is from the AutoJ paper). To fix the positional bias, they duplicated and reversed the data... which seemed to work?

![[Pasted image 20240214122613.png]]
Egocentric Bias, Attentional Bias, Length of Response Bias

![[Pasted image 20240214122657.png]]
![[Pasted image 20240214122705.png]]
Speaker was working on a (bassim) rag chatbot that answers US immigration questions. We index on the US dept page that's all in english, but people who are seeking immigration help don't always speak in english.
In the eval dataset, he wanted to test on more than english; he created an eval function that tests whether the langauge used by the user matched the output of the LLM; he wanted vietnamese questions to have a vietnamese response.
- Given the response, a human would say yes -- this matches the question.
- The LLM said NO, this is a score of 0. He noticed this because there's a single word (Visa O-1) in the response; he had to change his prompt to make sure that "it's okay if a certain word is in english".









-------
# Questions
- 





























