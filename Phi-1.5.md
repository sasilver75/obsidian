September 11, 2023
Paper: [Textbooks Are All You Need II: Phi-1.5 Technical Report](https://arxiv.org/abs/2309.05463)

While Phi-1 was a code-producing model trained on "textbook-quality" datasets, Phi-1.5 (which was the same 1.3B size) took a stab at natural language generation.

> We continue the investigation into the power of smaller Transformer-based language models as initiated by [[TinyStories]] -- a 10 million parameter model that can produce coherent English -- and the follow-up work on [[Phi-1]], a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate "textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the "Textbooks Are All You Need" approach, ==focusing this time on common sense reasoning in natural language== ((instead of code)), and create a n==ew 1.3 billion parameter model named Phi-1.5==, with ==performance on natural language tasks comparable to models 5x larger==, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, Phi-1.5 ==exhibits many of the traits of much larger LLMs, both good -- such as the ability to "think step by step" or perform some rudimentary in-context learning== -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source Phi-1.5 to promote further research on these urgent topics.