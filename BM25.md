# BM 25
- [[BM25]] stands for "==Best Match, Attempt #25==", a classical IR approach whose name suggests a lot of exploration of the hyperparameters of the model that work best
	- It's an enduringly good solution taht's sort of an enhanced version of TF-IDF

![[Pasted image 20240426135115.png|400]]
- We begin with smooth IDF values (with a little bit of adjustment to handle the undefined case that we might worry about)
- We then have a Scoring component, which is sort of analogous to Term Freqeuncy; we also have two hyperparameters k and b
- The BM25 weight is then a combination of the adjusted, smooth IDF values and the Scoring values (analagous to term frequency).

Let's look at the individual components, starting with the Smoothed IDF

#### BM25: Smoothed IDF term (+s hyperparameter)
![[Pasted image 20240426135151.png|300]]
- This very closely matches the standard IDF values

![[Pasted image 20240426135210.png|300]]
- As we vary the $s$ hyperpameter, we see that we very closely mirror the results of the usual IDF from TF-IDF, with small differences.

#### BM25: Scoring Function component (+b, k hyperparameters)

![[Pasted image 20240426135301.png|400]]
The scoring function is more nuanced, as result of having more hyperparameters
- Term Frequency on the X axis, BM25 score on the Y axis
- If the document has average document length of 10, then as our example document becomes long relative ot its average doc length of 5 or 3, we can see that the score goes down; when our document is large relative to the average, score is dimished.
	- Intuition: Long documents, as a result of being long, contain more terms; we should "trust" the terms they *do* contain less, as a consequence

![[Pasted image 20240426135434.png|400]]
- The $b$ hyperparameter controls the amount of the document length penalty described above. Higher values of b mean more of a penalty given to long documents.
- Again, we have a target document of length 10, and an average document of length 5; As we increase the $b$ value, we see a lower BM25 score as a result of a greater penalty.
- Right: If your document has length 10, and the average length in the corpus is also 10, then the b hyperparameter doesn't make any difference, because the parenthetical penalty term just becomes 1.


![[Pasted image 20240426140547.png]]
- The $k$ hyperparamter has the effect of flattening out higher frequencies.
	- Think about the extreme situation in which k is very very low. In this situation, you're essentially turning the scoring function into an indicator function (see right side, red line). "You appeared, I don't care how many times you appear."
	- As you make k larger, you get less and less of a dramatic effect, and you care more and more about how many times the document appears. 
	- Something like 1.2 is a more realistic value; as you get *very frequently* occurring terms, we begin to sort of taper off our weighting of it.
