March 20, 2024 -- [[HuggingFace]]
Blog Post: [Cosmopedia: How to create Large-Scale Synthetic Data for Pretraining](https://huggingface.co/blog/cosmopedia)
r/LocalLLaMA reaction, which you should read: [Thread](https://www.reddit.com/r/LocalLLaMA/comments/1avqw1a/huggingface_releases_cosmopedia_the_largest/)
- It seems like perhaps some of these people have a severe reaction.

==Cosmopedia== is a dataset of synthetic textbooks, blog posts, stories, posts, and WikiHow articles generated by [[Mistral]] 8x7B-Instruct-v.0.1 in an attempt to replicate the Phi-1.5 dataset. It contains ==over 30 million files and 25B tokens==, making it the ==largest open synthetic dataset to date!==
- They note that this is just the initial version of Cosmopedia, and that they're interested in doing more experimenting with a more powerul model.

# Blog Post excerpts
> This post outlines the challenges/solutions involved in ==generating a synthetic dataset with billions of tokens== to replicate [[Phi-1.5]], leading to the creation of [[Cosmopedia]].

> However, this is not another blog post on generating synthetic instruction-tuning datasets, a subject the community is already extensively exploring. We focus on scaling from a **few thousand** to **==millions** of samples that can be used for **pre-training LLMs from scratch**==. This presents a unique set of challenges.

Microsoft's Phi models were trained on synthetic data and surpassed larger models that were trained much longer on web datasets. ==The Phi technical reports for these models ***leave out substantial details*** regarding curation of their synthetic training datasets==, which aren't themselves released!
- Some praise the model capabilities
- Critics say that they're simply overfitting benchmarks!
Others say: "Garbage in, Garbage out!"
Yet it's worth exploring, which is why we choose to develop Cosmopedia, aiming to reproduce Phi-1.5's training data.

We release the code, dataset, and a 1B model trained on it called Cosmo-1B.

==Cosmopedia== is a dataset of synthetic textbooks, blog posts, stories, posts, and WikiHow articles generated by [[Mistral]] 8x7B-Instruct-v.0.1. It contains ==over 30 million files and 25B tokens==, making it the ==largest open synthetic dataset to date!==

> Most of the time for Cosmopedia was spent on meticulous prompt engineering!

They ==created over 30 million prompts for Cosmopedia, spanning hundreds of topics and achieving less than 1% duplicate content.==

In the Ph-1.5 technical report, authors curated *20,000 topics*, producing 20 billion tokens of synthetic textbooks, using samples from web datasets for diversity -- but the methodology remains unclear.

We combine two approaches to build Cosmopedia's prompts:
1. Conditioning on curated sources
2. Conditioning on web data

We refer to the source of the data we condition on as "seed data"

![[Pasted image 20240420005006.png]]

For our curated sources, we use reputable educational sources, covering many valuable topics for the LLM to learn. We extracted (for instance) various outlines for Stanford courses and constructed prompts that request the model to generate textbooks for individual units within those courses.

![[Pasted image 20240420005242.png]]
==This is very interesting; these are the textbook-generating prompts?==
- ((Would be interesting to compare this with the techniques used in [[Genstruct]], or with the [[Evol-Instruct]] or yet-to-be-documented [[Evol-Amplify]] technique.))

We ==target four different audiences== (young children, high school students, college students, researchers) and ==leverage three generation styles== (textbooks, blog posts, wikiHow articles) to get up to 12 times the number of prompts.

Using web data to construct prompts proved to be the most scalable, contributing to over 80% of the prompts used in Cosmopedia. ==This is our strategy for generating prompts.==
- We clustered millions of websamples and identified the topic of each cluster
- We removed low-educational-value clusters (eg celebrity gossip), leaving 112 topics
- We then built prompts by instructing the model to generate a textbook related to a web sample within the scope of the topic it belongs to, based on the clustering.
- We condition the prompts on the topic only 50% of the time, and change the audience and generation styles, as is explained in the previous section.
- We ultimately build ==23 million prompts== using this approach.

![[Pasted image 20240420010821.png]]

From the initial models trained using the generated textbooks, we observed a lack of common sense and fundamental knowledge typical of grade school education -- to address this, we incorporated *more stories* with day-to-day knowledge and basic common sense, using texts from [[UltraChat]] and Open[[Hermes]]2.5 instruction-tuning datasets as seed data for the prompts.
- From UltraChat, we use the "Questions about the world" subset
- From OpenHermes2.5, we omitted sources/categories unsuitable for story-telling (eg camelai, for advanced chemistry)

![[Pasted image 20240420010336.png]]

![[Pasted image 20240420011015.png]]
Benchmark decontamination information above is interesting as well

## Conclusion
- This is just the initial version of Cosmopedia, and we are actively working on enhancing the quality of the generated content.
	- The accuracy and reliability of the generations largely depends on the model used in the generation
	- Mixtral may sometimes hallucinate and produce incorrect information, for example when it comes to historical facts or mathematical reasoning within the AutoMathText and KhanAcademy subsets (they didn't compare Mixtral to other models, in this regard).
		- One strategy to mitigate the issue of hallucinations is the use of retrieval augmented generation ([[Retrieval-Augmented Generation (Model)|RAG]]). This involves retrieving information related to the seed sample, for example from Wikipedia, and incorporating it into the context