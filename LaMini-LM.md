April 27, 2023 (6 months after ChatGPT, 1 month after [[Alpaca]])
Mohammad bin Zayed University of AI (MBZUAI) (in the UAE)
Paper: [LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions](https://arxiv.org/abs/2304.14402) 🐂🐄🐎
#zotero 
Takeaway: The paper explores the power of distilling capable frontier models by using them to respond to large of instruction-tuning data (2.58M instructions, also released with the paper), and then using those responses to fine-tune a variety of smaller models of varying architectures, which all end up punching above their weight. The seed data comes from a variety of popular instruction datasets, and they also generate additional synthetic data using both **Example-Guided Instruction Generation** and **Topic-Guided Instruction Generation**.

Relevance: I'm trying to figure this out still, because this came out a month after Alpaca, which similarly distill-fine-tuned LLaMA using GPT-3.5. Did we need to do this with N different model architectures to show that it worked for all of them? The models *are* competitive with Alpaca, with many being smaller in size, too, which is impressive. I think the instruction generation process (example-guided, but really more interesting is the topic-guided process.)

----

Notes:
- Authors create an instruction-tuning dataset of ==2.58M examples==, first curating it from instructions from diverse existing datasets, including [[Self-Instruct]], P3, [[FLAN v2]], and [[Alpaca]].
	- Authors augment the dataset using :
		- ==Example-Guided Instruction Generation==: Using `gpt-3.5-turbo` to generate additional diverse instructions that match human-written ones in style and quality.
		- ==Topic-Guided Instruction Generation==: Technique to enhance instruction *diversity* by incorporating specific topics of interest from Wikipedia.
	- Finally, for all instructions, we use `gpt-3.5-turbo` to generate responses for each instruction, resulting in the ==LaMini instruction dataset==.
- Afterwards, we fine=tune multiple smaller language models with sizes ranging from 61M to 7B, across a both encoder-decoder and decoder-only, conducting extensive experiments and analyses, setting our work apart from previous analyses.
- The authors use =="sequence-level distillation"==, where we run inference on a dataset using our teacher model, and then the student trains on the output text in a supervised manner. Contrast this with the more involved type of distillation where you train the student to have the same output (or intermediate) representation (logits) as the teacher. This latter type is more effective, but it requires that you run both models in parallel. The former, sequence-level distillation, is great, especially in this case, because we just need to run the teacher model *once*, and then we can train all of our herd of LaMini models.
- Instruction Generation
	1. ==Example-Guided Strategy==
		- Inspired by [[Self-Instruct]] and [[Alpaca]]m we develop a prompt for generating instructions. It involves creating a prompt (fig. 6) with a 3 random examples and a limited number of constraints, and presenting it to `gpt-3.5-turbo`, asking it to generate additional examples.
		- To optimize the generation process, they sample three seed tasks from `self-instruct` and generate 20 instruction at once. For `P3` and `FLAN`, we randomly select three examples from the same subset -- Examples from these tend to be longer compared to those from `self-instruct`, so we only generate 10 instructions per time (because of context length limits).
	2. ==Topic-Guided Strategy==
		- It's of concern that `gpt-3.5-turbo` might now have the desired ability to generate ***diverse*** text without explicit guidance. To address this, we employ a strategy of collecting common topics from Wikipedia to provide guidance during the generation process. 
			- We initially gather a total of 2.2M categories from Wikipedia! Then we filter them based on two criteria:
				1. We select categories consisting of fewer than three words
				2. We choose categories that have more than 10 sub-categories and 50 pages associate with them. We noticed that lengthy category titles are more likely to be related to specific topics, and that responses generated by `gpt-3.5-turbo` for such instructions might contain factual errors/misinformation.
		- After filtering, we have ==3.5K categories== that serve as a list of common topics.
		- In this study, we exclusively generate topic-guided instructions using the seed tasks from self-instruct dataset. We generate approximately 280K instruction-response pairs within X_{t,SI}.
- Response Generation
	- Once we've acquired a large number of instructions, we perform sequence-level distillation by first generating responses for *all* of the collected+generated instructions using `gpt-3.5-turbo`.
- Results
	- We're surprised that some of our LaMini LMs even surpass LLaMA-7B and Alpaca-7B's performance. This highlights the critical significance of the instruction dataset.
	- (There are numerous pages on results and analysis. I'm less interested in this than in the data generation techniques.)

Abstract
> Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs into much smaller ones. To this end, ==we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions==. In addition to being sizable, we design our instructions to cover a broad set of topics to ensure diversity. Extensive analysis of our instruction dataset confirms its diversity, and ==we generate responses for these instructions using gpt-3.5-turbo==. Leveraging these instructions, ==we fine-tune a diverse herd of models, collectively referred to as LaMini-LM, which includes models from both the encoder-decoder and decoder-only families, with varying sizes==. We evaluate the performance of our models using automatic metrics on 15 different natural language processing (NLP) benchmarks, as well as through human assessment. The results demonstrate that our proposed ==LaMini-LM models are comparable to competitive baselines, while being much smaller in size==.

# Paper Figures
![[Pasted image 20240508223359.png|250]]

![[Pasted image 20240509001349.png|250]]
Above; An overview of the composition of the dataset. The "hatted" D's are datasets that are model generated. Subscript is the source. SI=Self Instruct, P3=P3, FLAN=FLAN v2, A=Alpaca.

![[Pasted image 20240509002718.png]]
Above: The prompt used for **Example-Guided Instruction Generation** (3-shot)

![[Pasted image 20240509002737.png]]
Above: The prompt used for **Topic-Guided Instruction Generation** (3 shot, with multiple topics provided).

![[Pasted image 20240509002855.png]]
Above: A rubric they used for human evaluation, which is interesting.

![[Pasted image 20240509002922.png]]
Above: This might be interesting to use for vibe-checks 🤔.

# Non-Paper Figures
