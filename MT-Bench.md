June 9, 2023 -- UC Berkeley, UCSD, Carnegie Melon, Stanford
Paper: [Judging LLM-as-aJudge with MT-Bench and Chatbot Arena](https://arxiv.org/pdf/2306.05685.pdf)
Leaderboard: [Link](https://chat.lmsys.org/?leaderboard)

This paper actually introduces two banger evaluations: [[MT-Bench]] and [[ChatBotArena]]!

---

MT Bench is an automated benchmark consisting of a series of ==open-ended questions that evaluate a chatbot's multi-turn conversational and instruction-following ability==. Consists of ==80 high-quality multi-turn questions== across 8 domains. Initial evaluations relied on human ratings only, but it was costly and laborious, so they end up using [[LLM-as-a-Judge]] (to score both, though I believe at this point they've turned the latter into a human preferences only crowd-sourced evaluation).

The authors advocate for this mixture of hybrid evaluation frameworks for future LLM benchmarks, combining existing capability-based benchmarks with new preference-based benchmarks with [[LLM-as-a-Judge]]. 
- The note that there are certain biases, including ==Positional bias== of LLMs in a judge capacity, ==Verbosity bias== (favoring longer, verbose responses), ==self-enhancement bias== (favors the answers generated by itself), as well as limited capability in either doing or grading Math and Reasoning questions.
- They use position-swapping, few-shot judging, and [[Chain of Thought]] on their judges.

Note: Generally, there is a risk of other models *overfitting* to the biases of the evaluating model -- often models will have higher scores on GPT4-rated evaluators but worse performance when tested by humans, because models will be trained such that they mimic GPT4's style, but not other aspects, like its factuality.

They're motivated by the ==three main types of existing benchmarks==:
- ==Core-knowledge benchmarks== ([[MMLU|MMLU]], [[HellaSWAG]], [[Abstraction and Reasoning Corpus|ARC]], [[Winogrande]], [[HumanEval]], [[GSM8K]], [[AGIEval]]), typically requiring LLMs to generate a short, specific answer to benchmark questions that can be automatically evaluated.
- ==Instruction-following benchmarks== (Flan, [[Self-Instruct]], [[Super-NaturalInstructions]]): Expand to slightly more open-ended questions and more diverse tasks, and are used to evaluate LLMs after [[Instruction-Tuning]].
- ==Conversational benchmarks== (CoQA, MMDialog, OpenAssistant)): Closest to our intended uses cases of language models, but diversity and complexity of questions often fall short in challenging the capabilities of latest chatbots.

Abstract
> Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: ==MT-bench==, a multi-turn question set; and ==Chatbot Arena==, a ==crowdsourced battle platform==. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available atÂ [this https URL](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge).

![[Pasted image 20240420155130.png]]