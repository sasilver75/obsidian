Without tokenization, computers would be unable to differentiate individual words, identify sentence boundaries, or recognize relationships between different linguistic elements.

Models don't understand words; they understand numbers.
When we receive a sequence of words, we convert them to numbers.
Sometimes we split words into pieces, such as splitting "tokenization" into "token" and "ization." 

This is needed because the model has a limited vocabulary; a token is the smallest unit of language that a model can understand.