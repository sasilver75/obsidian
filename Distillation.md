---
aliases:
  - Knowledge Distillation
---
Note:
- I've heard "Distillation" used in two contexts:
	- "Model Distillation" where a weaker model is trained on the probability distributions (a strong training signal) of the larger model
	- "Dataset Distillation" in the context of synthetic data, where we use (eg) GPT-4 to *distill* a set of synthetic data (eg using a technique like that from [[Self-Instruct]])

