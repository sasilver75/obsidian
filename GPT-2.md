February, 2019 (full release of GPT-2 on November 7, 2019) -- [[OpenAI]]
Paper: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

OpenAI's second language model in the GPT series; notable because we learned about zero-shot generalization to all sorts of interesting tasks (eg [[Question Answering]]).

1.5B parameter transformer model trained on [[Webtext]] 

Abstract
> Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that ==language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages== called ==WebText==. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.
> The capacity of the language model is essential to the success of ==zero-shot task transfer== and increasing it improves performance in a log-linear fashion across tasks. Our largest model, ==GPT-2,== is a ==1.5B parameter== Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits ==WebText==. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.


![[Pasted image 20240425141912.png]]