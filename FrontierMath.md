Friday, November 8
Epoch AI
Blog Post: [FrontierMath: A math benchmark testing the limits of AI](https://epochai.org/frontiermath)

  Existing math benchmarks like [[[GSM8K]]] and [[MATH]] are approaching saturation, with AI models scoring over 90%—partly due to data contamination. FrontierMath significantly raises the bar. Our problems often require hours or even days of effort from expert mathematicians.
 
 We evaluated six leading models, including [[Claude 3.5]] Sonnet, [[GPT-4o]], and [[Gemeni 1.5]] Pro. Even with extended thinking time (10,000 tokens), Python access, and the ability to run experiments, ==success rates remained below 2%==—compared to over 90% on traditional benchmarks.