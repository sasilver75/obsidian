April 12, 2023
Blog: [Free Dolly: The world's first truly-open instruction-tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)

- ==15k English instruction-following examples== written by ==[[DataBricks]] employees==.
- Both instructions and answers are ==human-generated==.
- This is in contrast to the other datasets above, where instruction and/or responses are generated by ChatGPT.
- ==Examples cover 7 use cases== (openQA, closedQA, information extraction and summarization of Wikipedia data, brainstorming, classification, creative writing).
- Permissively licensed.

Summary
- Dolly was an LLM trained for less than $30 to exhibit ChatGPT-like human interactivity.
- In this article, Dolly 2.0 is produced -- the first open-source, instruction-following LLM, fine-tuned on human-generated instruction dataset licensed for research and commercial use.
- Dolly 2.0 is a 12B parameter LM based on the [[Pythia]] model family, and fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced by Databricks employees.
- While Dolly 1.0 was trained for $30 using a dataset that the [[Alpaca]] team had created using the OpenAI API; but the legal terms of that service aren't favorable. The Dolly team wanted to create a new dataset that wasn't "tainted" for commercial use.
- They knew that the OG InstructGPT model was trained on 13k instructions, so they set up some crowdsourcing among their employees. They did so, and trained a model, after creating a dataset!

![[Pasted image 20240424015945.png]]

![[Pasted image 20240424015937.png]]


![[Pasted image 20240418165458.png]]