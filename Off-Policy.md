References:
- Lecture: [[David Silver RL (5) Model-Free Control]]

In [[Reinforcement Learning]], off-policy algorithms learn from the experience gathered by following a *==behavior policy==* $\mu$, while optimizing a separate *==target policy==* $\pi$.
- The behavior policy is used to generate actions and interact with the environment, while the target policy is the one being learned and optimized.
- The agent can learn from actions taken by other agents, or from a replay buffer that stores past experiences (==experience replay==).

Off-policy algorithms can learn from a wider range of experiences, including those generated by suboptimal or deliberately exploratory policies.

May suffer from instability and divergence issues, especially when the behavior policy is significantly different from the target policy.

Examples include [[Q-Learning]], [[Deep Q-Networks]] (DQN), and Deep Deterministic Policy Gradient (DDPG).

Compared to [[On-Policy]], Off-Policy has slower convergence and can have much more variance, but they benefit from techniques like experience replay. Off-Policy can learn from a wider range of experiences, including those generated by suboptimal or exploratory policies, or even do offline learning, where we learn from previously-collected data.
