September 12, 2024
[[OpenAI]]
Blog post: [Introducing OpenAI o1](https://openai.com/o1/)
- Product post: [Introducing Open AI o1-preview](https://openai.com/index/introducing-openai-o1-preview/)
- Eng post: [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/)
	- This one has a great set of examples of the thought process

References:
- Sequoia: [OpenAI's Noam Brown, Ilge Akkaya and Hunter Lightman on o1 and Teaching LLMs to Reason Better](https://youtu.be/jPluSXJpdrA?si=cdGuAYyW679EQfpa)
- [A wet blanket: Subbarao Kambhampati analyzing o1 performance, and comparing o1 to 4o-mini with an LLM-modulo framework](https://x.com/rao2z/status/1843307760311533768)
- Nate Lambert's [[Reverse Engineering OpenAI's o1 (September 16, 2024) {Interconnects}]]
- [Noam Brown Twitter Post](https://x.com/polynoamial/status/1834280155730043108)
- [Ross Taylor Twitter Post (Reasoning lead @ Meta)](https://x.com/rosstaylor90/status/1834435062214820153)

Models:
- o1-preview
- o1-mini (80% cheaper than preview, competitive with it on coding tasks)

Note: A painful aspect of this model is that it can't do streaming yet. So a wall of text just shows up. This makes it not good for some situations (eg UX, or if you want to do something with the results as they're streamed).

>  In a qualifying exam for the International Mathematics Olympiad (IMO), ==GPT-4o correctly solved only 13% of problems, while the reasoning model scored 83%.==

> As part of developing these new models, we have come up with a new safety training approach that ==harnesses their reasoning capabilities to make them adhere to safety and alignment guidelines==. By being able to reason about our safety rules in context, it can apply them more effectively. On one of our hardest jailbreaking tests, ==GPT-4o scored 22 (on a scale of 0-100) while our o1-preview model scored 84==. You can read more about this in the [system card](https://openai.com/index/openai-o1-system-card/) and our [research post](https://openai.com/index/learning-to-reason-with-llms/).

![[Pasted image 20241008101825.png|500]]


![[Pasted image 20241008102016.png|400]]
See above that the released o1-preview (along with o1-mini) aren't as powerful as the yet-to-be-released o1 model (as of the time of this blog post).

![[Pasted image 20241008102035.png|500]]


![[Pasted image 20241009103038.png|400]]
From Jim Fan

> This approach isn't good for every query -- eventually the ChatGPT product will absorb o1 and route your queries to the right model -- simple queries burn a lot of excess tokens in the system.
> - Nato, [[Reverse Engineering OpenAI's o1 (September 16, 2024) {Interconnects}]]

![[Pasted image 20241009104841.png|500]]

