November 17, 2023 -- [[Allen Institute]]
Paper: [Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2](https://huggingface.co/papers/2311.10702)
See previous work on evaluation of Instruction-Tuning datasets: [[Tulu]]

A dataset of [[Instruction-Tuning]] data released together with the [[Tulu 2]] paper from [[Allen Institute]].

Dataset is a mix of:
- [[FLAN]] (50,000 examples from FLANv2, with another 50,000 samples from the [[Chain of Thought|CoT]] subset of the FLAN v2 mixture)
- [[oasst1]] (We use the highest scoring paths in each conversation tree; 7,708 examples)
- [[ShareGPT]] (114,046 examples)
- GPT4-Alpaca (20,000 samples)
- Code-Alpaca (20,022 examples to improve code abilities)
- [[LIMA]]: (1030 examples)
- [[WizardLM]]'s [[Evol-Instruct]] (30,000 examples)
- Open-Orca (30,000 examples generated by GPT-4 from OpenOrca)
- Hardcoded (140 samples, eg "Tell me about yourself")
- Science (7,544 examples from a mixture of scientific document understanding tasks)

Abstract
> Since the release of Tulu [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to ==new finetuning techniques==. ==We test== and incorporate a number of these advances into Tulu, resulting in Tulu 2, a suite of improved Tulu models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) ==Tulu-V2-mix==, an ==improved collection of high-quality instruction datasets==; (2) ==Tulu 2==, ==LLAMA-2 models finetuned on the V2 mixture==; (3) ==Tulu 2+DPO==, Tulu 2 ==models trained with direct preference optimization (DPO),== including the largest DPO-trained model to date (Tulu 2+DPO 70B); (4) CODE Tulu 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the ==Tulu 2 suite achieves state-of-the-art performance among open models== and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.

