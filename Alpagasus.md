February 13, 2024
Lichang Chen et al (UMD, Samsung Research, USC)
[AlpaGasus: Training a better Alpaca with Fewer Data](https://arxiv.org/abs/2307.08701)
#zotero 
Takeaway: Introduces a novel data selection strategy that filters out low-quality instruction data using a strong LLM. The technique is just prompting a strong LM with a(instruction, input, response) triplet and asking it to generate a 0-5 score with explanation, then keeping the records with some score about a threshold $\tau$ (eg 4.5/5). With this technique, they train AlpaGasus. with is [[LLaMA]] model finetuned on a 9k high-quality subset of the 52k synthetic instruction data from the [[Alpaca]] dataset (generated by [[GPT-3.5]] using a [[Self-Instruct]] style), and see that they can get equivalent performance with a 6k subset, or superior performance with a 9k subset. Authors also apply this technique to the [[Dolly]] human-generated instruction dataset from Databricks, and achieve superior performance with a 3k subset of the 15k dataset.

----

There's been a lot of work using strong language models as teachers to create instruction-following datasets.
- Even with strong teachers, responses inevitably include incorrect or irrelevant answers to corresponding instructions, which can be misleading or detrimental to IFT -- this motivates the need to clean/filter poor quality data from IFT datasets.
- The bottleneck is that rating synthetically-generated data usually requires expensive human labor, but still may not be accurate for IFT data created by strong teachers (because these strong teachers can generate fluent responses whose incorrectness is hard to detect).
- Authors design a prompt applied to a powerful LLM for evaluating the quality of each (instruction, input, response) tuple, and then filter out the ones with score lower than some threshold.
	- Applying this to the 52k examples used to train [[Alpaca]], they find that the majority of the data suffered from low-quality issues.
	- Doing IFT on the smaller, filtered subset of 9k examples produced a better model than the original Alpaca, which they call [[AlpaGasus]]. This also reduced training time from 80 minutes to merely 14 minutes on 4x NVIDIA A100 GPUs. ==This shows that data quality in IFT can outweigh data quantity, and prioritizing data quality presents a new and more efficient paradigm to improving LLMs!==
- The authors select four different *human-instruction* test sets for evaluating AlpaGasus's instruction-following capability, including the ones used by [[WizardLM]], [[Vicuna]], [[Koala]], and [[Self-Instruct]].
	- Authors use GPT-4 as their judge for these evaluations.
	- AlpaGasus performs significantly better than Alpaca on all four test sets, indicating that the authors' data filtering approach exhibits significant benefits.
- Filtering
	- Authors prompt a strong LLM to provide a score for each triplet of (instruction, input, response) according to some user-specific property ("dimension"). authors use "Accuracy" as their dimension, and only keep the ~9k of the 52k that have scores of >= 4.5/5. They then use this dataset to fine-tune a LLaMA into AlpaGasus.
	- Their prompt is interesting (see figures) because... it's not very interesting. It basically says: "Please rate this (instruction, input, response) with respect to dimension X. THEN provide an explanation for why you gave that rating." The ordering of those two actions seems suboptimal and not very CoT-y to me.
- Evaluations
	- They benchmark AlpaGasus against [[Alpaca]], [[text-davinci-003]], [[ChatGPT]], and Claude.
	- Excited by recent LLM-as-a-Judge papers, they include both model's responses in the input to the judge (eg GPT-4), followed by an instruction aiming to rate the responses between 1 and 10. 
	- Authors limit LM responses to 1024 tokens, and to mitigate [[Positional Bias]], they try both orderings and define the final "Win-Tie-Lose" for AlpaGasus to be:
		- Win: WinWin or WinTie
		- Tie: TieTie or WinLose
		- Lose: LoseLose or LoseTie
	- To investigate the efficacy of data selection, they compare AlpaGasus with LLaMA models fine-tuned on *randomly sampled* subsets of the Alpaca 52k data (this is a stupid ablation), and AlpaGasus significantly outperforms.
	- The authors ablate the threshold $\tau$ used for data filtering, and see that the model trained on a more lenient threshold has worse performance (See figures).
	- Authors note that they only need 6k high-quality data to finetune LLaMA to similar performance as the original [[Alpaca]].
- Human-Written Instruction Set Filtering
	- The authors note that this technique could be used to filter human-written data, in addition to synthetically-generated data. 
	- They investigate the DataBricks [[Dolly]] dataset, which has 15,000 high-quality human-generated prompt/response pairs from ~5,000 Databricks contributors, covering activities from inventive brainstorming to succinct summarization.
		- Authors filtered the 15k down to 3k, using a threshold of $\tau=4.5$... and then realize that the model trained on the filtered dataset exhibits superior performance, highlighting how this can be used for human-composed datasets too.
- Authors note that Alpagasus achieves equal or better performance across nearly all skills, but doesn't show advantages on the remaining 7 skills like coding. This indicates that there's still importance in keeping the training data diverse and balanced across different categories in your IFT dataset.
- Appendix notes
	- 


Abstract
> Large language models (LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, ==widely used IFT datasets== (e.g., Alpaca's 52k data) ==surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT==. In this paper, ==we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM== (e.g., ChatGPT). To this end, we introduce ==AlpaGasus==, which is ==finetuned on only 9k high-quality data filtered from the 52k Alpaca data==. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches >90% performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the efficacy of our method across diverse datasets, base models, and LLM filters. ==Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.== Our project page is available at: [this https URL](https://lichang-chen.github.io/AlpaGasus/)


# Paper Figures
![[Pasted image 20240710013447.png|300]]
Showing performance of AlpaGasus on the test sets from four papers (Vicuna, Koala, WizardLM, Self-Instruct) at different levels of filtering (of the Alpaca dataset, using the AlpaGasus technique)

![[Pasted image 20240710161434.png]]
Showing along the top a few (instruction, input, response) pairs, and along the bottom, the corresponding scores from the response filtering method used to train AlpaGasus. See that two of the three examples were rejected.

![[Pasted image 20240710161423.png]]
- Really interesting that they choose to output a line indicating the score *before* outputting the explanation for it. As far as I know, it's better practice to output an explanation before outputting a score (more CoT-y)
- By "dimension", they mean any user-preferred property such as helpfulness and accuracy.
	- The authors used "accuracy" as their designated property, when training AlpaGasus.
![[Pasted image 20240710162532.png|400]]
Showing the distribution of scores assigned to responses in the Alpaca dataset by using their filtering method, focused on 0-5 ratings (+explanation) using "accuracy" as the dimension for rating purposes. They use a threshold $\tau=4.5$ as their cutoff for dataset filtering.

![[Pasted image 20240710163919.png]]
Showing Alpagasus beating Alpaca across four test sets, despite training on a smaller amount of instruction data. All hyperparameters/training scripts are the same.

![[Pasted image 20240710164944.png]]
Authors tried their data filtering for the human-generated [[Dolly]] dataset too, and found that a 3k subset of the 15k data gave superior performance.

![[Pasted image 20240710170451.png]]
This is the LLM-as-a-Judge prompt used to evaluate AlpaGasus against other models.

![[Pasted image 20240710170535.png]]
Examples of the few score=5.0 records from the Alpaca dataset. See that these aren't ... obviously incredible generations. It's more like they're simple/easy instructions.

![[Pasted image 20240710170707.png]]
Examples of score=4.5 records in the Alpaca dataset. See that "Milkshake" and "Milkshakes" are included, "Lemonades" is pluralized, etc. To me, this isn't a 4.5 response -- this should be lower. This speaks to their scoring prompt not being good, which isn't news to me. It should be specific and use a rubric and exemplars.

![[Pasted image 20240710171000.png]]
The plurality of scores in the Alpaca dataset are here.

![[Pasted image 20240710170931.png]]
Jesus christ. Luckily there are very few examples <3 in the Alpaca dataset.

# Non-Paper Figures