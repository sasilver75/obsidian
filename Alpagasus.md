February 13, 2024
Lichang Chen et al (UMD, Samsung Research, USC)
[Alpagasus: Training a better Alpaca with Fewer Data](https://arxiv.org/abs/2307.08701)
#zotero 
Takeaway: Introduces a novel data selection strategy that filters out low-quality instruction data using a strong LLM. With this technique, they train Alpagasus. with is [[LLaMA]] model finetuned on a 9k high-quality subset of the 52k instructions from the [[Alpaca]] dataset (generated by [[GPT-3.5]] using a [[Self-Instruct]] style).

----

There's been a lot of work using strong language models as teachers to create instruction-following datasets.
- Even with strong teachers, responses inevitably include incorrect or irrelevant answers to corresponding instructions, which can be misleading or detrimental to IFT -- this motivates the need to clean/filter poor quality data from IFT datasets.
- The bottleneck is that rating synthetically-generated data usually requires expensive human labor, but still may not be accurate for IFT data created by strong teachers (because these strong teachers can generate fluent responses whose incorrectness is hard to detect).
- Authors design a prompt applied to a powerful LLM for evaluating the quality of each (instruction, input, response) tuple, and then filter out the ones with score lower than some threshold.
	- Applying this to the 52k examples used to train [[Alpaca]], they find that the majority of the data suffered from low-quality issues.
	- Doing IFT on the smaller, filtered subset of 9k examples produced a better model than the original Alpaca, which they call [[Alpagasus]]. This also reduced training time from 80 minutes to merely 14 minutes on 4x NVIDIA A100 GPUs. ==This shows that data quality in IFT can outweigh data quantity, and prioritizing data quality presents a new and more efficient paradigm to improving LLMs!==
- The authors select four different *human-instruction* test sets for evaluating Alpagasus's instruction-following capability, including the ones used by [[WizardLM]], [[Vicuna]], [[Koala]], and [[Self-Instruct]].
	- Authors use GPT-4 as their judge for these evaluations.
	- Alpagasus performs significantly better than Alpaca on all four test sets, indicating that the authors' data filtering approach exhibits significant benefits.
- 


Abstract
> Large language models (LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, ==widely used IFT datasets== (e.g., Alpaca's 52k data) ==surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT==. In this paper, ==we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM== (e.g., ChatGPT). To this end, we introduce ==AlpaGasus==, which is ==finetuned on only 9k high-quality data filtered from the 52k Alpaca data==. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches >90% performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the efficacy of our method across diverse datasets, base models, and LLM filters. ==Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.== Our project page is available at: [this https URL](https://lichang-chen.github.io/AlpaGasus/)


# Paper Figures
![[Pasted image 20240710013447.png|300]]
Showing performance of Alpagasus on the test sets from four papers (Vicuna, Koala, WizardLM, Self-Instruct) at different levels of filtering (of the Alpaca dataset, using the Alpagasus technique)

![[Pasted image 20240710161434.png]]
Showing along the top a few (instruction, input, response) pairs, and along the bottom, the corresponding scores from the response filtering method used to train Alpagasus. See that two of the three examples were rejected.

![[Pasted image 20240710161423.png]]
Curious what they mean here by "dimension"; really interesting that they choose to output a line indicating the score *before* outputting the 



# Non-Paper Figures