---
aliases:
  - GQA
---
- Video: [Turns out Attention wasn't all we needed - How have modern Transformer architectures evolved?](https://youtu.be/mVLO9PHFc0I?si=rilWUkZy9z8zoFHq)

I've heard some people say recently that this is "the best form of attention" 🤷‍♂️ - Not sure what they meant by that.

![[Pasted image 20240214165238.png]]
