Feb 1, 2024 -- [[Allen Institute]] (AI2)
Paper: [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838)
Related: [[Dolma]]
...
Significance: ...
References:
- [AI2 OLMo technical blog post](https://blog.allenai.org/olmo-open-language-model-87ccfc95f580)

Family: 1B, 7B, 65B
Later, an OLMo-7B-Instruct was released

Abstract
> Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the ==most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed==. Given the importance of these details in scientifically studying these models, including their biases and potential risks, ==we believe it is essential for the research community to have access to powerful, truly open LMs==. To this end, this technical report details the first release of OLMo, ==a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling==. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, ==including== ==training data== and ==training and evaluation code==. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.


