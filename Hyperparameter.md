Hyperparameters are adjustable parameters that let us control the model optimization process -- different hyperparameter values can impact model training and convergence rates, and they're usually optimized in an outer-loop.

Examples are [[Learning Rate]], number of epochs, [[Batch]] size, Kernel Size, Stride, various Pooling settings, or even (?) sampling strategies in LMs.